{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install package\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q -U google-genai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import library\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import re\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "import time\n",
    "from typing import Dict, Any, Optional, List\n",
    "import random\n",
    "\n",
    "# Show all columns\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folder to store the name of food and location in the video\n",
    "FOOD_LOCATION_FOLDER = 'food_location'\n",
    "\n",
    "# Check if the folder exists\n",
    "if not os.path.exists(FOOD_LOCATION_FOLDER):\n",
    "    os.makedirs(FOOD_LOCATION_FOLDER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ƒê·ªçc d·ªØ li·ªáu v√†o dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 14252 entries, 0 to 14251\n",
      "Data columns (total 3 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   video.id    14252 non-null  object\n",
      " 1   desc        14252 non-null  object\n",
      " 2   transcript  13430 non-null  object\n",
      "dtypes: object(3)\n",
      "memory usage: 334.2+ KB\n"
     ]
    }
   ],
   "source": [
    "# Load data from parquet file\n",
    "# video_df = pd.read_parquet(\"top_20_percent_weekly_videos_transcripts.parquet\")\n",
    "video_df = pd.read_parquet(\"small_weekly_videos_transcripts.parquet\")\n",
    "\n",
    "# # Sort data by \"statsV2.playCount\" in descending order\n",
    "# # then reset the index to start from 0\n",
    "# video_df = video_df.sort_values(\n",
    "#     by=\"statsV2.playCount\",\n",
    "#     ascending=False\n",
    "# ).reset_index(drop=True)\n",
    "\n",
    "video_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chu·∫©n b·ªã x·ª≠ l√Ω d·ªØ li·ªáu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "range(0, 14252)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_id_range = range(video_df.shape[0])[0:20_000]\n",
    "video_id_range"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Danh s√°ch c√°c API ƒë·ªÉ ch·∫°y lu√¢n phi√™n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_list = [\n",
    "    # \"AIzaSyCgr0Af_ph5vvql_VXpyIwfumJOaehbLDo\",  # vmphat.24\n",
    "    \"AIzaSyAAmXLg2yM3Ygz3B_HYC4fcE1iJDNFhxm0\",  # pvminh\n",
    "    \"AIzaSyAB9vrQbQPxOp1tbYWN9hjmmmno-9uGwR0\",  # ngocquynh\n",
    "    \"AIzaSyCArspeWWKenZy4QSQlpBIrUAnXCWPRr90\",  # kiet\n",
    "    \"AIzaSyBMcY_CGvsXGJSOMu3vLfWsd4-qL0bQflg\",  # franie\n",
    "    \"AIzaSyAL9WZ2mO88O6DuwivJJWK2oqcy9_UXBNQ\",  # daniel\n",
    "    \"AIzaSyDrD1yVeRW85VxX433JKFxKbtFuQ83UhMo\",  # tulin\n",
    "    \"AIzaSyA8DDmJgizVgSiE2MdjnVpDZEXqTjEgBRg\",  # martin\n",
    "\n",
    "    # \"AIzaSyAcvcAtAlMW4QD1OzCoIsmZl04qjFZ_AZo\",  # khdludteam5\n",
    "    # \"AIzaSyCbs_KHkUr-BWL9X6_06kZb3brG7UI1a6w\",  # vmphat21\n",
    "\n",
    "    \"AIzaSyBrTgG4YDzJMuK9WknMTbdnnoskSX1nvMY\",  # pr\n",
    "\n",
    "    # \"AIzaSyDyjL0w1m1dWCNOP7_9UYXDQnNOqbAdbCw\",  # vmphat.24\n",
    "    \"AIzaSyAHiAgc7tIuq4YKtswB-AaHa0W9eqQ5jGw\",  # pvminh\n",
    "    \"AIzaSyCnUToo7FRJn8v3BwMOt3FWwrDDFf2b4UI\",  # ngocquynh\n",
    "    \"AIzaSyCAnhUoYz6YAYCSfSFF-JmGNbMdxzhDKYU\",  # kiet\n",
    "    \"AIzaSyBqu4Xbby4sc0vsCUbxhjqYcqOwKKAwaT4\",  # franie\n",
    "    \"AIzaSyDh32FdRtHzuRUaZUXafcmlPHqYQtbRx3A\",  # daniel\n",
    "    \"AIzaSyBRhc3Q6rdz3Ok93V5xB76Lfk3mNtdzQEI\",  # tulin\n",
    "    \"AIzaSyDPUFWmBABBPAYEa_lOkeony8C2eqKkXTw\",  # martin\n",
    "    \"AIzaSyAY8nfoP7DXfL571ovT8V_HlMWCTdHqdgc\",  # khdludteam5\n",
    "    \"AIzaSyC4WprE1HsmCUwOoGi4HFfA1Lzg5XSE0Cg\",  # vmphat21\n",
    "\n",
    "    \"AIzaSyC-letXWg8hVdOA8H6BlEXb-TXF7W7twQM\",\n",
    "    \"AIzaSyCmJQlfuGKf2FNvrUWYd-fPuxYRcmm3p4Q\",\n",
    "    \"AIzaSyDlKoywc1dVIaiv4UGVDc0OuaEBFluS2IU\",\n",
    "    \"AIzaSyDk5UZkrHP6H3fgAI0FidWJKcVptQdEWBE\",\n",
    "    \"AIzaSyBkVUkCK_mMBhJnyi9KoZ9WFf1tfJnlOac\",\n",
    "    \"AIzaSyATHBdVQsH-7J8M2v6UcciZyWbzkr13uTA\",\n",
    "    \"AIzaSyAvAt0as8Zs0r_iustkbWyimOhdLOzCm8w\",\n",
    "    \"AIzaSyDaUPT6NQS8sqs16_hm9_A8ONHsVbh8QiY\",\n",
    "\n",
    "\n",
    "    \"AIzaSyAdbNfxlQQQjKSgAcOjQt-XUwil-FMl6V8\",  # Luc - Ca nhan\n",
    "    \"AIzaSyCSGNpc1IlacTUwN31TKWms0RzF_we17vk\",  # Luc - Truong\n",
    "\n",
    "    \"AIzaSyBE8VObttX0oOGz5Jd82AtOiLTSzavIBL8\",\n",
    "    \"AIzaSyAo5sCKOhGgYNgJ0m1QKsT29Ov-GNQHeSo\",\n",
    "    \"AIzaSyDLa5CYtBGeXoV1-8y_ojA9eR_xzONABew\",\n",
    "    \"AIzaSyCUtzli8ZRql653-0u_RrL7zM2SgEdb5ss\",\n",
    "    \"AIzaSyBPi15fdt_YtyqaBIEuvSQ66T6T0ROHEA4\",\n",
    "    \"AIzaSyC3DhYcDb8sLzzwywoJe8Foki3kMnVKPwQ\",\n",
    "    \"AIzaSyDRE-VA4R9-UBRekhCGCcy3NlhlwQa1fnU\",\n",
    "    \"AIzaSyD-WsC4RABxdqebovU-TYMueMHxPT2l6Nw\",\n",
    "\n",
    "    \"AIzaSyDBscWgL9o2QtfNvx_xnZP3CWweyknGJKM\",  # han-asd\n",
    "    \"AIzaSyAhbb5IEibxRDH8a8XB8Zk7lopPh8jhgwI\",  # han-len\n",
    "]\n",
    "\n",
    "assert len(api_list) == len(set(api_list)), \"Duplicate API keys found\"\n",
    "n_apis = len(api_list)\n",
    "api_request_threshold = 14\n",
    "api_idx = random.randint(0, n_apis - 1)\n",
    "n_consecutive_requests = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L·∫•y danh s√°ch c√°c video ƒë√£ ƒë∆∞·ª£c x·ª≠ l√Ω\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read text file to get a set of video IDs that have been preprocessed\n",
    "preprocessed_video_ids = set()\n",
    "if os.path.exists(\"preprocessed_video_ids.txt\"):\n",
    "    with open(\"preprocessed_video_ids.txt\", \"r\") as f:\n",
    "        preprocessed_video_ids = set(f.read().splitlines())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of videos already preprocessed: 14252\n"
     ]
    }
   ],
   "source": [
    "# Print the number of videos that have been preprocessed\n",
    "print(f\"Number of videos already preprocessed: {len(preprocessed_video_ids)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C√°c h√†m ti·ªán √≠ch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Ph√¢n t√≠ch n·ªôi dung sau (t·ª´ transcript video ho·∫∑c m√¥ t·∫£) v√† tr√≠ch xu·∫•t th√¥ng tin:\n",
    "{text}\n",
    "\n",
    "Ph√¢n t√≠ch v√† tr√≠ch xu·∫•t c√°c th√¥ng tin theo c·∫•u tr√∫c d∆∞·ªõi ƒë√¢y:\n",
    "B∆∞·ªõc 1: Ph√¢n lo·∫°i Video (d·ª±a tr√™n n·ªôi dung):\n",
    "1.1. Ph√¢n lo·∫°i video (c√≥ th·ªÉ d·ª±a tr√™n c√°c t·ª´ kh√≥a):\n",
    "- \"ƒÇn v·∫∑t ƒë∆∞·ªùng ph·ªë\": t·ª´ kh√≥a \"xe ƒë·∫©y\", \"v·ªâa h√®\", \"ƒë·ªì ƒÉn v·∫∑t\", \"ch·ª£ ƒë√™m\", \"h√†ng rong\", \"ƒë∆∞·ªùng ph·ªë\" -> \"type\": 0\n",
    "- \"Nh√† h√†ng, Qu√°n ƒÉn\": t·ª´ kh√≥a \"nh√† h√†ng\", \"qu√°n ƒÉn\", \"ƒë·∫∑t b√†n\", \"ph·ª•c v·ª•\", \"buffet\", \"sang tr·ªçng\", \"b√¨nh d√¢n\", \"qu·∫ßy bar\" -> \"type\": 1\n",
    "- \"N·∫•u ƒÉn\": t·ª´ kh√≥a \"n·∫•u ƒÉn\", \"c√¥ng th·ª©c\", \"ch·∫ø bi·∫øn\", \"nguy√™n li·ªáu\", \"b·∫øp\", \"t·∫°i nh√†\", \"d·ª•ng c·ª• n·∫•u b·∫øp\" -> \"type\": 2\n",
    "- \"Kh√°c\": Kh√¥ng th·ªÉ ph√¢n lo·∫°i -> \"type\": 3\n",
    "\n",
    "1.2. X√°c ƒë·ªãnh t·∫•t c·∫£ c√°c m√≥n ƒÉn ƒë∆∞·ª£c ƒë·ªÅ c·∫≠p (c√≥ th·ªÉ c√≥ nhi·ªÅu m√≥n ƒÉn): L∆∞u t·∫•t c·∫£ c√°c m√≥n ƒÉn v√†o \"food\" v√† tr·∫£ v·ªÅ theo d·∫°ng string data type (str), c√°c m√≥n ƒÉn ƒë∆∞·ª£c c√°ch nhau b·ªüi d·∫•u ph·∫©y (\",\").\n",
    "1.3. ƒê·ªãa ƒëi·ªÉm: X√°c ƒë·ªãnh th√†nh ph·ªë, qu·∫≠n/huy·ªán t·∫°i Vi·ªát Nam\n",
    "\n",
    "B∆∞·ªõc 2: Tr·∫£ v·ªÅ k·∫øt qu·∫£ JSON:\n",
    "{{\n",
    "    \"type\": \"Ph√¢n lo·∫°i 0, 1, 2, 3\",\n",
    "    \"food\": \"T·∫•t c·∫£ c√°c m√≥n ƒÉn ƒë∆∞·ª£c ƒë·ªÅ c·∫≠p\",\n",
    "    \"city\": \"Th√†nh ph·ªë\",\n",
    "    \"district\": \"Qu·∫≠n/Huy·ªán\",\n",
    "    \"source\": \"transcript/desc\"\n",
    "}}\n",
    "\n",
    "L∆∞u √ù:\n",
    "- ∆Øu ti√™n th√¥ng tin ch√≠nh x√°c\n",
    "- B√°m s√°t n·ªôi dung\n",
    "- Tr·∫£ v·ªÅ ƒë√∫ng ƒë·ªãnh d·∫°ng JSON\n",
    "- C√°c th√¥ng tin tr·∫£ v·ªÅ ƒë·ªÅu ph·∫£i c√≥ ƒë√∫ng ƒë·ªãnh d·∫°ng\n",
    "- ƒê·ªëi v·ªõi c√°c video kh√¥ng thu·ªôc ch·ªß ƒë·ªÅ ·∫©m th·ª±c th√¨ h√£y tr·∫£ v·ªÅ None cho t·∫•t c·∫£ c√°c tr∆∞·ªùng\n",
    "\"\"\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "B·∫°n l√† m·ªôt chuy√™n gia trong lƒ©nh v·ª±c ph√¢n t√≠ch d·ªØ li·ªáu th√¥ng minh. B·∫°n c√≥ th·ªÉ ph√¢n t√≠ch n·ªôi dung video v√† tr√≠ch xu·∫•t th√¥ng tin t·ª´ ƒë√≥. Ng∆∞·ªùi d√πng s·∫Ω cung c·∫•p th√¥ng tin v·ªÅ m√¥ t·∫£ v√† transcript c·ªßa video TikTok v·ªÅ ch·ªß ƒë·ªÅ ·∫©m th·ª±c. H√£y tr√≠ch xu·∫•t th√¥ng tin v·ªÅ c√°c m√≥n ƒÉn v√† ƒë·ªãa ƒëi·ªÉm ƒë∆∞·ª£c ƒë·ªÅ c·∫≠p trong video ƒë√≥. B·∫°n s·∫Ω tr·∫£ v·ªÅ m·ªôt JSON ch·ª©a c√°c th√¥ng tin sau:\n",
    "{\n",
    "    \"foods\": Danh s√°ch c√°c m√≥n ƒÉn ƒë∆∞·ª£c ƒë·ªÅ c·∫≠p trong video (danh s√°ch string),\n",
    "    \"city: T√™n th√†nh ph·ªë (string),\n",
    "    \"district\": T√™n qu·∫≠n/huy·ªán (string)\n",
    "}\n",
    "\n",
    "ƒê√¢y l√† m·ªôt s·ªë l∆∞u √Ω quan tr·ªçng:\n",
    "- ƒê·ªëi v·ªõi c√°c video kh√¥ng thu·ªôc ch·ªß ƒë·ªÅ ·∫©m th·ª±c th√¨ h√£y tr·∫£ v·ªÅ None cho t·∫•t c·∫£ c√°c tr∆∞·ªùng\n",
    "- C√°c th√¥ng tin tr·∫£ v·ªÅ ƒë·ªÅu ph·∫£i c√≥ ƒë√∫ng ƒë·ªãnh d·∫°ng\n",
    "- Tr·∫£ v·ªÅ k·∫øt qu·∫£ theo ƒë√∫ng ƒë·ªãnh d·∫°ng JSON\n",
    "- ∆Øu ti√™n th√¥ng tin ch√≠nh x√°c\n",
    "- C√¢u tr·∫£ l·ªùi ph·∫£i b√°m s√°t theo n·ªôi dung ƒë∆∞·ª£c cung c·∫•p\n",
    "\n",
    "D∆∞·ªõi ƒë√¢y l√† m√¥ t·∫£ v√† transcript c·ªßa video TikTok:\n",
    "- M√¥ t·∫£: %s\n",
    "- Transcript: %s\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ERROR = None\n",
    "\n",
    "\n",
    "def process_video_content(desc: str, transcript: str,\n",
    "                          api_key: str) -> str:\n",
    "    global ERROR\n",
    "    try:\n",
    "        # Call the API to generate content\n",
    "        client = genai.Client(api_key=api_key)\n",
    "        response = client.models.generate_content(\n",
    "            model='gemini-2.0-flash',\n",
    "            contents=[\n",
    "                prompt % (desc, transcript),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Extract JSON content from the markdown-formatted response\n",
    "        json_text: str = response.text\n",
    "        # Remove the markdown code block formatting\n",
    "        json_text: str = re.sub(r'^```json\\n|\\n```$', '', json_text)\n",
    "\n",
    "        return json_text\n",
    "\n",
    "    except Exception as e:\n",
    "        ERROR = e\n",
    "        print(f\"[Error] API `{api_key}` failed: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def save_response(video_id: str, json_text: str) -> bool:\n",
    "    # Define the file path for the target JSON file\n",
    "    output_path: str = FOOD_LOCATION_FOLDER + f\"/{video_id}.json\"\n",
    "\n",
    "    # Save the JSON response to a file\n",
    "    with open(output_path, 'w') as f:\n",
    "        f.write(json_text)\n",
    "\n",
    "    if os.path.exists(output_path):\n",
    "        print(f\"Saved response to {output_path}\")\n",
    "        return True\n",
    "    else:\n",
    "        print(f\"Failed to save response to {output_path}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ƒêo·∫°n ch∆∞∆°ng tr√¨nh ch√≠nh\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "B·∫°n l√† m·ªôt chuy√™n gia trong lƒ©nh v·ª±c ph√¢n t√≠ch d·ªØ li·ªáu th√¥ng minh. B·∫°n c√≥ th·ªÉ ph√¢n t√≠ch n·ªôi dung video v√† tr√≠ch xu·∫•t th√¥ng tin t·ª´ ƒë√≥. Ng∆∞·ªùi d√πng s·∫Ω cung c·∫•p th√¥ng tin v·ªÅ m√¥ t·∫£ v√† transcript c·ªßa video TikTok v·ªÅ ch·ªß ƒë·ªÅ ·∫©m th·ª±c. H√£y tr√≠ch xu·∫•t th√¥ng tin v·ªÅ c√°c m√≥n ƒÉn v√† ƒë·ªãa ƒëi·ªÉm ƒë∆∞·ª£c ƒë·ªÅ c·∫≠p trong video ƒë√≥. B·∫°n s·∫Ω tr·∫£ v·ªÅ m·ªôt JSON ch·ª©a c√°c th√¥ng tin sau:\n",
      "{\n",
      "    \"foods\": Danh s√°ch c√°c m√≥n ƒÉn ƒë∆∞·ª£c ƒë·ªÅ c·∫≠p trong video (danh s√°ch string),\n",
      "    \"city: T√™n th√†nh ph·ªë (string),\n",
      "    \"district\": T√™n qu·∫≠n/huy·ªán (string)\n",
      "}\n",
      "\n",
      "ƒê√¢y l√† m·ªôt s·ªë l∆∞u √Ω quan tr·ªçng:\n",
      "- ƒê·ªëi v·ªõi c√°c video kh√¥ng thu·ªôc ch·ªß ƒë·ªÅ ·∫©m th·ª±c th√¨ h√£y tr·∫£ v·ªÅ None cho t·∫•t c·∫£ c√°c tr∆∞·ªùng\n",
      "- C√°c th√¥ng tin tr·∫£ v·ªÅ ƒë·ªÅu ph·∫£i c√≥ ƒë√∫ng ƒë·ªãnh d·∫°ng\n",
      "- Tr·∫£ v·ªÅ k·∫øt qu·∫£ theo ƒë√∫ng ƒë·ªãnh d·∫°ng JSON\n",
      "- ∆Øu ti√™n th√¥ng tin ch√≠nh x√°c\n",
      "- C√¢u tr·∫£ l·ªùi ph·∫£i b√°m s√°t theo n·ªôi dung ƒë∆∞·ª£c cung c·∫•p\n",
      "\n",
      "D∆∞·ªõi ƒë√¢y l√† m√¥ t·∫£ v√† transcript c·ªßa video TikTok:\n",
      "- M√¥ t·∫£: Say n·∫Øng tr∆∞·ªõc khi ƒë∆∞·ª£c ly tr√† s·ªØa ü§° #ReviewAnNgon #vtmgr #reviewamthuc #tittungtang #LearnOnTiktok #AnCungTikTok #quan1 \n",
      "- Transcript: r·ªìi ƒë·ªÉ em x√≠u nha. m·ªói ng∆∞·ªùi m·ªôt m·ªôt √≠t u·ªëng cho bi·∫øt th√¥i nha. d·∫°. d·∫° ok tr√† ƒë√° c√≤n nhi·ªÅu l·∫Øm ch√∫ ∆°i. ch·ªù tr√† ƒë√° th√¥i nha. tr·ªùi ∆°i c√°i g√¨ ƒë√¢y v·∫≠y?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# View complete prompt from 1 random video\n",
    "row_idx = random.choice(video_id_range)\n",
    "video_id, desc, transcript = video_df.loc[\n",
    "    row_idx, [\"video.id\", \"desc\", \"transcript\"]]\n",
    "print(prompt % (desc, transcript))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 14252/14252 [00:00<00:00, 43022.99it/s]\n"
     ]
    }
   ],
   "source": [
    "for row_id in tqdm(video_id_range):\n",
    "    # Extract the videoID, description, and transcript from the DataFrame\n",
    "    video_id = video_df.loc[row_id, \"video.id\"]\n",
    "    desc = video_df.loc[row_id, \"desc\"]\n",
    "    transcript = video_df.loc[row_id, \"transcript\"]\n",
    "\n",
    "    # Skip if the video ID has already been preprocessed\n",
    "    if video_id in preprocessed_video_ids:\n",
    "        # print(f\"Row {row_id} => Already preprocessed\")\n",
    "        continue\n",
    "\n",
    "    # ========================================================\n",
    "    # ********** MUST STOP IF API QUOTA IS EXCEEDED **********\n",
    "    # ========================================================\n",
    "    # Process the audio to generate the transcript\n",
    "    json_text = process_video_content(\n",
    "        desc=desc, transcript=transcript, api_key=api_list[api_idx])\n",
    "\n",
    "    # Increment the number of consecutive requests\n",
    "    n_consecutive_requests += 1\n",
    "    # Check if the number of consecutive requests exceeds the threshold\n",
    "    # and change the API key if necessary\n",
    "    if n_consecutive_requests >= api_request_threshold:\n",
    "        n_consecutive_requests = 0\n",
    "        api_idx = (api_idx + 1) % n_apis\n",
    "        print(f\"Row {row_id} => Change API key\")\n",
    "\n",
    "    # # Sleep for a while to avoid hitting the API rate limit\n",
    "    # time.sleep(1)\n",
    "\n",
    "    if not json_text:\n",
    "        print(f\"Error processing content for the row: {row_id}\")\n",
    "        # break\n",
    "\n",
    "        sec = 15\n",
    "        print(f\">> Sleeping for {sec} seconds before retrying...\")\n",
    "        time.sleep(sec)\n",
    "        n_consecutive_requests += 8\n",
    "\n",
    "        # # Change the API key\n",
    "        # api_idx = (api_idx + 1) % n_apis\n",
    "        # n_consecutive_requests = 0\n",
    "        # print(f\"Row {row_id} => Change API key\")\n",
    "        continue\n",
    "\n",
    "    # Save the transcript to a JSON file\n",
    "    if not save_response(video_id, json_text):\n",
    "        print(f\"Error saving response for video ID: {video_id}\")\n",
    "        break\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T√¨m ra c√°c file JSON trong th∆∞ m·ª•c\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import os\n",
    "\n",
    "\n",
    "def list_file_types(directory: str, file_extension: str) -> List[str]:\n",
    "    \"\"\" List all files with a specific extension in a directory.\n",
    "\n",
    "    Args:\n",
    "        directory (str): Directory path.\n",
    "        file_extension (str): File extension.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: List of file paths.\n",
    "    \"\"\"\n",
    "\n",
    "    file_list: List[str] = []\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith(file_extension):\n",
    "                file_list.append(os.path.join(root, file))\n",
    "    return file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of JSON files found: 14252\n"
     ]
    }
   ],
   "source": [
    "json_files = list_file_types(\"food_location\", \".json\")\n",
    "print(f\"Number of JSON files found: {len(json_files)}\")\n",
    "# print(json_files[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 14252/14252 [00:00<00:00, 763516.33it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Set of filenames\n",
    "file_names = set()\n",
    "\n",
    "# Get a list of JSON files\n",
    "for json_file in tqdm(json_files):\n",
    "    # Extract base name of the file\n",
    "    # remove file extension\n",
    "    file_name = os.path.basename(json_file).replace(\".json\", \"\")\n",
    "    # print(f\"Processing file: {file_name}\")\n",
    "\n",
    "    # Append to the set\n",
    "    file_names.add(file_name)\n",
    "\n",
    "assert len(file_names) == len(json_files), \"Duplicate file names found!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the set of filenames to a text file\n",
    "with open(\"preprocessed_video_ids.txt\", \"w\") as f:\n",
    "    for file_name in file_names:\n",
    "        f.write(f\"{file_name}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
