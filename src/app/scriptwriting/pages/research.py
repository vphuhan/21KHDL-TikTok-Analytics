import os
import sys
try:
    import streamlit as st
    from google import genai
    import time
except ImportError as e:
    import traceback
    print(f"Error importing required packages: {e}")
    print(traceback.format_exc())
    try:
        # If streamlit is successfully imported, show error in UI
        if 'st' in globals():
            st.error(
                f"Failed to import required packages. Please install them using: pip install -r requirements.txt\n\nError: {e}")
            st.stop()
    except:
        # If we can't even use streamlit, exit the program
        sys.exit(1)


# App configuration
st.set_page_config(
    # Research on topics
    page_title="Nghi√™n c·ª©u ch·ªß ƒë·ªÅ",  # "Research on topics"
    layout="wide",
    initial_sidebar_state="expanded",
    page_icon=":material/search:",  # "üîç"
)

# ƒê·ªãnh nghƒ©a system prompt v√† user prompt
# System prompt
system_prompt = f"""
B·∫°n l√† m·ªôt chuy√™n gia nghi√™n c·ª©u v·ªõi ki·∫øn th·ª©c s√¢u r·ªông v·ªÅ nhi·ªÅu lƒ©nh v·ª±c. Nhi·ªám v·ª• c·ªßa b·∫°n l√† cung c·∫•p c√°c b√†i gi·∫£i th√≠ch chi ti·∫øt v·ªÅ m·ªôt ch·ªß ƒë·ªÅ ƒë∆∞·ª£c ng∆∞·ªùi d√πng nh·∫≠p v√†o. Khi nh·∫≠n y√™u c·∫ßu, h√£y tr·∫£ l·ªùi theo ƒë·ªãnh d·∫°ng Markdown v·ªõi t·ª´ng ph·∫ßn sau (m·ªói ph·∫ßn c√°ch nhau b·∫±ng d√≤ng tr·ªëng):

```
### Overview
- Gi·ªõi thi·ªáu chung v·ªÅ ch·ªß ƒë·ªÅ, gi·∫£i th√≠ch √Ω nghƒ©a v√† t·∫ßm quan tr·ªçng c·ªßa n√≥.

### Key Points
- Li·ªát k√™ v√† gi·∫£i th√≠ch c√°c y·∫øu t·ªë c·ªët l√µi, nh·ªØng kh√≠a c·∫°nh quan tr·ªçng c·ªßa ch·ªß ƒë·ªÅ.

### Examples
- ƒê∆∞a ra c√°c v√≠ d·ª• c·ª• th·ªÉ ho·∫∑c tr∆∞·ªùng h·ª£p nghi√™n c·ª©u li√™n quan ƒë·∫øn ch·ªß ƒë·ªÅ.

### Challenges
- N√™u ra nh·ªØng th√°ch th·ª©c, kh√≥ khƒÉn ho·∫∑c nh·ªØng hi·ªÉu l·∫ßm ph·ªï bi·∫øn li√™n quan ƒë·∫øn ch·ªß ƒë·ªÅ.

### Best Practices
- ƒê·ªÅ xu·∫•t c√°c chi·∫øn l∆∞·ª£c, h∆∞·ªõng d·∫´n ho·∫∑c m·∫πo h·ªØu √≠ch trong vi·ªác √°p d·ª•ng ch·ªß ƒë·ªÅ v√†o th·ª±c ti·ªÖn.

### Trends
- Ph√¢n t√≠ch c√°c xu h∆∞·ªõng hi·ªán t·∫°i v√† d·ª± ƒëo√°n nh·ªØng h∆∞·ªõng ph√°t tri·ªÉn trong t∆∞∆°ng lai c·ªßa ch·ªß ƒë·ªÅ.

### Resources
- G·ª£i √Ω th√™m c√°c ngu·ªìn t√†i li·ªáu, website, s√°ch, b√†i vi·∫øt ho·∫∑c video ƒë·ªÉ ng∆∞·ªùi d√πng c√≥ th·ªÉ nghi√™n c·ª©u s√¢u h∆°n.
```

**Ch√∫ √Ω:**
- Cung c·∫•p th√¥ng tin m·ªôt c√°ch r√µ r√†ng, s√∫c t√≠ch v√† ch√≠nh x√°c.
- Gi·ªØ cho m·ªói ph·∫ßn c√≥ n·ªôi dung ƒë·∫ßy ƒë·ªß nh∆∞ng kh√¥ng qu√° d√†i d√≤ng, d·ªÖ hi·ªÉu cho ng∆∞·ªùi d√πng.
- S·ª≠ d·ª•ng ƒë·ªãnh d·∫°ng Markdown v·ªõi ti√™u ƒë·ªÅ r√µ r√†ng v√† danh s√°ch g·∫°ch ƒë·∫ßu d√≤ng ƒë·ªÉ t·∫°o c·∫•u tr√∫c tr·ª±c quan cho b√†i gi·∫£i th√≠ch.
- T·∫≠p trung v√†o nh·ªØng ƒëi·ªÉm c·ªët l√µi v√† th√¥ng tin h·ªØu √≠ch nh·∫•t cho ng∆∞·ªùi d√πng.
- Ch·ªâ tr·∫£ v·ªÅ n·ªôi dung ƒë∆∞·ª£c gi·ªõi h·∫°n gi·ªØa c√°c d·∫•u ba d·∫•u nh√°y ƒë∆°n (```) m√† kh√¥ng c√≥ b·∫•t k·ª≥ vƒÉn b·∫£n n√†o kh√°c b√™n ngo√†i.
- N·ªôi dung gi·ªØa c√°c d·∫•u ba d·∫•u nh√°y ƒë∆°n c√≥ th·ªÉ ƒë∆∞·ª£c thay ƒë·ªïi ƒë·ªÉ ph√π h·ª£p v·ªõi y√™u c·∫ßu c·ªßa ng∆∞·ªùi d√πng.
"""
# Default user prompt
default_user_prompt = f"""
B√†i gi·∫£i th√≠ch s·∫Ω bao g·ªìm c√°c ph·∫ßn sau:

- T·ªïng quan: Gi·ªõi thi·ªáu v·ªÅ ch·ªß ƒë·ªÅ v√† t·∫ßm quan tr·ªçng c·ªßa n√≥.
- C√°c ƒëi·ªÉm ch√≠nh: Li·ªát k√™ v√† gi·∫£i th√≠ch c√°c y·∫øu t·ªë quan tr·ªçng c·ªßa ch·ªß ƒë·ªÅ.
- V√≠ d·ª•: Cung c·∫•p v√≠ d·ª• ho·∫∑c tr∆∞·ªùng h·ª£p nghi√™n c·ª©u li√™n quan ƒë·∫øn ch·ªß ƒë·ªÅ.
- Th√°ch th·ª©c: Th·∫£o lu·∫≠n v·ªÅ nh·ªØng th√°ch th·ª©c ho·∫∑c hi·ªÉu l·∫ßm ph·ªï bi·∫øn.
- Best Practices: ƒê·ªÅ xu·∫•t c√°c chi·∫øn l∆∞·ª£c ho·∫∑c m·∫πo hi·ªáu qu·∫£.
- Xu h∆∞·ªõng: X√°c ƒë·ªãnh c√°c xu h∆∞·ªõng hi·ªán t·∫°i ho·∫∑c h∆∞·ªõng ƒëi trong t∆∞∆°ng lai.
- T√†i nguy√™n: ƒê·ªÅ xu·∫•t c√°c t√†i nguy√™n b·ªï sung ƒë·ªÉ t√¨m hi·ªÉu th√™m.
"""


# Available AI Models
available_models = [
    # Input token limit: 1,048,576
    # Output token limit: 65,536
    "gemini-2.5-pro-exp-03-25",

    # Input token limit: 1,048,576
    # Output token limit: 8,192
    "gemini-2.0-flash",

    # Input token limit 2,048,576
    # Output token limit 8,192
    "gemini-2.0-pro-exp",

    "gemini-2.0-flash-thinking-exp",

    # Input token limit 1,048,576
    # Output token limit 8,192
    "gemini-2.0-flash-lite",

    # Input token limit 1,048,576
    # Output token limit 8,192
    "gemini-1.5-flash",

    # Input token limit 1,048,576
    # Output token limit 8,192
    "gemini-1.5-flash-8b",
    "gemini-1.5-pro",
    "learnlm-1.5-pro-experimental",
    "gemma-3-27b-it",
]


def standardize_response(response: str) -> str:
    """ Standardize the response from Gemini API by removing unnecessary parts """

    # Remove first line
    if response.startswith("```"):
        response = response.split("\n", 1)[1]
    # Remove last line
    if response.endswith("```"):
        response = response.rsplit("\n", 1)[0]
    # Remove leading and trailing whitespace
    response = response.strip()

    return response


def get_model():
    # Function to get the selected model
    return st.sidebar.selectbox(
        label="Ch·ªçn m√¥ h√¨nh AI:",
        options=available_models,
        index=available_models.index(
            "gemini-2.5-pro-exp-03-25"),  # Default model
    )


# Get selected model
model = get_model()
# print("Selected model:", model)

# Ti√™u ƒë·ªÅ trang v√† m√¥ t·∫£
st.title("Nghi√™n c·ª©u ch·ªß ƒë·ªÅ")
st.markdown(
    "T·∫°o ra c√°c gi·∫£i th√≠ch chi ti·∫øt v·ªÅ nhi·ªÅu ch·ªß ƒë·ªÅ kh√°c nhau v·ªõi s·ª± h·ªó tr·ª£ c·ªßa AI")


# Set start session time
if 'start_time' not in st.session_state:
    st.session_state.start_time = time.time()
# Prompt customization section
if 'prompt' not in st.session_state:
    st.session_state.research_prompt = default_user_prompt
# Store last response from the AI
if 'last_response' not in st.session_state:
    st.session_state.last_response = ""

# with st.expander("View/Edit Research Prompt", expanded=False):
with st.expander(f"**:blue[Xem/Ch·ªânh s·ª≠a prompt]**", expanded=False):
    st.markdown(
        "ƒê√¢y l√† h∆∞·ªõng d·∫´n g·ª≠i ƒë·∫øn m√¥ h√¨nh AI. B·∫°n c√≥ th·ªÉ s·ª≠a ƒë·ªïi n√≥ ƒë·ªÉ ph√π h·ª£p h∆°n v·ªõi nhu c·∫ßu c·ªßa b·∫°n.")

    # Text area for editing the prompt
    custom_prompt = st.text_area(
        "**B·∫°n c√≥ th·ªÉ t√πy ch·ªânh prompt b√™n d∆∞·ªõi:**",
        # Use the same key as the session state variable
        key=f"1_research_custom_prompt_{st.session_state.start_time}",
        value=st.session_state.research_prompt,
        height=300
    )

    col1, col2 = st.columns(2)

    with col1:
        if st.button("C·∫≠p nh·∫≠t prompt"):
            st.success("ƒê√£ c·∫≠p nh·∫≠t prompt!")
            st.session_state.research_prompt = custom_prompt
            # st.session_state.start_time = time.time()
            # st.rerun()

    with col2:
        if st.button("Kh√¥i ph·ª•c prompt m·∫∑c ƒë·ªãnh"):
            st.success("ƒê√£ kh√¥i ph·ª•c prompt m·∫∑c ƒë·ªãnh!")
            st.session_state.research_prompt = default_user_prompt
            # st.session_state.start_time = time.time()
            # st.rerun()

# # C·∫≠p nh·∫≠t l·∫°i bi·∫øn prompt
# prompt = st.session_state.research_prompt
# # print("Prompt:", prompt)


# Main content
topic = st.text_area(
    "**Nh·∫≠p ch·ªß ƒë·ªÅ b·∫°n mu·ªën nghi√™n c·ª©u:**",
    height=100,
    placeholder="Ch·∫≥ng h·∫°n: ƒê√°nh gi√° m√≥n ƒÉn, N·∫•u ƒÉn, Du l·ªãch, v.v.",
)
# Th·ªÉ hi·ªán 1 v√≠ d·ª• minh h·ªça
with st.expander(label="**V√≠ d·ª• minh h·ªça**", expanded=True):
    st.write("ƒê√°nh gi√° m√≥n ƒÉn")

# print(f"Topic: {topic}")

# Generate content when button is pressed
if st.button("Ti·∫øn h√†nh nghi√™n c·ª©u") and topic:
    try:
        # Research on the topic
        with st.spinner("ƒêang nghi√™n c·ª©u..."):
            # print(prompt % topic)

            client = genai.Client(
                api_key="AIzaSyBYqr4g63GOBTslf5xP0-AbIcSSlAuvMnM")
            response = client.models.generate_content(
                model=model,
                contents=[
                    system_prompt,
                    f"H√£y gi√∫p t√¥i t·∫°o ra m·ªôt b√†i gi·∫£i th√≠ch chi ti·∫øt v·ªÅ ch·ªß ƒë·ªÅ {topic}." +
                    st.session_state.research_prompt,
                ]
            )
            response = standardize_response(response.text)
            st.session_state.last_response = response

        # Display results
        st.subheader("K·∫øt qu·∫£ nghi√™n c·ª©u:", divider="gray")
        # Display the response in a text area
        # st.write(response)
        st.write(st.session_state.last_response)

        # Cho ph√©p ng∆∞·ªùi download n·ªôi dung
        st.download_button(
            label="T·∫£i xu·ªëng n·ªôi dung",
            data=f"# Ch·ªß ƒë·ªÅ: {topic}\n\n{response}",
            file_name="research_topic.md",
            mime="text/markdown",
            help="T·∫£i xu·ªëng n·ªôi dung ƒë√£ nghi√™n c·ª©u d∆∞·ªõi d·∫°ng t·ªáp .md",
        )

        # # Expandable sections for additional info
        # with st.expander('Title History'):
        #     st.info(title_memory.buffer)

        # with st.expander('Script History'):
        #     st.info(script_memory.buffer)

        # with st.expander('Wikipedia Research'):
        #     st.info(wiki_research)

        # # Download options
        # st.download_button(
        #     label="Download Script",
        #     data=f"TITLE: {title}\n\n{script}",
        #     file_name="youtube_script.txt",
        #     mime="text/plain"
        # )

    except Exception as e:
        st.error(f"An error occurred: {e}")

elif topic == "":
    # H∆∞·ªõng d·∫´n nhanh cho ng∆∞·ªùi d√πng
    st.info("H√£y nh·∫≠p ch·ªß ƒë·ªÅ b·∫°n mu·ªën nghi√™n c·ª©u v√† nh·∫•n n√∫t 'Ti·∫øn h√†nh nghi√™n c·ª©u'.")
