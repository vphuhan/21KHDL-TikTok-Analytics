{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U91UVADr4Phh",
        "outputId": "f4a3bd1b-2a17-4e04-dbda-2bfbe125eeda"
      },
      "outputs": [],
      "source": [
        "# %%shell\n",
        "# git clone --branch TrggTin --single-branch https://github.com/vphuhan/21KHDL-TikTok-Analytics.git\n",
        "# cd 21KHDL-TikTok-Analytics\n",
        "# git sparse-checkout init --cone\n",
        "# git sparse-checkout set data/interim\n",
        "# git checkout"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0NZ7_Zrw5PFe",
        "outputId": "d4ed584f-a229-4763-aa43-af3871f0893e"
      },
      "outputs": [],
      "source": [
        "# pip install pandas nltk underthesea scikit-learn tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Imports and Initialization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "7GYwq-Tf4PTa"
      },
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "partially initialized module 'pandas' has no attribute '_pandas_parser_CAPI' (most likely due to a circular import)",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[12], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01municodedata\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\nguye\\.conda\\envs\\AppliedDataProject\\Lib\\site-packages\\pandas\\__init__.py:138\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcomputation\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;28meval\u001b[39m\n\u001b[0;32m    121\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mreshape\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m    122\u001b[0m     concat,\n\u001b[0;32m    123\u001b[0m     lreshape,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    135\u001b[0m     qcut,\n\u001b[0;32m    136\u001b[0m )\n\u001b[1;32m--> 138\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m api, arrays, errors, io, plotting, tseries\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m testing\n\u001b[0;32m    140\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_print_versions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m show_versions\n",
            "File \u001b[1;32mc:\\Users\\nguye\\.conda\\envs\\AppliedDataProject\\Lib\\site-packages\\pandas\\api\\__init__.py:2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\" public toolkit API \"\"\"\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      3\u001b[0m     extensions,\n\u001b[0;32m      4\u001b[0m     indexers,\n\u001b[0;32m      5\u001b[0m     interchange,\n\u001b[0;32m      6\u001b[0m     types,\n\u001b[0;32m      7\u001b[0m     typing,\n\u001b[0;32m      8\u001b[0m )\n\u001b[0;32m     10\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minterchange\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mextensions\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtyping\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     16\u001b[0m ]\n",
            "File \u001b[1;32mc:\\Users\\nguye\\.conda\\envs\\AppliedDataProject\\Lib\\site-packages\\pandas\\api\\typing\\__init__.py:31\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwindow\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     20\u001b[0m     Expanding,\n\u001b[0;32m     21\u001b[0m     ExpandingGroupby,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     26\u001b[0m     Window,\n\u001b[0;32m     27\u001b[0m )\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# TODO: Can't import Styler without importing jinja2\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# from pandas.io.formats.style import Styler\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mjson\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_json\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m JsonReader\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StataReader\n\u001b[0;32m     34\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataFrameGroupBy\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatetimeIndexResamplerGroupby\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWindow\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     55\u001b[0m ]\n",
            "File \u001b[1;32mc:\\Users\\nguye\\.conda\\envs\\AppliedDataProject\\Lib\\site-packages\\pandas\\io\\json\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mjson\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_json\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      2\u001b[0m     read_json,\n\u001b[0;32m      3\u001b[0m     to_json,\n\u001b[0;32m      4\u001b[0m     ujson_dumps,\n\u001b[0;32m      5\u001b[0m     ujson_loads,\n\u001b[0;32m      6\u001b[0m )\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mjson\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_table_schema\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m build_table_schema\n\u001b[0;32m      9\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mujson_dumps\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mujson_loads\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbuild_table_schema\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     15\u001b[0m ]\n",
            "File \u001b[1;32mc:\\Users\\nguye\\.conda\\envs\\AppliedDataProject\\Lib\\site-packages\\pandas\\io\\json\\_json.py:71\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mjson\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_normalize\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m convert_to_line_delimits\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mjson\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_table_schema\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     68\u001b[0m     build_table_schema,\n\u001b[0;32m     69\u001b[0m     parse_table_schema,\n\u001b[0;32m     70\u001b[0m )\n\u001b[1;32m---> 71\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparsers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mreaders\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m validate_integer\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n\u001b[0;32m     74\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcollections\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mabc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     75\u001b[0m         Hashable,\n\u001b[0;32m     76\u001b[0m         Mapping,\n\u001b[0;32m     77\u001b[0m     )\n",
            "File \u001b[1;32mc:\\Users\\nguye\\.conda\\envs\\AppliedDataProject\\Lib\\site-packages\\pandas\\io\\parsers\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparsers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mreaders\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      2\u001b[0m     TextFileReader,\n\u001b[0;32m      3\u001b[0m     TextParser,\n\u001b[0;32m      4\u001b[0m     read_csv,\n\u001b[0;32m      5\u001b[0m     read_fwf,\n\u001b[0;32m      6\u001b[0m     read_table,\n\u001b[0;32m      7\u001b[0m )\n\u001b[0;32m      9\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTextFileReader\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTextParser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread_csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread_fwf\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread_table\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
            "File \u001b[1;32mc:\\Users\\nguye\\.conda\\envs\\AppliedDataProject\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:32\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_config\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m using_copy_on_write\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m lib\n\u001b[1;32m---> 32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparsers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m STR_NA_VALUES\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01merrors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     34\u001b[0m     AbstractMethodError,\n\u001b[0;32m     35\u001b[0m     ParserWarning,\n\u001b[0;32m     36\u001b[0m )\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_decorators\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Appender\n",
            "File \u001b[1;32mparsers.pyx:1418\u001b[0m, in \u001b[0;36minit pandas._libs.parsers\u001b[1;34m()\u001b[0m\n",
            "\u001b[1;31mAttributeError\u001b[0m: partially initialized module 'pandas' has no attribute '_pandas_parser_CAPI' (most likely due to a circular import)"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import unicodedata\n",
        "import nltk\n",
        "from underthesea import word_tokenize, pos_tag, ner\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from difflib import get_close_matches\n",
        "import logging\n",
        "import json\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import string\n",
        "import regex as re\n",
        "import traceback\n",
        "import jdc  \n",
        "from spellchecker import SpellChecker\n",
        "from datetime import datetime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pfZUR8vY4POT"
      },
      "outputs": [],
      "source": [
        "# Configure logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
        "    handlers=[\n",
        "        logging.FileHandler(\"extraction_log.log\"),\n",
        "        logging.StreamHandler()\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aDuHFYPU6NKm",
        "outputId": "2a87ddda-2e79-4d97-ed72-1a24c515c520"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt_tab to\n",
            "[nltk_data]     C:\\Users\\nguye\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 207,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# VietnameseTextProcessor Class Definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class VietnameseTextProcessor:\n",
        "    def __init__(self, food_list_path=None, location_list_path=None):\n",
        "        \"\"\"\n",
        "        Khởi tạo Bộ xử lý văn bản tiếng Việt\n",
        "\n",
        "        Tham số:\n",
        "            food_list_path (str): Đường dẫn đến tệp JSON chứa danh sách món ăn Việt Nam\n",
        "            location_list_path (str): Đường dẫn đến tệp JSON chứa danh sách địa điểm ở Việt Nam\n",
        "        \"\"\"\n",
        "        # Tải hoặc khởi tạo danh sách món ăn và địa điểm\n",
        "        self.foods = self._load_entity_list(food_list_path, \"foods\")\n",
        "        self.locations = self._load_entity_list(location_list_path, \"locations\")\n",
        "\n",
        "        # Các từ khóa phổ biến liên quan đến món ăn và hương vị trong tiếng Việt để hỗ trợ nhận diện\n",
        "        self.food_indicators = [\n",
        "            \"bánh mì\", \"phở\", \"bún\", \"xèo\", \"cơm\", \"gỏi\", \"chả\", \"xôi\", \"cao lầu\", \"cháo\",\n",
        "            \"mì gói\", \"hủ tiếu\", \"nem\", \"chả ram\", \"bánh khọt\",\n",
        "            \"lẩu\", \"cá\", \"thịt\", \"canh\", \"rau\", \"đậu\", \"ốc\", \"súp\", \"bắp\", \"lươn\", \"măng\", \"nấm\",\n",
        "            \"chuối\", \"nộm\", \"trà\", \"cà phê\", \"sinh tố\", \"kem\", \"tàu hủ\", \"chè\", \"yaourt\", \"nước mía\",\n",
        "            \"sữa\", \"kẹo\", \"đa\", \"nem chua\", \"gà\", \"bò\", \"heo\", \"vịt\", \"cá\", \"tôm\", \"mực, ốc\", \"sò\", \"hàu\",\n",
        "            \"bún riêu\", \"bún bò\", \"bún mắm\", \"bún mọc\", \"bún chả\", \"bún đậu\", \"bún ốc\"\n",
        "        ]\n",
        "\n",
        "        self.taste_indicators = [\n",
        "            \"ngon\", \"ngọt\", \"chua\", \"cay\", \"đắng\", \"mặn\", \"bùi\", \"béo\", \"giòn\", \"mềm\",\n",
        "            \"thơm\", \"nồng\", \"đậm đà\", \"nhạt\", \"thanh\", \"tươi\", \"chát\", \"cay nồng\", \"cay nhẹ\", \"cay vừa\",\n",
        "            \"sần sật\", \"mọng nước\", \"đắng nghét\", \"chát\", \"cay xè\", \"tê\", \"mặn chát\", \"ngọt lịm\", \"béo ngậy\", \"thơm lừng\",\n",
        "            \"nồng nàn\", \"đậm vị\", \"nhạt nhẽo\", \"thanh mát\", \"tươi\", \"đậm đà hương vị\", \"vừa ăn\", \"hợp khẩu vị\"\n",
        "        ]\n",
        "\n",
        "        self.locations_indicators = [ \n",
        "            \"quận 1\", \"quận 2\", \"quận 3\", \"quận 4\", \"quận 5\", \"quận 6\", \"quận 7\", \"quận 8\", \"quận 9\", \"quận 10\",\n",
        "            \"quận 11\", \"quận 12\", \"bình thạnh\", \"tân bình\", \"tân phú\", \"phú nhuận\", \"gò vấp\", \"bình tân\", \"thủ đức\", \"hóc môn\",\n",
        "            \"củ chi\", \"nhà bè\", \"cần giờ\", \"bình chánh\", \"tp thủ đức\",\n",
        "            \"hà nội\", \"hồ chí minh\", \"đà nẵng\", \"hải phòng\", \"cần thơ\", \"huế\", \"nha trang\", \"vũng tàu\", \"đà lạt\",\n",
        "            \"hạ long\", \"mỹ tho\", \"long xuyên\", \"rạch giá\", \"cà mau\", \"biên hòa\", \"buôn ma thuột\", \"thái nguyên\", \"nam định\"\n",
        "        ]\n",
        "\n",
        "\n",
        "        # Tải các tài nguyên của NLTK nếu cần\n",
        "        try:\n",
        "            nltk.data.find('tokenizers/punkt')\n",
        "        except LookupError:\n",
        "            nltk.download('punkt')\n",
        "\n",
        "        # Tạo thư mục để lưu trữ các tệp dữ liệu được trích xuất\n",
        "        os.makedirs(\"extracted_data\", exist_ok=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Helper Methods"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def _load_entity_list(self, file_path, entity_type):\n",
        "    \"\"\"Tải danh sách thực thể từ tệp hoặc trả về tập rỗng mặc định\"\"\"\n",
        "    if file_path and os.path.exists(file_path):\n",
        "        try:\n",
        "            with open(file_path, 'r', encoding='utf-8') as f:\n",
        "                return set(json.load(f))\n",
        "        except Exception as e:\n",
        "            logging.warning(f\"Lỗi khi tải danh sách {entity_type}: {e}\")\n",
        "\n",
        "    logging.info(f\"Không tìm thấy danh sách {entity_type} hiện có, bắt đầu với tập rỗng\")\n",
        "    return set()\n",
        "\n",
        "def save_entity_list(self, entity_list, entity_type):\n",
        "    \"\"\"Lưu danh sách thực thể đã cập nhật vào tệp\"\"\"\n",
        "    file_path = f\"extracted_data/{entity_type}_list.json\"\n",
        "    with open(file_path, 'w', encoding='utf-8') as f:\n",
        "        json.dump(list(entity_list), f, ensure_ascii=False, indent=2)\n",
        "    logging.info(f\"Đã lưu {len(entity_list)} {entity_type} vào {file_path}\")\n",
        "\n",
        "def normalize_vietnamese_text(self, text):\n",
        "    \"\"\"Chuẩn hóa văn bản tiếng Việt bằng cách xử lý dấu và chữ hoa/thường\"\"\"\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "\n",
        "    # Chuẩn hóa ký tự Unicode\n",
        "    text = unicodedata.normalize('NFC', text)\n",
        "\n",
        "    # Loại bỏ khoảng trắng thừa\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    return text\n",
        "\n",
        "def clean_text(self, text):\n",
        "    \"\"\"Làm sạch văn bản bằng cách loại bỏ ký tự đặc biệt và chuẩn hóa\"\"\"\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "\n",
        "    # Chuẩn hóa văn bản\n",
        "    text = self.normalize_vietnamese_text(text)\n",
        "    text = text.lower()\n",
        "\n",
        "    # Loại bỏ đường dẫn URL\n",
        "    text = re.sub(r'https?://\\S+|www\\.\\S+|\\S+@\\S+\\.\\S+', '', text)\n",
        "    text = re.sub(r'@\\w+|#\\w+', '', text)\n",
        "\n",
        "    # Loại bỏ biểu tượng cảm xúc và ký tự đặc biệt trong khi giữ lại chữ tiếng Việt\n",
        "    emoji_pattern = re.compile(\"[\"\n",
        "            u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "            u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "            u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "            u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "            u\"\\U00002702-\\U000027B0\"  # dingbats\n",
        "            u\"\\U000024C2-\\U0001F251\" \n",
        "            \"]+\", flags=re.UNICODE)\n",
        "    text = emoji_pattern.sub('', text)\n",
        "\n",
        "    symbols_to_remove = [\n",
        "            '!', '\"', '#', '$', '%', '&', \"'\", '*', '+', ',', \n",
        "            '-', '.', '/', ':', ';', '<', '=', '>', '?', '@', \n",
        "            '[', '\\\\', ']', '^', '_', '`', '{', '|', '}', '~',\n",
        "            '\"', '\"', ''', ''', '…', '–', '—', '•', '′', '″',\n",
        "            '„', '«', '»', '‹', '›', '⟨', '⟩', '〈', '〉'\n",
        "    ]\n",
        "    \n",
        "    # Create a pattern that excludes Vietnamese diacritics\n",
        "    pattern = f'[{\"\".join(map(re.escape, symbols_to_remove))}]'\n",
        "    text = re.sub(pattern, ' ', text)\n",
        "\n",
        "    # Handle ellipsis and multiple dots\n",
        "    text = re.sub(r'\\.{2,}', ' ', text)\n",
        "\n",
        "    # Handle multiple spaces and normalize whitespace\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "\n",
        "    # Handle parentheses and brackets\n",
        "    text = re.sub(r'[\\(\\)\\[\\]\\{\\}⟨⟩〈〉]', ' ', text)\n",
        "\n",
        "    # Clean up extra spaces around Vietnamese words\n",
        "    text = re.sub(r'\\s+([^\\w\\s])|([^\\w\\s])\\s+', r'\\1\\2', text)\n",
        "\n",
        "    # Final whitespace cleanup\n",
        "    text = text.strip()\n",
        "\n",
        "    return text\n",
        "\n",
        "def auto_correct_text(self, text):\n",
        "    \"\"\"Tự động sửa lỗi chính tả bằng bộ kiểm tra chính tả\"\"\"\n",
        "    spell = SpellChecker(language='vi')\n",
        "    words = word_tokenize(text)\n",
        "    corrected_words = [spell.correction(word) for word in words]\n",
        "    return \" \".join(corrected_words)\n",
        "\n",
        "def load_stopwords(self, file_path):\n",
        "    \"\"\"Tải danh sách từ dừng từ tệp\"\"\"\n",
        "    if file_path and os.path.exists(file_path):\n",
        "        try:\n",
        "            with open(file_path, 'r', encoding='utf-8') as f:\n",
        "                return set(f.read().splitlines())\n",
        "        except Exception as e:\n",
        "            logging.warning(f\"Lỗi khi tải danh sách từ dừng: {e}\")\n",
        "    logging.info(\"Không tìm thấy tệp từ dừng, bắt đầu với tập rỗng\")\n",
        "    return set()\n",
        "\n",
        "def remove_stopwords(self, text, stopwords):\n",
        "    \"\"\"Loại bỏ từ dừng khỏi văn bản\"\"\"\n",
        "    words = word_tokenize(text)\n",
        "    filtered_words = [word for word in words if word.lower() not in stopwords]\n",
        "    return \" \".join(filtered_words)\n",
        "\n",
        "def preprocess_text(self, text):\n",
        "    \"\"\"Áp dụng tất cả các bước tiền xử lý lên văn bản\"\"\"\n",
        "    try:\n",
        "        text = self.clean_text(text)\n",
        "        text = self.auto_correct_text(text)  # Đã sửa lỗi tại đây\n",
        "        stopwords = self.load_stopwords('vietnamese-stopwords.txt')\n",
        "        text = self.remove_stopwords(text, stopwords)\n",
        "        return text\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Lỗi khi tiền xử lý văn bản: {e}\")\n",
        "        return text if isinstance(text, str) else \"\"\n",
        "\n",
        "# Gán các phương thức vào lớp VietnameseTextProcessor\n",
        "VietnameseTextProcessor._load_entity_list = _load_entity_list\n",
        "VietnameseTextProcessor.save_entity_list = save_entity_list \n",
        "VietnameseTextProcessor.normalize_vietnamese_text = normalize_vietnamese_text\n",
        "VietnameseTextProcessor.clean_text = clean_text\n",
        "VietnameseTextProcessor.auto_correct_text = auto_correct_text\n",
        "VietnameseTextProcessor.load_stopwords = load_stopwords\n",
        "VietnameseTextProcessor.preprocess_text = preprocess_text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def test_text_processing():\n",
        "    \"\"\"Test function to demonstrate all text preprocessing and cleaning steps\"\"\"\n",
        "    \n",
        "    # Initialize the processor\n",
        "    processor = VietnameseTextProcessor()\n",
        "    \n",
        "    # Test text with various cases to check\n",
        "    test_text = \"\"\"\n",
        "    🔥 Quán Phở ngon ở Quận 1 TPHCM! https://example.com\n",
        "    Món phở bò tái nạm gầu cực kỳ ngon, nước dùng đậm đà...\n",
        "    Địa chỉ: 123 Lê Lợi, P. Bến Nghé, Quận 1, TP.HCM\n",
        "    #pho #amthuc #reviewdoan @foodblogger\n",
        "    \"\"\"\n",
        "    \n",
        "    print(\"Original Text:\")\n",
        "    print(\"-\" * 50)\n",
        "    print(test_text)\n",
        "    print(\"\\n\")\n",
        "\n",
        "    # Test normalize_vietnamese_text\n",
        "    print(\"1. After Vietnamese Text Normalization:\")\n",
        "    print(\"-\" * 50)\n",
        "    normalized = processor.normalize_vietnamese_text(test_text)\n",
        "    print(normalized)\n",
        "    print(\"\\n\")\n",
        "\n",
        "    # Test clean_text\n",
        "    print(\"2. After Text Cleaning:\")\n",
        "    print(\"-\" * 50)\n",
        "    cleaned = processor.clean_text(test_text)\n",
        "    print(cleaned)\n",
        "    print(\"\\n\")\n",
        "\n",
        "    # Test stopwords removal\n",
        "    print(\"3. After Stopwords Removal:\")\n",
        "    print(\"-\" * 50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-03-09 23:28:59,300 - INFO - Không tìm thấy danh sách foods hiện có, bắt đầu với tập rỗng\n",
            "2025-03-09 23:28:59,300 - INFO - Không tìm thấy danh sách locations hiện có, bắt đầu với tập rỗng\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original Text:\n",
            "--------------------------------------------------\n",
            "\n",
            "    🔥 Quán Phở ngon ở Q.1 TPHCM! https://example.com\n",
            "    Món phở bò tái nạm gầu cực kỳ ngon, nước dùng đậm đà...\n",
            "    Địa chỉ: 123 Lê Lợi, P. Bến Nghé, Quận 1, TP.HCM\n",
            "    #pho #amthuc #reviewdoan @foodblogger\n",
            "    \n",
            "\n",
            "\n",
            "1. After Vietnamese Text Normalization:\n",
            "--------------------------------------------------\n",
            "🔥 Quán Phở ngon ở Q.1 TPHCM! https://example.com Món phở bò tái nạm gầu cực kỳ ngon, nước dùng đậm đà... Địa chỉ: 123 Lê Lợi, P. Bến Nghé, Quận 1, TP.HCM #pho #amthuc #reviewdoan @foodblogger\n",
            "\n",
            "\n",
            "2. After Text Cleaning:\n",
            "--------------------------------------------------\n",
            "quán phở ngon ở q 1 tphcm món phở bò tái nạm gầu cực kỳ ngon nước dùng đậm đà địa chỉ 123 lê lợi p bến nghé quận 1 tp hcm\n",
            "\n",
            "\n",
            "3. After Stopwords Removal:\n",
            "--------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "test_text_processing()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Entity Extraction Methods"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_entities_from_ner(self, text):\n",
        "    \"\"\"Trích xuất thực thể từ văn bản bằng Named Entity Recognition (NER) của underthesea.\"\"\"\n",
        "    locations = []\n",
        "\n",
        "    try:\n",
        "        ner_tags = ner(text)  # Thực hiện nhận dạng thực thể có tên (NER)\n",
        "\n",
        "        # Kiểm tra nếu kết quả từ NER có định dạng mong đợi\n",
        "        if not isinstance(ner_tags, list):\n",
        "            return locations\n",
        "\n",
        "        # Trích xuất các địa điểm từ NER\n",
        "        current_loc = []\n",
        "\n",
        "        for item in ner_tags:\n",
        "            # Xử lý các định dạng đầu ra khác nhau từ NER\n",
        "            if isinstance(item, (list, tuple)) and len(item) == 2:\n",
        "                word, tag = item\n",
        "            else:\n",
        "                continue\n",
        "\n",
        "            if tag.startswith('B-LOC'):\n",
        "                if current_loc:\n",
        "                    locations.append(' '.join(current_loc))\n",
        "                    current_loc = []\n",
        "                current_loc.append(word)\n",
        "            elif tag.startswith('I-LOC') and current_loc:\n",
        "                current_loc.append(word)\n",
        "            elif current_loc:\n",
        "                locations.append(' '.join(current_loc))\n",
        "                current_loc = []\n",
        "\n",
        "        # Thêm thực thể địa điểm cuối cùng nếu có\n",
        "        if current_loc:\n",
        "            locations.append(' '.join(current_loc))\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Lỗi khi trích xuất thực thể bằng NER: {e}\")\n",
        "        logging.error(traceback.format_exc())\n",
        "\n",
        "    return locations\n",
        "\n",
        "def extract_entities_from_patterns(self, text, sentences, pos_tags):\n",
        "    \"\"\"Trích xuất thực thể bằng cách sử dụng phương pháp dựa trên mẫu (Pattern Matching).\"\"\"\n",
        "    foods = []\n",
        "    locations = []\n",
        "    tastes = []\n",
        "\n",
        "    # Xử lý từng câu để trích xuất thực thể\n",
        "    for idx, sentence in enumerate(sentences):\n",
        "        words = word_tokenize(sentence)\n",
        "        sentence_pos_tags = pos_tags[idx] if idx < len(pos_tags) else []\n",
        "\n",
        "        # Tìm thực thể về thực phẩm\n",
        "        self._extract_food_entities(sentence, sentence_pos_tags, foods)\n",
        "\n",
        "        # Tìm thực thể về địa điểm\n",
        "        self._extract_location_entities(sentence, sentence_pos_tags, locations)\n",
        "\n",
        "        # Tìm mô tả về hương vị\n",
        "        self._extract_taste_descriptions(sentence, words, tastes)\n",
        "\n",
        "    return foods, locations, tastes\n",
        "\n",
        "def _extract_food_entities(self, sentence, pos_tags, foods):\n",
        "    \"\"\"Trích xuất thực thể thực phẩm từ một câu.\"\"\"\n",
        "    # Kiểm tra danh sách thực phẩm có sẵn\n",
        "    for food in self.foods:\n",
        "        if food.lower() in sentence.lower():\n",
        "            foods.append(food)\n",
        "\n",
        "    # Tìm các từ chỉ thực phẩm\n",
        "    for idx, (word, tag) in enumerate(pos_tags):\n",
        "        if word.lower() in self.food_indicators:\n",
        "            noun_phrase = [word]\n",
        "            for i in range(1, 4):\n",
        "                if idx + i < len(pos_tags):\n",
        "                    next_word, next_tag = pos_tags[idx + i]\n",
        "                    if next_tag.startswith(('N', 'A')):  # Danh từ hoặc Tính từ\n",
        "                        noun_phrase.append(next_word)\n",
        "                    else:\n",
        "                        break\n",
        "            \n",
        "            if noun_phrase:\n",
        "                food_name = \" \".join(noun_phrase)\n",
        "                foods.append(food_name)\n",
        "                self.foods.add(food_name)\n",
        "\n",
        "def _extract_location_entities(self, sentence, pos_tags, locations):\n",
        "    \"\"\"Trích xuất thực thể địa điểm từ một câu.\"\"\"\n",
        "    # Kiểm tra danh sách địa điểm có sẵn\n",
        "    for location in self.locations:\n",
        "        if location.lower() in sentence.lower():\n",
        "            locations.append(location)\n",
        "\n",
        "    # Tìm các từ chỉ địa điểm\n",
        "    for idx, (word, tag) in enumerate(pos_tags):\n",
        "        if any(indicator.lower() in word.lower() for indicator in self.locations_indicators):\n",
        "            noun_phrase = [word]\n",
        "            for i in range(1, 4):\n",
        "                if idx + i < len(pos_tags):\n",
        "                    next_word, next_tag = pos_tags[idx + i]\n",
        "                    if next_tag.startswith(('N', 'M', 'Np')):  # Danh từ, Số, Danh từ riêng\n",
        "                        noun_phrase.append(next_word)\n",
        "                    else:\n",
        "                        break\n",
        "            \n",
        "            if noun_phrase:\n",
        "                location_name = \" \".join(noun_phrase)\n",
        "                locations.append(location_name)\n",
        "                self.locations.add(location_name)\n",
        "\n",
        "def _extract_taste_descriptions(self, sentence, words, tastes):\n",
        "    \"\"\"Trích xuất mô tả về hương vị từ một câu.\"\"\"\n",
        "    for taste_word in self.taste_indicators:\n",
        "        if taste_word in sentence.lower():\n",
        "            taste_idx = -1\n",
        "            for idx, word in enumerate(words):\n",
        "                if taste_word in word.lower():\n",
        "                    taste_idx = idx\n",
        "                    break\n",
        "            \n",
        "            if taste_idx >= 0:\n",
        "                start = max(0, taste_idx - 1)  # Lấy tối đa 1 từ trước\n",
        "                end = min(len(words), taste_idx + 2)  # Lấy tối đa 2 từ sau\n",
        "                taste_phrase = \" \".join(words[start:end])\n",
        "\n",
        "                # Đảm bảo cụm từ có ít nhất 2 từ và tối đa 3 từ\n",
        "                if 2 <= len(taste_phrase.split()) <= 3:\n",
        "                    tastes.append(taste_phrase)\n",
        "\n",
        "def extract_entities(self, text):\n",
        "    \"\"\"Trích xuất các thực thể về thực phẩm, địa điểm và hương vị từ văn bản.\"\"\"\n",
        "    if not text or not isinstance(text, str):\n",
        "        return {\"foods\": [], \"locations\": [], \"tastes\": []}\n",
        "\n",
        "    try:\n",
        "        results = {\"foods\": [], \"locations\": [], \"tastes\": []}\n",
        "\n",
        "        # Trích xuất địa điểm bằng NER\n",
        "        ner_locations = self.extract_entities_from_ner(text)\n",
        "        results[\"locations\"].extend(ner_locations)\n",
        "        self.locations.update(ner_locations)\n",
        "\n",
        "        # Trích xuất thực thể bằng phương pháp dựa trên mẫu\n",
        "        sentences = nltk.sent_tokenize(text)\n",
        "        pos_tags = [pos_tag(sent) for sent in sentences]\n",
        "\n",
        "        foods, locations, tastes = self.extract_entities_from_patterns(text, sentences, pos_tags)\n",
        "\n",
        "        results[\"foods\"].extend(foods)\n",
        "        results[\"locations\"].extend(locations)\n",
        "        results[\"tastes\"].extend(tastes)\n",
        "\n",
        "        # Cập nhật danh sách thực thể\n",
        "        self.foods.update(foods)\n",
        "        self.locations.update(locations)\n",
        "\n",
        "        # Loại bỏ trùng lặp và lọc bỏ chuỗi rỗng\n",
        "        for key in results:\n",
        "            results[key] = list(set(filter(None, results[key])))\n",
        "\n",
        "        return results\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Lỗi khi trích xuất thực thể: {e}\")\n",
        "        logging.error(traceback.format_exc())\n",
        "        return {\"foods\": [], \"locations\": [], \"tastes\": []}\n",
        "    \n",
        "VietnameseTextProcessor.extract_entities_from_ner = extract_entities_from_ner\n",
        "VietnameseTextProcessor.extract_entities_from_patterns = extract_entities_from_patterns\n",
        "VietnameseTextProcessor._extract_food_entities = _extract_food_entities\n",
        "VietnameseTextProcessor._extract_location_entities = _extract_location_entities\n",
        "VietnameseTextProcessor._extract_taste_descriptions = _extract_taste_descriptions\n",
        "VietnameseTextProcessor.extract_entities = extract_entities\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# DataFrame Processing and Bootstrapping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def process_dataframe(self, df, text_column=\"video_transcription\", batch_size=100):\n",
        "    \"\"\"\n",
        "    Xử lý toàn bộ DataFrame và trích xuất các thực thể.\n",
        "\n",
        "    Tham số:\n",
        "        df (pd.DataFrame): DataFrame chứa dữ liệu văn bản.\n",
        "        text_column (str): Tên cột chứa văn bản.\n",
        "        batch_size (int): Kích thước batch để xử lý nhằm tiết kiệm bộ nhớ.\n",
        "\n",
        "    Trả về:\n",
        "        pd.DataFrame: DataFrame gốc với các cột chứa thực thể được trích xuất.\n",
        "    \"\"\"\n",
        "    # Kiểm tra nếu DataFrame trống hoặc không có cột văn bản\n",
        "    if df.empty or text_column not in df.columns:\n",
        "        logging.error(f\"DataFrame không hợp lệ hoặc thiếu cột '{text_column}'\")\n",
        "        return df\n",
        "\n",
        "    # Tạo thư mục lưu trữ nếu chưa tồn tại\n",
        "    os.makedirs(\"extracted_data\", exist_ok=True)\n",
        "\n",
        "    # Khởi tạo các cột để lưu thực thể trích xuất\n",
        "    df['preprocessed_text'] = \"\"\n",
        "    df['extracted_foods'] = None\n",
        "    df['extracted_locations'] = None\n",
        "    df['extracted_tastes'] = None\n",
        "\n",
        "    total_batches = (len(df) + batch_size - 1) // batch_size  # Tính số batch cần xử lý\n",
        "\n",
        "    for i in tqdm(range(total_batches), desc=\"Đang xử lý batch\"):\n",
        "        start_idx = i * batch_size\n",
        "        end_idx = min((i + 1) * batch_size, len(df))\n",
        "\n",
        "        batch = df.iloc[start_idx:end_idx].copy()\n",
        "\n",
        "        # Tiền xử lý văn bản\n",
        "        batch['preprocessed_text'] = batch[text_column].apply(self.preprocess_text)\n",
        "\n",
        "        # Trích xuất thực thể\n",
        "        entities_list = []\n",
        "        for text in batch['preprocessed_text']:\n",
        "            entities_list.append(self.extract_entities(text))\n",
        "\n",
        "        # Cập nhật DataFrame với thực thể trích xuất\n",
        "        batch['extracted_foods'] = [data['foods'] for data in entities_list]\n",
        "        batch['extracted_locations'] = [data['locations'] for data in entities_list]\n",
        "        batch['extracted_tastes'] = [data['tastes'] for data in entities_list]\n",
        "\n",
        "        # Cập nhật vào DataFrame gốc\n",
        "        df.iloc[start_idx:end_idx] = batch\n",
        "\n",
        "        # Lưu kết quả tạm thời theo từng batch\n",
        "        if (i + 1) % 5 == 0 or (i + 1) == total_batches:\n",
        "            self.save_entity_list(self.foods, \"foods\")\n",
        "            self.save_entity_list(self.locations, \"locations\")\n",
        "\n",
        "            # Lưu kết quả trung gian\n",
        "            checkpoint_file = f\"extracted_data/processed_data_batch_{i+1}.csv\"\n",
        "            df.iloc[:end_idx].to_csv(checkpoint_file, index=False)\n",
        "            logging.info(f\"Đã lưu kết quả trung gian vào {checkpoint_file} sau batch {i+1}/{total_batches}\")\n",
        "\n",
        "    # Thống kê số lượng thực thể đã tìm thấy\n",
        "    food_count = len(self.foods)\n",
        "    location_count = len(self.locations)\n",
        "\n",
        "    logging.info(f\"Trích xuất hoàn tất. Tìm thấy {food_count} thực thể món ăn và {location_count} thực thể địa điểm.\")\n",
        "\n",
        "    return df\n",
        "\n",
        "def bootstrap_entity_lists(self, df, text_column=\"preprocessed_text\", min_freq=3):\n",
        "    \"\"\"\n",
        "    Mở rộng danh sách thực thể bằng TF-IDF để tìm các thực thể tiềm năng.\n",
        "    \n",
        "    Tham số:\n",
        "        df (pd.DataFrame): DataFrame chứa dữ liệu văn bản.\n",
        "        text_column (str): Tên cột chứa văn bản đã tiền xử lý.\n",
        "        min_freq (int): Số lần xuất hiện tối thiểu để xem xét một thực thể.\n",
        "\n",
        "    Trả về:\n",
        "        set: Tập hợp các thực thể món ăn mới được nhận diện.\n",
        "    \"\"\"\n",
        "    if df.empty or text_column not in df.columns:\n",
        "        logging.error(f\"Không thể mở rộng thực thể: DataFrame không hợp lệ hoặc thiếu cột '{text_column}'\")\n",
        "        return set()\n",
        "\n",
        "    # Lọc ra các văn bản hợp lệ\n",
        "    valid_texts = df[text_column].dropna().replace('', pd.NA).dropna().tolist()\n",
        "\n",
        "    if not valid_texts:\n",
        "        logging.warning(\"Không tìm thấy văn bản hợp lệ để mở rộng thực thể\")\n",
        "        return set()\n",
        "\n",
        "    try:\n",
        "        min_df_val = max(1, min(min_freq, len(valid_texts) // 2))\n",
        "        \n",
        "        tfidf = TfidfVectorizer(\n",
        "            ngram_range=(1, 3),  # Xét các n-gram từ 1 đến 3 từ\n",
        "            min_df=min_df_val,  # Điều chỉnh min_df\n",
        "            max_df=0.7  # Loại bỏ các cụm từ quá phổ biến\n",
        "        )\n",
        "\n",
        "        tfidf_matrix = tfidf.fit_transform(valid_texts)\n",
        "        feature_names = tfidf.get_feature_names_out()\n",
        "\n",
        "        # Lấy danh sách n-gram có giá trị TF-IDF cao\n",
        "        important_ngrams = []\n",
        "        for i in range(min(tfidf_matrix.shape[0], 100)):\n",
        "            feature_index = tfidf_matrix[i,:].nonzero()[1]\n",
        "            tfidf_scores = zip(feature_index, [tfidf_matrix[i, x] for x in feature_index])\n",
        "            # Sắp xếp theo điểm TF-IDF giảm dần\n",
        "            for idx, score in sorted(tfidf_scores, key=lambda x: x[1], reverse=True)[:20]:\n",
        "                important_ngrams.append(feature_names[idx])\n",
        "\n",
        "        # Lọc các cụm từ có thể là tên món ăn (dựa vào từ chỉ món ăn)\n",
        "        potential_foods = set()\n",
        "        for text in valid_texts:\n",
        "            for indicator in self.food_indicators:\n",
        "                if indicator in text:\n",
        "                    for ngram in important_ngrams:\n",
        "                        # Kiểm tra nếu ngram xuất hiện gần từ chỉ món ăn\n",
        "                        if ngram in text and re.search(r'\\b' + re.escape(indicator) + r'.{0,30}' + re.escape(ngram), text, re.IGNORECASE):\n",
        "                            potential_foods.add(ngram)\n",
        "                        if ngram in text and re.search(r'\\b' + re.escape(ngram) + r'.{0,30}' + re.escape(indicator), text, re.IGNORECASE):\n",
        "                            potential_foods.add(ngram)\n",
        "\n",
        "        # Lọc bỏ các thực thể không hợp lệ (quá ngắn, chỉ chứa số, v.v.)\n",
        "        filtered_foods = {food for food in potential_foods if len(food) > 2 and not food.isdigit()}\n",
        "\n",
        "        # Cập nhật danh sách món ăn\n",
        "        self.foods.update(filtered_foods)\n",
        "        logging.info(f\"Đã thêm {len(filtered_foods)} thực thể món ăn tiềm năng từ mở rộng thực thể\")\n",
        "\n",
        "        return filtered_foods\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Lỗi khi mở rộng thực thể: {e}\")\n",
        "        logging.error(traceback.format_exc())\n",
        "        return set()\n",
        "\n",
        "VietnameseTextProcessor.process_dataframe = process_dataframe\n",
        "VietnameseTextProcessor.bootstrap_entity_lists = bootstrap_entity_lists"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YI6pKb0WGWVC",
        "outputId": "174f413c-1aac-4c97-9d7b-a886256d1cf2"
      },
      "outputs": [],
      "source": [
        "def main():\n",
        "    try:\n",
        "        # Tạo một thể hiện của bộ xử lý văn bản\n",
        "        processor = VietnameseTextProcessor()\n",
        "\n",
        "        # Tải tập dữ liệu\n",
        "        logging.info(\"Đang tải tập dữ liệu...\")\n",
        "        try:\n",
        "            # df = pd.read_csv(\"/content/21KHDL-TikTok-Analytics/data/interim/small_video_transcription.csv\")\n",
        "            df = pd.read_csv(\"C:/Users/nguye/OneDrive/Tài liệu/GitHub/21KHDL-TikTok-Analytics/data/interim/small_video_transcription.csv\")\n",
        "            if df.empty:\n",
        "                logging.error(\"Tập dữ liệu được tải về trống\")\n",
        "                return\n",
        "            logging.info(f\"Tập dữ liệu đã tải có {len(df)} dòng\")\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Lỗi khi tải tập dữ liệu: {e}\")\n",
        "            logging.error(traceback.format_exc())\n",
        "            return\n",
        "\n",
        "        # Xử lý một mẫu nhỏ để kiểm thử (sử dụng .head(10) để thử nghiệm, xóa bỏ để xử lý toàn bộ)\n",
        "        sample_df = df.head(20)\n",
        "\n",
        "        # Xử lý dữ liệu văn bản\n",
        "        logging.info(\"Bắt đầu xử lý văn bản và trích xuất thực thể...\")\n",
        "        processed_df = processor.process_dataframe(sample_df, text_column='video_transcription')\n",
        "\n",
        "        # Mở rộng danh sách thực thể bằng phương pháp bootstrapping\n",
        "        logging.info(\"Thực hiện bootstrapping để mở rộng danh sách thực thể...\")\n",
        "        processor.bootstrap_entity_lists(processed_df)\n",
        "\n",
        "        # Lưu kết quả cuối cùng\n",
        "        processed_df.to_csv(\"extracted_data/fully_processed_data.csv\", index=False)\n",
        "        processor.save_entity_list(processor.foods, \"foods\")\n",
        "        processor.save_entity_list(processor.locations, \"locations\")\n",
        "\n",
        "        # Lưu kết quả có cấu trúc dưới dạng JSON gồm video_id, author_id và các thực thể trích xuất\n",
        "        structured_data = []\n",
        "        for _, row in processed_df.iterrows():\n",
        "            structured_data.append({\n",
        "                'video_id': row.get('video_id', ''),\n",
        "                'author_id': row.get('author_id', ''),\n",
        "                'extracted_entities': {\n",
        "                    'foods': row.get('extracted_foods', []),\n",
        "                    'locations': row.get('extracted_locations', []),\n",
        "                    'tastes': row.get('extracted_tastes', [])\n",
        "                }\n",
        "            })\n",
        "\n",
        "        with open(\"extracted_data/structured_entities.json\", 'w', encoding='utf-8') as f:\n",
        "            json.dump(structured_data, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "        logging.info(\"Quá trình xử lý hoàn tất. Kết quả đã được lưu trong thư mục 'extracted_data'.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Lỗi nghiêm trọng trong hàm main: {e}\")\n",
        "        logging.error(traceback.format_exc())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-03-09 23:28:59,374 - INFO - Không tìm thấy danh sách foods hiện có, bắt đầu với tập rỗng\n",
            "2025-03-09 23:28:59,375 - INFO - Không tìm thấy danh sách locations hiện có, bắt đầu với tập rỗng\n",
            "2025-03-09 23:28:59,377 - INFO - Đang tải tập dữ liệu...\n",
            "2025-03-09 23:28:59,651 - INFO - Tập dữ liệu đã tải có 10673 dòng\n",
            "2025-03-09 23:28:59,652 - INFO - Bắt đầu xử lý văn bản và trích xuất thực thể...\n",
            "C:\\Users\\nguye\\AppData\\Local\\Temp\\ipykernel_13796\\74775411.py:22: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df['preprocessed_text'] = \"\"\n",
            "C:\\Users\\nguye\\AppData\\Local\\Temp\\ipykernel_13796\\74775411.py:23: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df['extracted_foods'] = None\n",
            "C:\\Users\\nguye\\AppData\\Local\\Temp\\ipykernel_13796\\74775411.py:24: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df['extracted_locations'] = None\n",
            "C:\\Users\\nguye\\AppData\\Local\\Temp\\ipykernel_13796\\74775411.py:25: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df['extracted_tastes'] = None\n",
            "Đang xử lý batch:   0%|          | 0/1 [00:00<?, ?it/s]2025-03-09 23:28:59,657 - ERROR - Lỗi khi tiền xử lý văn bản: The provided dictionary language (vi) does not exist!\n",
            "2025-03-09 23:28:59,659 - ERROR - Lỗi khi tiền xử lý văn bản: The provided dictionary language (vi) does not exist!\n",
            "2025-03-09 23:28:59,662 - ERROR - Lỗi khi tiền xử lý văn bản: The provided dictionary language (vi) does not exist!\n",
            "2025-03-09 23:28:59,664 - ERROR - Lỗi khi tiền xử lý văn bản: The provided dictionary language (vi) does not exist!\n",
            "2025-03-09 23:28:59,667 - ERROR - Lỗi khi tiền xử lý văn bản: The provided dictionary language (vi) does not exist!\n",
            "2025-03-09 23:28:59,669 - ERROR - Lỗi khi tiền xử lý văn bản: The provided dictionary language (vi) does not exist!\n",
            "2025-03-09 23:28:59,670 - ERROR - Lỗi khi tiền xử lý văn bản: The provided dictionary language (vi) does not exist!\n",
            "2025-03-09 23:28:59,672 - ERROR - Lỗi khi tiền xử lý văn bản: The provided dictionary language (vi) does not exist!\n",
            "2025-03-09 23:28:59,674 - ERROR - Lỗi khi tiền xử lý văn bản: The provided dictionary language (vi) does not exist!\n",
            "2025-03-09 23:28:59,676 - ERROR - Lỗi khi tiền xử lý văn bản: The provided dictionary language (vi) does not exist!\n",
            "2025-03-09 23:29:12,573 - INFO - Đã lưu 74 foods vào extracted_data/foods_list.json\n",
            "2025-03-09 23:29:12,574 - INFO - Đã lưu 7 locations vào extracted_data/locations_list.json\n",
            "2025-03-09 23:29:12,577 - INFO - Đã lưu kết quả trung gian vào extracted_data/processed_data_batch_1.csv sau batch 1/1\n",
            "Đang xử lý batch: 100%|██████████| 1/1 [00:12<00:00, 12.92s/it]\n",
            "2025-03-09 23:29:12,578 - INFO - Trích xuất hoàn tất. Tìm thấy 74 thực thể món ăn và 7 thực thể địa điểm.\n",
            "2025-03-09 23:29:12,579 - INFO - Thực hiện bootstrapping để mở rộng danh sách thực thể...\n",
            "2025-03-09 23:29:14,933 - INFO - Đã thêm 101 thực thể món ăn tiềm năng từ mở rộng thực thể\n",
            "2025-03-09 23:29:14,937 - INFO - Đã lưu 170 foods vào extracted_data/foods_list.json\n",
            "2025-03-09 23:29:14,938 - INFO - Đã lưu 7 locations vào extracted_data/locations_list.json\n",
            "2025-03-09 23:29:14,940 - INFO - Quá trình xử lý hoàn tất. Kết quả đã được lưu trong thư mục 'extracted_data'.\n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_food_prompts():\n",
        "    food_prompt_template = \"\"\"\n",
        "    Hãy phân tích thông tin về món ăn sau đây:\n",
        "    \n",
        "    Danh sách món ăn: {foods}\n",
        "    \n",
        "    Yêu cầu:\n",
        "    1. Phân loại các món ăn thành các nhóm (ví dụ: món nước, món nướng, đồ uống, etc.)\n",
        "    2. Xác định các món đặc trưng nhất\n",
        "    3. Đề xuất món ăn phổ biến nhất dựa trên tần suất xuất hiện\n",
        "    4. Liên kết món ăn với văn hóa ẩm thực Việt Nam\n",
        "    5. Đề xuất các kết hợp món ăn phù hợp\n",
        "    \n",
        "    Vui lòng trình bày kết quả một cách chi tiết và có cấu trúc.\n",
        "    \"\"\"\n",
        "    return food_prompt_template\n",
        "\n",
        "def create_location_prompts():\n",
        "    location_prompt_template = \"\"\"\n",
        "    Hãy phân tích thông tin về địa điểm ẩm thực sau đây:\n",
        "    \n",
        "    Danh sách địa điểm: {locations}\n",
        "    \n",
        "    Yêu cầu:\n",
        "    1. Nhóm các địa điểm theo khu vực (quận/huyện)\n",
        "    2. Xác định các khu vực ẩm thực nổi tiếng\n",
        "    3. Đề xuất tuyến đường khám phá ẩm thực\n",
        "    4. Liên kết địa điểm với đặc trưng ẩm thực\n",
        "    5. Xác định các điểm ẩm thực có mật độ cao\n",
        "    \n",
        "    Vui lòng phân tích và đưa ra các gợi ý chi tiết cho người dùng.\n",
        "    \"\"\"\n",
        "    return location_prompt_template\n",
        "\n",
        "def analyze_food_locations(structured_data):\n",
        "    \"\"\"Analyze food and location data using Gemini API\"\"\"\n",
        "    \n",
        "    import google.generativeai as genai\n",
        "    from collections import Counter\n",
        "    \n",
        "    # Configure API\n",
        "    genai.configure(api_key='AIzaSyCtSe_5iLidRs0CaVSIOGgRrLK7H29jZfY')\n",
        "    model = genai.GenerativeModel('models/gemini-2.0-flash-thinking-exp-1219')\n",
        "    \n",
        "    # Extract unique foods and locations\n",
        "    all_foods = []\n",
        "    all_locations = []\n",
        "    \n",
        "    for item in structured_data:\n",
        "        all_foods.extend(item['extracted_entities']['foods'])\n",
        "        all_locations.extend(item['extracted_entities']['locations'])\n",
        "    \n",
        "    # Count frequencies\n",
        "    food_counts = Counter(all_foods)\n",
        "    location_counts = Counter(all_locations)\n",
        "    \n",
        "    # Create prompts\n",
        "    food_prompt = create_food_prompts().format(\n",
        "        foods=\"\\n\".join(f\"- {food} (xuất hiện {count} lần)\" \n",
        "                       for food, count in food_counts.most_common())\n",
        "    )\n",
        "    \n",
        "    location_prompt = create_location_prompts().format(\n",
        "        locations=\"\\n\".join(f\"- {loc} (xuất hiện {count} lần)\"\n",
        "                           for loc, count in location_counts.most_common())\n",
        "    )\n",
        "    \n",
        "    # Get responses from Gemini\n",
        "    food_analysis = model.generate_content(food_prompt)\n",
        "    location_analysis = model.generate_content(location_prompt)\n",
        "    \n",
        "    return {\n",
        "        'food_analysis': food_analysis.text,\n",
        "        'location_analysis': location_analysis.text,\n",
        "        'statistics': {\n",
        "            'total_unique_foods': len(set(all_foods)),\n",
        "            'total_unique_locations': len(set(all_locations)),\n",
        "            'most_common_foods': dict(food_counts.most_common(10)),\n",
        "            'most_common_locations': dict(location_counts.most_common(10))\n",
        "        }\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Phân tích món ăn:\n",
            "Dựa trên danh sách món ăn và tần suất xuất hiện của chúng, chúng ta có thể tiến hành phân tích chi tiết như sau:\n",
            "\n",
            "**1. Phân loại các món ăn thành các nhóm:**\n",
            "\n",
            "Để dễ dàng phân tích và hiểu rõ hơn về danh sách món ăn này, chúng ta có thể phân loại chúng vào các nhóm dựa trên đặc điểm chung. Dưới đây là một cách phân loại chi tiết:\n",
            "\n",
            "*   **Nhóm Thịt Gia Cầm:**\n",
            "    *   Gà (xuất hiện 9 lần): Bao gồm \"gà\", \"gà tàu\", \"thịt gà\".\n",
            "    *   Vịt (xuất hiện 1 lần): \"vịt lậu\".\n",
            "*   **Nhóm Hải Sản:**\n",
            "    *   Cá (xuất hiện 8 lần): Bao gồm \"cá\", \"cá tai tượng\".\n",
            "    *   Tôm (xuất hiện 4 lần): Bao gồm \"tôm\", \"tôm tươi\", \"tôm đại dương\", \"tôm thịt\", \"tôm đại dương chân gà\", \"tôm hôm\", \"cơm hùm\".\n",
            "    *   Ốc/Sò (xuất hiện 23 lần): Bao gồm \"ốc\", \"ốc tươi\", \"ốc hương\", \"ốc hương hấp\", \"ốc khế\", \"ốc muộn nướng tiêu\", \"ốc bưu nướng\", \"ốc trứng muối\", \"ốc trứng muối bạn\", \"ốc hương xốt trứng muối\", \"ốc trứng muối trứng\", \"ốc boulot\", \"sò lông mỡ\", \"sò đồng giá\", \"sò ốc\".  *Lưu ý: Nhóm này có số lượng món và biến thể rất đa dạng, cho thấy sự phổ biến của ốc và sò.*\n",
            "*   **Nhóm Thịt Đỏ:**\n",
            "    *   Thịt (xuất hiện 7 lần): Bao gồm \"thịt\", \"thịt sông khói\", \"thịt nguội sông khói\", \"thịt ngọt nước\", \"thịt tươi\", \"thịt đông lạnh\". *Lưu ý: \"Thịt\" ở đây có thể hiểu là thịt heo hoặc thịt nói chung.*\n",
            "    *   Bò (xuất hiện 18 lần): Bao gồm \"bò\", \"bò tươi\", \"thịt bò tươi\", \"thịt bò\", \"bò bò\", \"bò vàng\", \"bò bóp\", \"gỏi bò bóp\", \"bò mắm ruốt\", \"bò mềm\", \"bò tươi thái\", \"bò thai\", \"bò bên quán\", \"bò nướng tảng\", \"cơm miếng bò thai\", \"thịt bò tươi thái\". *Lưu ý: Nhóm bò có nhiều biến thể và cách chế biến khác nhau.*\n",
            "    *   Heo (xuất hiện 1 lần): \"heo\".\n",
            "*   **Nhóm Rau củ quả:**\n",
            "    *   Rau (xuất hiện 2 lần): Bao gồm \"rau\", \"rau trước\".\n",
            "    *   Măng (xuất hiện 1 lần): \"măng\".\n",
            "    *   Nấm (xuất hiện 1 lần): \"nấm\".\n",
            "*   **Nhóm Đậu:**\n",
            "    *   Đậu hủ (xuất hiện 2 lần): Bao gồm \"đậu hủ\", \"đậu hủ giòn béo\", \"đậu hủ kì\".\n",
            "*   **Nhóm Sữa và Sản phẩm từ Sữa:**\n",
            "    *   Sữa (xuất hiện 3 lần): Bao gồm \"sữa\", \"sữa phường\".\n",
            "    *   Sữa chua (xuất hiện 2 lần): Bao gồm \"sữa chua\", \"sữa chua nếp cẩm\".\n",
            "    *   Kem (xuất hiện 2 lần): Bao gồm \"kem sữa\", \"kem\".\n",
            "*   **Nhóm Tinh bột:**\n",
            "    *   Cơm (xuất hiện 2 lần): Bao gồm \"cơm\", \"cơm hùm\", \"cơm miếng bò thai\".\n",
            "    *   Bánh mì (xuất hiện 1 lần): \"bánh mì\".\n",
            "    *   Xôi (xuất hiện 1 lần): \"xôi quanh\".\n",
            "    *   Hủ tiếu mì (xuất hiện 4 lần): Bao gồm \"hủ tiếu mì bò\", \"hủ tiếu mì sườn\", \"hủ tiếu mì hải sản\", \"hủ tiếu mì sườn há\".\n",
            "*   **Nhóm Lẩu:**\n",
            "    *   Lẩu (xuất hiện 3 lần): Bao gồm \"lẩu măng\", \"lẩu bò\", \"lẩu tái nhúng\".\n",
            "\n",
            "**2. Xác định các món đặc trưng nhất:**\n",
            "\n",
            "*   **Ốc/Sò:** Với số lượng xuất hiện và biến thể đa dạng, các món ốc và sò là một trong những món đặc trưng nhất trong danh sách này. Sự đa dạng trong tên gọi (ốc hương, ốc khế, ốc bưu, ốc trứng muối, ốc boulot, sò lông, sò đồng) cho thấy sự phong phú của các loại ốc và sò được ưa chuộng.\n",
            "*   **Bò:** Các món bò cũng rất đa dạng và phong phú, từ bò tươi, bò bóp, bò nướng tảng đến các món lẩu bò, cơm bò, hủ tiếu bò. Điều này cho thấy thịt bò là một nguyên liệu phổ biến và được chế biến thành nhiều món ăn khác nhau.\n",
            "*   **Hủ tiếu mì:**  Sự xuất hiện của nhiều loại hủ tiếu mì (bò, sườn, hải sản, sườn há) cho thấy đây là một món ăn đặc trưng và phổ biến trong danh sách này, có thể là món ăn sáng, trưa hoặc tối.\n",
            "*   **Lẩu:**  Các món lẩu như lẩu măng, lẩu bò, lẩu tái nhúng cũng là những món ăn đặc trưng, thường được dùng trong các bữa ăn gia đình hoặc nhóm bạn bè, đặc biệt vào thời tiết mát mẻ.\n",
            "*   **Các món chế biến từ sữa:** Sữa chua nếp cẩm, kem sữa, kem cho thấy sự quan tâm đến các món tráng miệng hoặc đồ uống từ sữa.\n",
            "\n",
            "**3. Đề xuất món ăn phổ biến nhất dựa trên tần suất xuất hiện:**\n",
            "\n",
            "Dựa trên tần suất xuất hiện, có thể đề xuất các món ăn phổ biến nhất như sau:\n",
            "\n",
            "*   **Gà (9 lần):**  Gà là món ăn phổ biến nhất trong danh sách, cho thấy sự ưa chuộng các món ăn từ gà.\n",
            "*   **Cá (8 lần):** Cá đứng thứ hai về tần suất, cho thấy các món cá cũng rất được yêu thích.\n",
            "*   **Thịt và Ốc (cùng 7 lần):**  Thịt (có thể hiểu là thịt heo hoặc thịt nói chung) và ốc có tần suất xuất hiện tương đương nhau, cho thấy sự phổ biến của cả hai loại nguyên liệu này.\n",
            "*   **Bò (6 lần):**  Bò cũng là một món ăn phổ biến, đứng thứ năm về tần suất.\n",
            "*   **Tôm (4 lần):** Tôm cũng là một lựa chọn hải sản được ưa chuộng.\n",
            "*   **Sữa (3 lần):** Sữa và các sản phẩm từ sữa cũng xuất hiện nhiều lần, cho thấy sự quan tâm đến đồ uống và tráng miệng.\n",
            "\n",
            "**4. Liên kết món ăn với văn hóa ẩm thực Việt Nam:**\n",
            "\n",
            "Danh sách món ăn này phản ánh nhiều nét đặc trưng của văn hóa ẩm thực Việt Nam:\n",
            "\n",
            "*   **Sự đa dạng của nguyên liệu:** Danh sách sử dụng nhiều loại nguyên liệu khác nhau từ thịt gia cầm (gà, vịt), hải sản (cá, tôm, ốc, sò), thịt đỏ (thịt, bò, heo), rau củ (rau, măng, nấm), đậu hủ, sữa, tinh bột (cơm, bánh mì, xôi, hủ tiếu mì). Điều này thể hiện sự phong phú và đa dạng trong nguồn nguyên liệu của ẩm thực Việt Nam.\n",
            "*   **Ưu tiên hải sản và các món từ ốc/sò:** Số lượng món ốc/sò và hải sản xuất hiện nhiều cho thấy sự ưa chuộng các món ăn từ biển, đặc biệt là ốc và sò, vốn là những đặc sản phổ biến ở nhiều vùng ven biển Việt Nam.\n",
            "*   **Sự phổ biến của các món thịt:** Gà, thịt, bò là những loại thịt được tiêu thụ nhiều nhất, phản ánh thói quen ăn uống của người Việt Nam.\n",
            "*   **Các món nước và lẩu:** Hủ tiếu mì và lẩu là những món ăn phổ biến trong ẩm thực Việt Nam, thường được dùng trong các bữa ăn gia đình hoặc khi tụ tập bạn bè. Hủ tiếu mì là món ăn sáng, trưa, tối quen thuộc, còn lẩu thường được ưa chuộng trong thời tiết mát mẻ hoặc các dịp đặc biệt.\n",
            "*   **Sử dụng rau củ tươi:** Sự xuất hiện của \"rau\" và \"rau trước\" cho thấy rau xanh là một phần không thể thiếu trong bữa ăn Việt Nam, đảm bảo sự cân bằng dinh dưỡng và hương vị tươi ngon.\n",
            "*   **Sự kết hợp giữa truyền thống và hiện đại:** Bên cạnh các món ăn truyền thống như gà, cá, thịt, ốc, bò, danh sách cũng có những món mang hơi hướng hiện đại hơn như \"đậu hủ giòn béo\", \"kem sữa\", \"sữa chua nếp cẩm\", thể hiện sự phát triển và giao thoa của ẩm thực Việt Nam.\n",
            "\n",
            "**5. Đề xuất các kết hợp món ăn phù hợp:**\n",
            "\n",
            "Dựa trên danh sách và văn hóa ẩm thực Việt Nam, có thể đề xuất một số kết hợp món ăn phù hợp như sau:\n",
            "\n",
            "*   **Bữa ăn gia đình truyền thống:**\n",
            "    *   **Món chính:** Gà luộc hoặc gà nướng, cá chiên hoặc cá kho, thịt kho tàu hoặc thịt ram, bò xào hoặc bò nướng.\n",
            "    *   **Món rau:** Rau luộc hoặc rau xào.\n",
            "    *   **Món canh:** Lẩu măng hoặc lẩu bò (nếu phù hợp thời tiết).\n",
            "    *   **Cơm trắng.**\n",
            "*   **Bữa ăn hải sản:**\n",
            "    *   **Món khai vị:** Gỏi bò bóp (nếu muốn kết hợp thịt), hoặc chỉ tập trung hải sản như sò lông mỡ hành, ốc hương xốt trứng muối.\n",
            "    *   **Món chính:** Tôm nướng, cá tai tượng chiên xù, ốc hương hấp, ốc bưu nướng, các món sò ốc khác.\n",
            "    *   **Món rau:** Rau sống ăn kèm hải sản hoặc rau xào.\n",
            "    *   **Bún hoặc cơm.**\n",
            "*   **Bữa ăn nhanh buổi sáng/trưa:**\n",
            "    *   Hủ tiếu mì bò, hủ tiếu mì sườn, hủ tiếu mì hải sản.\n",
            "    *   Bánh mì thịt.\n",
            "    *   Xôi gà.\n",
            "*   **Món tráng miệng/đồ uống:**\n",
            "    *   Sữa chua nếp cẩm.\n",
            "    *   Kem sữa hoặc kem.\n",
            "    *   Sữa tươi.\n",
            "\n",
            "**Kết luận:**\n",
            "\n",
            "Danh sách món ăn này thể hiện sự đa dạng và phong phú của ẩm thực Việt Nam, với sự ưu tiên các món từ hải sản (đặc biệt là ốc/sò), thịt gia cầm và thịt đỏ. Các món nước như hủ tiếu mì và lẩu cũng rất phổ biến. Phân tích tần suất xuất hiện giúp nhận diện được những món ăn được ưa chuộng nhất, đồng thời liên kết với văn hóa ẩm thực Việt Nam giúp hiểu rõ hơn về thói quen ăn uống và sở thích ẩm thực của người Việt. Các đề xuất kết hợp món ăn mang tính gợi ý, có thể tùy chỉnh theo sở thích và hoàn cảnh cụ thể.\n",
            "\n",
            "Phân tích địa điểm:\n",
            "Dựa trên thông tin bạn cung cấp về danh sách địa điểm ẩm thực, chúng ta có thể phân tích và đưa ra các gợi ý như sau:\n",
            "\n",
            "**1. Nhóm các địa điểm theo khu vực (quận/huyện):**\n",
            "\n",
            "* **Quận Bình Thạnh:** bình thạnh (xuất hiện 2 lần)\n",
            "* **Quận Thủ Đức:** thủ đức (xuất hiện 1 lần)\n",
            "* **Huyện Củ Chi:** củ chi (xuất hiện 1 lần)\n",
            "* **Quận Tân Bình:** tân bình (xuất hiện 1 lần)\n",
            "* **Quận Phú Nhuận:** phú nhuận (xuất hiện 1 lần), phú nhuận gò (xuất hiện 1 lần) - *Lưu ý: \"phú nhuận gò\" có thể là cách gọi không chính xác, có thể chỉ khu vực Phú Nhuận gần Gò Vấp hoặc là lỗi gõ phím. Để đơn giản, tạm thời xem xét là khu vực thuộc Phú Nhuận hoặc lân cận.*\n",
            "* **\"khuyến\":**  *Địa điểm này không rõ ràng và không xác định được quận/huyện. Có thể là lỗi chính tả hoặc thông tin không đầy đủ. Cần loại trừ khỏi phân tích khu vực nếu không có thêm thông tin.*\n",
            "\n",
            "**Danh sách khu vực sau khi nhóm:**\n",
            "\n",
            "* **Quận Bình Thạnh:** (2 lần)\n",
            "* **Quận Phú Nhuận (bao gồm cả \"phú nhuận gò\" không rõ ràng):** (2 lần)\n",
            "* **Quận Thủ Đức:** (1 lần)\n",
            "* **Huyện Củ Chi:** (1 lần)\n",
            "* **Quận Tân Bình:** (1 lần)\n",
            "\n",
            "**2. Xác định các khu vực ẩm thực nổi tiếng:**\n",
            "\n",
            "Dựa trên số lần xuất hiện trong danh sách (mặc dù số lượng địa điểm còn hạn chế) và kiến thức chung về ẩm thực TP.HCM, có thể nhận định:\n",
            "\n",
            "* **Quận Bình Thạnh và Quận Phú Nhuận:**  Có vẻ là những khu vực được nhắc đến nhiều hơn trong danh sách này, cho thấy có thể đây là những điểm đến ẩm thực được quan tâm. Cả hai quận này đều nổi tiếng với sự đa dạng ẩm thực, từ quán ăn đường phố, nhà hàng gia đình đến các địa điểm sang trọng hơn.\n",
            "* **Các khu vực khác (Thủ Đức, Củ Chi, Tân Bình):**  Cũng là những khu vực có ẩm thực riêng, nhưng trong danh sách này xuất hiện ít hơn. Điều này không có nghĩa là chúng kém nổi tiếng về ẩm thực, mà có thể chỉ đơn giản là ít được đề cập trong nguồn thông tin bạn cung cấp.\n",
            "\n",
            "**Lưu ý quan trọng:**  Danh sách địa điểm này còn rất hạn chế và không đại diện cho toàn bộ bản đồ ẩm thực TP.HCM.  Việc đánh giá độ nổi tiếng chỉ dựa trên số lần xuất hiện trong danh sách này là chưa đủ. Cần tham khảo thêm nhiều nguồn thông tin khác để có cái nhìn toàn diện hơn.\n",
            "\n",
            "**3. Đề xuất tuyến đường khám phá ẩm thực:**\n",
            "\n",
            "Dựa trên các khu vực đã xác định, có thể đề xuất một số tuyến đường khám phá ẩm thực như sau:\n",
            "\n",
            "* **Tuyến 1: Khám phá ẩm thực Bình Thạnh - Phú Nhuận:**\n",
            "    * **Bắt đầu từ Bình Thạnh:** Khám phá các khu vực ẩm thực đường phố nổi tiếng như đường Phan Văn Hân, khu vực chợ Bà Chiểu, đường Xô Viết Nghệ Tĩnh. Bình Thạnh nổi tiếng với hải sản ven sông, lẩu, nướng, và các món ăn vặt đa dạng.\n",
            "    * **Di chuyển sang Phú Nhuận:**  Khám phá các con đường ẩm thực như Phan Xích Long, Huỳnh Văn Bánh, Nguyễn Trọng Tuyển. Phú Nhuận đa dạng từ cafe, bánh ngọt, món ăn gia đình, đến các nhà hàng phong cách.\n",
            "    * **Ưu điểm:** Hai quận này liền kề, dễ dàng di chuyển, tập trung nhiều địa điểm ẩm thực đa dạng.\n",
            "    * **Phù hợp:** Với người muốn khám phá nhiều loại hình ẩm thực trong một ngày hoặc cuối tuần, từ bình dân đến tầm trung.\n",
            "\n",
            "* **Tuyến 2:  Ẩm thực đường phố và sinh viên Thủ Đức:**\n",
            "    * **Tập trung tại Thủ Đức:**  Khám phá các khu vực xung quanh các trường đại học (Làng Đại Học Thủ Đức), các tuyến đường như Võ Văn Ngân, Hoàng Diệu 2. Thủ Đức nổi tiếng với ẩm thực sinh viên giá rẻ, quán ăn vặt, quán nhậu bình dân.\n",
            "    * **Ưu điểm:** Giá cả phải chăng, trải nghiệm ẩm thực đường phố sôi động.\n",
            "    * **Phù hợp:** Với sinh viên, giới trẻ, hoặc người muốn tiết kiệm chi phí và thích không khí náo nhiệt.\n",
            "\n",
            "* **Tuyến 3:  Ẩm thực dân dã và đặc sản Củ Chi (dành cho một ngày):**\n",
            "    * **Di chuyển đến Củ Chi:**  Khám phá các nhà hàng, quán ăn chuyên về đặc sản Củ Chi như bò tơ, rau rừng, bánh xèo, gỏi gà.\n",
            "    * **Ưu điểm:** Trải nghiệm ẩm thực vùng quê, không gian thoáng đãng, gần gũi thiên nhiên.\n",
            "    * **Phù hợp:** Với người muốn đổi gió, tìm kiếm không gian yên tĩnh và thưởng thức các món ăn đặc trưng vùng miền.\n",
            "\n",
            "* **Tuyến 4:  Ẩm thực đa dạng Tân Bình (gần sân bay):**\n",
            "    * **Khám phá Tân Bình:**  Tập trung vào các khu vực xung quanh sân bay Tân Sơn Nhất, đường Trường Sơn, khu vực chợ Phạm Văn Hai. Tân Bình có nhiều quán ăn phục vụ khách du lịch, nhà hàng đa dạng món Á - Âu, quán cafe.\n",
            "    * **Ưu điểm:**  Tiện lợi cho du khách, nhiều lựa chọn ẩm thực quốc tế và đặc sản vùng miền.\n",
            "    * **Phù hợp:** Với du khách, người có thời gian hạn chế gần sân bay, hoặc muốn thưởng thức ẩm thực đa dạng.\n",
            "\n",
            "**4. Liên kết địa điểm với đặc trưng ẩm thực:**\n",
            "\n",
            "* **Bình Thạnh:**  Ẩm thực đường phố sôi động, hải sản ven sông, quán nhậu bình dân và nhà hàng gia đình. Đặc biệt nổi tiếng với các món lẩu, nướng, hải sản tươi sống.\n",
            "* **Phú Nhuận:**  Đa dạng từ cafe, bánh ngọt, món ăn vặt, quán ăn gia đình đến nhà hàng sang trọng.  Phong phú về ẩm thực quốc tế và các món ăn healthy, cafe đẹp.\n",
            "* **Thủ Đức:**  Ẩm thực sinh viên giá rẻ, quán ăn vặt, trà sữa, lẩu nướng bình dân, quán nhậu. Giá cả phải chăng, không khí náo nhiệt.\n",
            "* **Củ Chi:**  Ẩm thực dân dã, đặc sản vùng quê như bò tơ Củ Chi, rau rừng, bánh xèo, gỏi gà, các món ăn đồng quê. Không gian thoáng đãng, gần gũi thiên nhiên.\n",
            "* **Tân Bình:**  Ẩm thực đa dạng, phục vụ khách du lịch, nhà hàng gia đình, quán ăn đặc sản vùng miền, ẩm thực quốc tế. Do gần sân bay nên có nhiều lựa chọn đa dạng.\n",
            "\n",
            "**5. Xác định các điểm ẩm thực có mật độ cao:**\n",
            "\n",
            "Dựa trên danh sách của bạn, **Quận Bình Thạnh** là khu vực xuất hiện nhiều nhất (2 lần).  Điều này có thể gợi ý rằng Bình Thạnh là một trong những khu vực có mật độ địa điểm ẩm thực được nhắc đến cao hơn trong nguồn thông tin này.\n",
            "\n",
            "Tuy nhiên, cần nhấn mạnh lại rằng:\n",
            "\n",
            "* **Dữ liệu còn quá ít:** Chỉ có 7 địa điểm được liệt kê, và \"bình thạnh\" lặp lại 2 lần.  Không thể kết luận chắc chắn về mật độ cao dựa trên dữ liệu này.\n",
            "* **\"Mật độ cao\" cần được định nghĩa rõ hơn:**  \"Mật độ cao\" có thể hiểu là số lượng địa điểm ẩm thực trên một đơn vị diện tích, hoặc mức độ tập trung các địa điểm nổi tiếng, hoặc tần suất được nhắc đến trong các nguồn thông tin.  Cần có định nghĩa rõ ràng hơn để đánh giá chính xác.\n",
            "\n",
            "**Gợi ý thêm cho người dùng:**\n",
            "\n",
            "* **Mở rộng nguồn thông tin:**  Để có cái nhìn toàn diện hơn, bạn nên tham khảo thêm nhiều nguồn thông tin khác về địa điểm ẩm thực TP.HCM, ví dụ:\n",
            "    * Các trang web và ứng dụng đánh giá ẩm thực (Foody, Lozi, GrabFood, Baemin...).\n",
            "    * Các blog, review ẩm thực trên mạng xã hội (Facebook, Instagram, TikTok...).\n",
            "    * Các bài báo, tạp chí về ẩm thực địa phương.\n",
            "    * Hỏi ý kiến bạn bè, người thân, người dân địa phương.\n",
            "* **Xác định rõ nhu cầu và sở thích:**  Bạn muốn khám phá loại hình ẩm thực nào? Ngân sách bao nhiêu?  Thích không gian nào (đường phố, nhà hàng, quán cafe...)?  Khi xác định rõ nhu cầu, bạn sẽ dễ dàng lựa chọn được khu vực và tuyến đường khám phá ẩm thực phù hợp.\n",
            "* **Lưu ý về \"phú nhuận gò\" và \"khuyến\":**  Cần làm rõ thông tin về \"phú nhuận gò\" (có thể là Phú Nhuận hoặc Gò Vấp, hoặc khu vực lân cận) và loại bỏ \"khuyến\" vì không xác định được địa điểm. Nếu có thêm thông tin, hãy cập nhật để phân tích chính xác hơn.\n",
            "\n",
            "Hy vọng phân tích này sẽ hữu ích cho bạn trong việc khám phá ẩm thực TP.HCM!\n"
          ]
        }
      ],
      "source": [
        "# Example usage\n",
        "import json\n",
        "\n",
        "# Load structured data\n",
        "with open('C:/Users/nguye/OneDrive/Tài liệu/GitHub/21KHDL-TikTok-Analytics/notebooks/extracted_data/structured_entities.json', 'r', encoding='utf-8') as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "# Run analysis\n",
        "results = analyze_food_locations(data)\n",
        "\n",
        "# Save results\n",
        "with open('food_location_analysis.json', 'w', encoding='utf-8') as f:\n",
        "    json.dump(results, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(\"\\nPhân tích món ăn:\")\n",
        "print(results['food_analysis'])\n",
        "print(\"\\nPhân tích địa điểm:\")\n",
        "print(results['location_analysis'])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "AppliedDataProject",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
