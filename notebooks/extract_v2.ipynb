{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 218,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U91UVADr4Phh",
        "outputId": "f4a3bd1b-2a17-4e04-dbda-2bfbe125eeda"
      },
      "outputs": [],
      "source": [
        "# %%shell\n",
        "# git clone --branch TrggTin --single-branch https://github.com/vphuhan/21KHDL-TikTok-Analytics.git\n",
        "# cd 21KHDL-TikTok-Analytics\n",
        "# git sparse-checkout init --cone\n",
        "# git sparse-checkout set data/interim\n",
        "# git checkout"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 219,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0NZ7_Zrw5PFe",
        "outputId": "d4ed584f-a229-4763-aa43-af3871f0893e"
      },
      "outputs": [],
      "source": [
        "# pip install pandas nltk underthesea scikit-learn tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Imports and Initialization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 220,
      "metadata": {
        "id": "7GYwq-Tf4PTa"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import unicodedata\n",
        "import nltk\n",
        "from underthesea import word_tokenize, pos_tag, ner\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from difflib import get_close_matches\n",
        "import logging\n",
        "import json\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import string\n",
        "import regex as re\n",
        "import traceback\n",
        "import jdc  \n",
        "from spellchecker import SpellChecker\n",
        "from datetime import datetime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 221,
      "metadata": {
        "id": "pfZUR8vY4POT"
      },
      "outputs": [],
      "source": [
        "# Configure logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
        "    handlers=[\n",
        "        logging.FileHandler(\"extraction_log.log\"),\n",
        "        logging.StreamHandler()\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 222,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aDuHFYPU6NKm",
        "outputId": "2a87ddda-2e79-4d97-ed72-1a24c515c520"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt_tab to\n",
            "[nltk_data]     C:\\Users\\nguye\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 222,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# VietnameseTextProcessor Class Definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 223,
      "metadata": {},
      "outputs": [],
      "source": [
        "class VietnameseTextProcessor:\n",
        "    def __init__(self, food_list_path=None, location_list_path=None):\n",
        "        \"\"\"\n",
        "        Khởi tạo Bộ xử lý văn bản tiếng Việt\n",
        "\n",
        "        Tham số:\n",
        "            food_list_path (str): Đường dẫn đến tệp JSON chứa danh sách món ăn Việt Nam\n",
        "            location_list_path (str): Đường dẫn đến tệp JSON chứa danh sách địa điểm ở Việt Nam\n",
        "        \"\"\"\n",
        "        # Tải hoặc khởi tạo danh sách món ăn và địa điểm\n",
        "        self.foods = self._load_entity_list(food_list_path, \"foods\")\n",
        "        self.locations = self._load_entity_list(location_list_path, \"locations\")\n",
        "\n",
        "        # Các từ khóa phổ biến liên quan đến món ăn và hương vị trong tiếng Việt để hỗ trợ nhận diện\n",
        "        self.food_indicators = [\n",
        "            \"bánh\", \"phở\", \"bún\", \"xèo\", \"cơm\", \"gỏi\", \"chả\", \"xôi\", \"cao lầu\", \"cháo\",\n",
        "            \"mì\", \"hủ tiếu\", \"nem\", \"ram\", \"khọt\",\n",
        "            \"lẩu\", \"cá\", \"thịt\", \"canh\", \"rau\", \"đậu\", \"nướng\", \"ốc\", \"súp\", \"bắp\",\n",
        "            \"chuối\", \"nộm\", \"trà\", \"cà phê\", \"sinh tố\", \"kem\", \"tàu hủ\", \"chè\", \"yaourt\", \"nước mía\",\n",
        "            \"sữa\", \"kẹo\", \"đa\", \"nem chua\", \"gà\", \"món\", \"ăn\"\n",
        "        ]\n",
        "\n",
        "        self.taste_indicators = [\n",
        "            \"ngon\", \"ngọt\", \"chua\", \"cay\", \"đắng\", \"mặn\", \"bùi\", \"béo\", \"giòn\", \"mềm\",\n",
        "            \"thơm\", \"nồng\", \"đậm đà\", \"nhạt\", \"thanh\", \"tươi\", \"ướp\", \"rim\", \"kho\", \"xào\",\n",
        "            \"nướng\", \"luộc\", \"hấp\", \"chiên\", \"xốt\", \"tẩm\", \"ướt\", \"khô\", \"giòn tan\", \"dai\",\n",
        "            \"sần sật\", \"mọng nước\", \"đắng nghét\", \"chát\", \"cay xè\", \"tê\", \"mặn chát\", \"ngọt lịm\", \"béo ngậy\", \"thơm lừng\",\n",
        "            \"nồng nàn\", \"đậm vị\", \"nhạt nhẽo\", \"thanh mát\", \"tươi rói\", \"tươi ngon\", \"đậm đà hương vị\", \"vừa ăn\", \"hợp khẩu vị\"\n",
        "        ]\n",
        "\n",
        "        self.locations_indicators = [\n",
        "            \"Quận\", \"Huyện\", \"Phường\", \"Xã\", \"Thành phố\", \"TP\", \"Tỉnh\", \"đường\", \"phố\", \"chợ\", \"địa chỉ\", \"nằm ở\", \"tại\",\n",
        "            \"Quận 1\", \"Quận 2\", \"Quận 3\", \"Quận 4\", \"Quận 5\", \"Quận 6\", \"Quận 7\", \"Quận 8\", \"Quận 9\", \"Quận 10\",\n",
        "            \"Quận 11\", \"Quận 12\", \"Bình Thạnh\", \"Tân Bình\", \"Tân Phú\", \"Phú Nhuận\", \"Gò Vấp\", \"Bình Tân\", \"Thủ Đức\", \"Hóc Môn\",\n",
        "            \"Củ Chi\", \"Nhà Bè\", \"Cần Giờ\", \"Bình Chánh\", \"TP Thủ Đức\",\n",
        "            \"Hà Nội\", \"Hồ Chí Minh\", \"Đà Nẵng\", \"Hải Phòng\", \"Cần Thơ\", \"Huế\", \"Nha Trang\", \"Vũng Tàu\", \"Đà Lạt\",\n",
        "            \"Hạ Long\", \"Mỹ Tho\", \"Long Xuyên\", \"Rạch Giá\", \"Cà Mau\", \"Biên Hòa\", \"Buôn Ma Thuột\", \"Thái Nguyên\", \"Nam Định\"\n",
        "        ]\n",
        "\n",
        "        # Tải các tài nguyên của NLTK nếu cần\n",
        "        try:\n",
        "            nltk.data.find('tokenizers/punkt')\n",
        "        except LookupError:\n",
        "            nltk.download('punkt')\n",
        "\n",
        "        # Tạo thư mục để lưu trữ các tệp dữ liệu được trích xuất\n",
        "        os.makedirs(\"extracted_data\", exist_ok=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Helper Methods"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 224,
      "metadata": {},
      "outputs": [],
      "source": [
        "def _load_entity_list(self, file_path, entity_type):\n",
        "    \"\"\"Tải danh sách thực thể từ tệp hoặc trả về tập rỗng mặc định\"\"\"\n",
        "    if file_path and os.path.exists(file_path):\n",
        "        try:\n",
        "            with open(file_path, 'r', encoding='utf-8') as f:\n",
        "                return set(json.load(f))\n",
        "        except Exception as e:\n",
        "            logging.warning(f\"Lỗi khi tải danh sách {entity_type}: {e}\")\n",
        "\n",
        "    logging.info(f\"Không tìm thấy danh sách {entity_type} hiện có, bắt đầu với tập rỗng\")\n",
        "    return set()\n",
        "\n",
        "def save_entity_list(self, entity_list, entity_type):\n",
        "    \"\"\"Lưu danh sách thực thể đã cập nhật vào tệp\"\"\"\n",
        "    file_path = f\"extracted_data/{entity_type}_list.json\"\n",
        "    with open(file_path, 'w', encoding='utf-8') as f:\n",
        "        json.dump(list(entity_list), f, ensure_ascii=False, indent=2)\n",
        "    logging.info(f\"Đã lưu {len(entity_list)} {entity_type} vào {file_path}\")\n",
        "\n",
        "def normalize_vietnamese_text(self, text):\n",
        "    \"\"\"Chuẩn hóa văn bản tiếng Việt bằng cách xử lý dấu và chữ hoa/thường\"\"\"\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "\n",
        "    # Chuẩn hóa ký tự Unicode\n",
        "    text = unicodedata.normalize('NFC', text)\n",
        "\n",
        "    # Loại bỏ khoảng trắng thừa\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    return text\n",
        "\n",
        "def clean_text(self, text):\n",
        "    \"\"\"Làm sạch văn bản bằng cách loại bỏ ký tự đặc biệt và chuẩn hóa\"\"\"\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "\n",
        "    # Chuẩn hóa văn bản\n",
        "    text = self.normalize_vietnamese_text(text)\n",
        "\n",
        "    # Loại bỏ đường dẫn URL\n",
        "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
        "\n",
        "    # Loại bỏ biểu tượng cảm xúc và ký tự đặc biệt trong khi giữ lại chữ tiếng Việt\n",
        "    # Biểu thức chính quy này giữ lại chữ cái tiếng Việt, số, dấu câu và khoảng trắng\n",
        "    vietnamese_pattern = r'[^\\p{L}\\p{N}\\p{P}\\s]+'\n",
        "    text = re.sub(vietnamese_pattern, '', text, flags=re.UNICODE)\n",
        "\n",
        "    # Sửa khoảng cách xung quanh dấu câu\n",
        "    text = re.sub(r'\\s+([.,;:?!])', r'\\1', text)\n",
        "\n",
        "    return text\n",
        "\n",
        "def auto_correct_text(self, text):\n",
        "    \"\"\"Tự động sửa lỗi chính tả bằng bộ kiểm tra chính tả\"\"\"\n",
        "    spell = SpellChecker(language='vi')\n",
        "    words = word_tokenize(text)\n",
        "    corrected_words = [spell.correction(word) for word in words]\n",
        "    return \" \".join(corrected_words)\n",
        "\n",
        "def load_stopwords(self, file_path):\n",
        "    \"\"\"Tải danh sách từ dừng từ tệp\"\"\"\n",
        "    if file_path and os.path.exists(file_path):\n",
        "        try:\n",
        "            with open(file_path, 'r', encoding='utf-8') as f:\n",
        "                return set(f.read().splitlines())\n",
        "        except Exception as e:\n",
        "            logging.warning(f\"Lỗi khi tải danh sách từ dừng: {e}\")\n",
        "    logging.info(\"Không tìm thấy tệp từ dừng, bắt đầu với tập rỗng\")\n",
        "    return set()\n",
        "\n",
        "def remove_stopwords(self, text, stopwords):\n",
        "    \"\"\"Loại bỏ từ dừng khỏi văn bản\"\"\"\n",
        "    words = word_tokenize(text)\n",
        "    filtered_words = [word for word in words if word.lower() not in stopwords]\n",
        "    return \" \".join(filtered_words)\n",
        "\n",
        "def preprocess_text(self, text):\n",
        "    \"\"\"Áp dụng tất cả các bước tiền xử lý lên văn bản\"\"\"\n",
        "    try:\n",
        "        text = self.clean_text(text)\n",
        "        text = self.auto_correct_text(text)  # Đã sửa lỗi tại đây\n",
        "        stopwords = self.load_stopwords('vietnamese-stopwords.txt')\n",
        "        text = self.remove_stopwords(text, stopwords)\n",
        "        return text\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Lỗi khi tiền xử lý văn bản: {e}\")\n",
        "        return text if isinstance(text, str) else \"\"\n",
        "\n",
        "# Gán các phương thức vào lớp VietnameseTextProcessor\n",
        "VietnameseTextProcessor._load_entity_list = _load_entity_list\n",
        "VietnameseTextProcessor.save_entity_list = save_entity_list \n",
        "VietnameseTextProcessor.normalize_vietnamese_text = normalize_vietnamese_text\n",
        "VietnameseTextProcessor.clean_text = clean_text\n",
        "VietnameseTextProcessor.auto_correct_text = auto_correct_text\n",
        "VietnameseTextProcessor.load_stopwords = load_stopwords\n",
        "VietnameseTextProcessor.preprocess_text = preprocess_text\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Entity Extraction Methods"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 225,
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_entities_from_ner(self, text):\n",
        "    \"\"\"Trích xuất thực thể từ văn bản bằng Named Entity Recognition (NER) của underthesea.\"\"\"\n",
        "    locations = []\n",
        "\n",
        "    try:\n",
        "        ner_tags = ner(text)  # Thực hiện nhận dạng thực thể có tên (NER)\n",
        "\n",
        "        # Kiểm tra nếu kết quả từ NER có định dạng mong đợi\n",
        "        if not isinstance(ner_tags, list):\n",
        "            return locations\n",
        "\n",
        "        # Trích xuất các địa điểm từ NER\n",
        "        current_loc = []\n",
        "\n",
        "        for item in ner_tags:\n",
        "            # Xử lý các định dạng đầu ra khác nhau từ NER\n",
        "            if isinstance(item, (list, tuple)) and len(item) == 2:\n",
        "                word, tag = item\n",
        "            else:\n",
        "                continue\n",
        "\n",
        "            if tag.startswith('B-LOC'):\n",
        "                if current_loc:\n",
        "                    locations.append(' '.join(current_loc))\n",
        "                    current_loc = []\n",
        "                current_loc.append(word)\n",
        "            elif tag.startswith('I-LOC') and current_loc:\n",
        "                current_loc.append(word)\n",
        "            elif current_loc:\n",
        "                locations.append(' '.join(current_loc))\n",
        "                current_loc = []\n",
        "\n",
        "        # Thêm thực thể địa điểm cuối cùng nếu có\n",
        "        if current_loc:\n",
        "            locations.append(' '.join(current_loc))\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Lỗi khi trích xuất thực thể bằng NER: {e}\")\n",
        "        logging.error(traceback.format_exc())\n",
        "\n",
        "    return locations\n",
        "\n",
        "def extract_entities_from_patterns(self, text, sentences, pos_tags):\n",
        "    \"\"\"Trích xuất thực thể bằng cách sử dụng phương pháp dựa trên mẫu (Pattern Matching).\"\"\"\n",
        "    foods = []\n",
        "    locations = []\n",
        "    tastes = []\n",
        "\n",
        "    # Xử lý từng câu để trích xuất thực thể\n",
        "    for idx, sentence in enumerate(sentences):\n",
        "        words = word_tokenize(sentence)\n",
        "        sentence_pos_tags = pos_tags[idx] if idx < len(pos_tags) else []\n",
        "\n",
        "        # Tìm thực thể về thực phẩm\n",
        "        self._extract_food_entities(sentence, sentence_pos_tags, foods)\n",
        "\n",
        "        # Tìm thực thể về địa điểm\n",
        "        self._extract_location_entities(sentence, sentence_pos_tags, locations)\n",
        "\n",
        "        # Tìm mô tả về hương vị\n",
        "        self._extract_taste_descriptions(sentence, words, tastes)\n",
        "\n",
        "    return foods, locations, tastes\n",
        "\n",
        "def _extract_food_entities(self, sentence, pos_tags, foods):\n",
        "    \"\"\"Trích xuất thực thể thực phẩm từ một câu.\"\"\"\n",
        "    # Kiểm tra danh sách thực phẩm có sẵn\n",
        "    for food in self.foods:\n",
        "        if food.lower() in sentence.lower():\n",
        "            foods.append(food)\n",
        "\n",
        "    # Tìm các từ chỉ thực phẩm\n",
        "    for idx, (word, tag) in enumerate(pos_tags):\n",
        "        if word.lower() in self.food_indicators:\n",
        "            noun_phrase = [word]\n",
        "            for i in range(1, 4):\n",
        "                if idx + i < len(pos_tags):\n",
        "                    next_word, next_tag = pos_tags[idx + i]\n",
        "                    if next_tag.startswith(('N', 'A')):  # Danh từ hoặc Tính từ\n",
        "                        noun_phrase.append(next_word)\n",
        "                    else:\n",
        "                        break\n",
        "            \n",
        "            if noun_phrase:\n",
        "                food_name = \" \".join(noun_phrase)\n",
        "                foods.append(food_name)\n",
        "                self.foods.add(food_name)\n",
        "\n",
        "def _extract_location_entities(self, sentence, pos_tags, locations):\n",
        "    \"\"\"Trích xuất thực thể địa điểm từ một câu.\"\"\"\n",
        "    # Kiểm tra danh sách địa điểm có sẵn\n",
        "    for location in self.locations:\n",
        "        if location.lower() in sentence.lower():\n",
        "            locations.append(location)\n",
        "\n",
        "    # Tìm các từ chỉ địa điểm\n",
        "    for idx, (word, tag) in enumerate(pos_tags):\n",
        "        if any(indicator.lower() in word.lower() for indicator in self.locations_indicators):\n",
        "            noun_phrase = [word]\n",
        "            for i in range(1, 4):\n",
        "                if idx + i < len(pos_tags):\n",
        "                    next_word, next_tag = pos_tags[idx + i]\n",
        "                    if next_tag.startswith(('N', 'M', 'Np')):  # Danh từ, Số, Danh từ riêng\n",
        "                        noun_phrase.append(next_word)\n",
        "                    else:\n",
        "                        break\n",
        "            \n",
        "            if noun_phrase:\n",
        "                location_name = \" \".join(noun_phrase)\n",
        "                locations.append(location_name)\n",
        "                self.locations.add(location_name)\n",
        "\n",
        "def _extract_taste_descriptions(self, sentence, words, tastes):\n",
        "    \"\"\"Trích xuất mô tả về hương vị từ một câu.\"\"\"\n",
        "    for taste_word in self.taste_indicators:\n",
        "        if taste_word in sentence.lower():\n",
        "            taste_idx = -1\n",
        "            for idx, word in enumerate(words):\n",
        "                if taste_word in word.lower():\n",
        "                    taste_idx = idx\n",
        "                    break\n",
        "            \n",
        "            if taste_idx >= 0:\n",
        "                start = max(0, taste_idx - 3)\n",
        "                end = min(len(words), taste_idx + 4)\n",
        "                taste_phrase = \" \".join(words[start:end])\n",
        "                tastes.append(taste_phrase)\n",
        "\n",
        "def extract_entities(self, text):\n",
        "    \"\"\"Trích xuất các thực thể về thực phẩm, địa điểm và hương vị từ văn bản.\"\"\"\n",
        "    if not text or not isinstance(text, str):\n",
        "        return {\"foods\": [], \"locations\": [], \"tastes\": []}\n",
        "\n",
        "    try:\n",
        "        results = {\"foods\": [], \"locations\": [], \"tastes\": []}\n",
        "\n",
        "        # Trích xuất địa điểm bằng NER\n",
        "        ner_locations = self.extract_entities_from_ner(text)\n",
        "        results[\"locations\"].extend(ner_locations)\n",
        "        self.locations.update(ner_locations)\n",
        "\n",
        "        # Trích xuất thực thể bằng phương pháp dựa trên mẫu\n",
        "        sentences = nltk.sent_tokenize(text)\n",
        "        pos_tags = [pos_tag(sent) for sent in sentences]\n",
        "\n",
        "        foods, locations, tastes = self.extract_entities_from_patterns(text, sentences, pos_tags)\n",
        "\n",
        "        results[\"foods\"].extend(foods)\n",
        "        results[\"locations\"].extend(locations)\n",
        "        results[\"tastes\"].extend(tastes)\n",
        "\n",
        "        # Cập nhật danh sách thực thể\n",
        "        self.foods.update(foods)\n",
        "        self.locations.update(locations)\n",
        "\n",
        "        # Loại bỏ trùng lặp và lọc bỏ chuỗi rỗng\n",
        "        for key in results:\n",
        "            results[key] = list(set(filter(None, results[key])))\n",
        "\n",
        "        return results\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Lỗi khi trích xuất thực thể: {e}\")\n",
        "        logging.error(traceback.format_exc())\n",
        "        return {\"foods\": [], \"locations\": [], \"tastes\": []}\n",
        "    \n",
        "VietnameseTextProcessor.extract_entities_from_ner = extract_entities_from_ner\n",
        "VietnameseTextProcessor.extract_entities_from_patterns = extract_entities_from_patterns\n",
        "VietnameseTextProcessor._extract_food_entities = _extract_food_entities\n",
        "VietnameseTextProcessor._extract_location_entities = _extract_location_entities\n",
        "VietnameseTextProcessor._extract_taste_descriptions = _extract_taste_descriptions\n",
        "VietnameseTextProcessor.extract_entities = extract_entities\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# DataFrame Processing and Bootstrapping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 226,
      "metadata": {},
      "outputs": [],
      "source": [
        "def process_dataframe(self, df, text_column=\"video_transcription\", batch_size=100):\n",
        "    \"\"\"\n",
        "    Xử lý toàn bộ DataFrame và trích xuất các thực thể.\n",
        "\n",
        "    Tham số:\n",
        "        df (pd.DataFrame): DataFrame chứa dữ liệu văn bản.\n",
        "        text_column (str): Tên cột chứa văn bản.\n",
        "        batch_size (int): Kích thước batch để xử lý nhằm tiết kiệm bộ nhớ.\n",
        "\n",
        "    Trả về:\n",
        "        pd.DataFrame: DataFrame gốc với các cột chứa thực thể được trích xuất.\n",
        "    \"\"\"\n",
        "    # Kiểm tra nếu DataFrame trống hoặc không có cột văn bản\n",
        "    if df.empty or text_column not in df.columns:\n",
        "        logging.error(f\"DataFrame không hợp lệ hoặc thiếu cột '{text_column}'\")\n",
        "        return df\n",
        "\n",
        "    # Tạo thư mục lưu trữ nếu chưa tồn tại\n",
        "    os.makedirs(\"extracted_data\", exist_ok=True)\n",
        "\n",
        "    # Khởi tạo các cột để lưu thực thể trích xuất\n",
        "    df['preprocessed_text'] = \"\"\n",
        "    df['extracted_foods'] = None\n",
        "    df['extracted_locations'] = None\n",
        "    df['extracted_tastes'] = None\n",
        "\n",
        "    total_batches = (len(df) + batch_size - 1) // batch_size  # Tính số batch cần xử lý\n",
        "\n",
        "    for i in tqdm(range(total_batches), desc=\"Đang xử lý batch\"):\n",
        "        start_idx = i * batch_size\n",
        "        end_idx = min((i + 1) * batch_size, len(df))\n",
        "\n",
        "        batch = df.iloc[start_idx:end_idx].copy()\n",
        "\n",
        "        # Tiền xử lý văn bản\n",
        "        batch['preprocessed_text'] = batch[text_column].apply(self.preprocess_text)\n",
        "\n",
        "        # Trích xuất thực thể\n",
        "        entities_list = []\n",
        "        for text in batch['preprocessed_text']:\n",
        "            entities_list.append(self.extract_entities(text))\n",
        "\n",
        "        # Cập nhật DataFrame với thực thể trích xuất\n",
        "        batch['extracted_foods'] = [data['foods'] for data in entities_list]\n",
        "        batch['extracted_locations'] = [data['locations'] for data in entities_list]\n",
        "        batch['extracted_tastes'] = [data['tastes'] for data in entities_list]\n",
        "\n",
        "        # Cập nhật vào DataFrame gốc\n",
        "        df.iloc[start_idx:end_idx] = batch\n",
        "\n",
        "        # Lưu kết quả tạm thời theo từng batch\n",
        "        if (i + 1) % 5 == 0 or (i + 1) == total_batches:\n",
        "            self.save_entity_list(self.foods, \"foods\")\n",
        "            self.save_entity_list(self.locations, \"locations\")\n",
        "\n",
        "            # Lưu kết quả trung gian\n",
        "            checkpoint_file = f\"extracted_data/processed_data_batch_{i+1}.csv\"\n",
        "            df.iloc[:end_idx].to_csv(checkpoint_file, index=False)\n",
        "            logging.info(f\"Đã lưu kết quả trung gian vào {checkpoint_file} sau batch {i+1}/{total_batches}\")\n",
        "\n",
        "    # Thống kê số lượng thực thể đã tìm thấy\n",
        "    food_count = len(self.foods)\n",
        "    location_count = len(self.locations)\n",
        "\n",
        "    logging.info(f\"Trích xuất hoàn tất. Tìm thấy {food_count} thực thể món ăn và {location_count} thực thể địa điểm.\")\n",
        "\n",
        "    return df\n",
        "\n",
        "def bootstrap_entity_lists(self, df, text_column=\"preprocessed_text\", min_freq=3):\n",
        "    \"\"\"\n",
        "    Mở rộng danh sách thực thể bằng TF-IDF để tìm các thực thể tiềm năng.\n",
        "    \n",
        "    Tham số:\n",
        "        df (pd.DataFrame): DataFrame chứa dữ liệu văn bản.\n",
        "        text_column (str): Tên cột chứa văn bản đã tiền xử lý.\n",
        "        min_freq (int): Số lần xuất hiện tối thiểu để xem xét một thực thể.\n",
        "\n",
        "    Trả về:\n",
        "        set: Tập hợp các thực thể món ăn mới được nhận diện.\n",
        "    \"\"\"\n",
        "    if df.empty or text_column not in df.columns:\n",
        "        logging.error(f\"Không thể mở rộng thực thể: DataFrame không hợp lệ hoặc thiếu cột '{text_column}'\")\n",
        "        return set()\n",
        "\n",
        "    # Lọc ra các văn bản hợp lệ\n",
        "    valid_texts = df[text_column].dropna().replace('', pd.NA).dropna().tolist()\n",
        "\n",
        "    if not valid_texts:\n",
        "        logging.warning(\"Không tìm thấy văn bản hợp lệ để mở rộng thực thể\")\n",
        "        return set()\n",
        "\n",
        "    try:\n",
        "        min_df_val = max(1, min(min_freq, len(valid_texts) // 2))\n",
        "        \n",
        "        tfidf = TfidfVectorizer(\n",
        "            ngram_range=(1, 3),  # Xét các n-gram từ 1 đến 3 từ\n",
        "            min_df=min_df_val,  # Điều chỉnh min_df\n",
        "            max_df=0.9  # Loại bỏ các cụm từ quá phổ biến\n",
        "        )\n",
        "\n",
        "        tfidf_matrix = tfidf.fit_transform(valid_texts)\n",
        "        feature_names = tfidf.get_feature_names_out()\n",
        "\n",
        "        # Lấy danh sách n-gram có giá trị TF-IDF cao\n",
        "        important_ngrams = []\n",
        "        for i in range(min(tfidf_matrix.shape[0], 100)):\n",
        "            feature_index = tfidf_matrix[i,:].nonzero()[1]\n",
        "            tfidf_scores = zip(feature_index, [tfidf_matrix[i, x] for x in feature_index])\n",
        "            # Sắp xếp theo điểm TF-IDF giảm dần\n",
        "            for idx, score in sorted(tfidf_scores, key=lambda x: x[1], reverse=True)[:20]:\n",
        "                important_ngrams.append(feature_names[idx])\n",
        "\n",
        "        # Lọc các cụm từ có thể là tên món ăn (dựa vào từ chỉ món ăn)\n",
        "        potential_foods = set()\n",
        "        for text in valid_texts:\n",
        "            for indicator in self.food_indicators:\n",
        "                if indicator in text:\n",
        "                    for ngram in important_ngrams:\n",
        "                        # Kiểm tra nếu ngram xuất hiện gần từ chỉ món ăn\n",
        "                        if ngram in text and re.search(r'\\b' + re.escape(indicator) + r'.{0,30}' + re.escape(ngram), text, re.IGNORECASE):\n",
        "                            potential_foods.add(ngram)\n",
        "                        if ngram in text and re.search(r'\\b' + re.escape(ngram) + r'.{0,30}' + re.escape(indicator), text, re.IGNORECASE):\n",
        "                            potential_foods.add(ngram)\n",
        "\n",
        "        # Lọc bỏ các thực thể không hợp lệ (quá ngắn, chỉ chứa số, v.v.)\n",
        "        filtered_foods = {food for food in potential_foods if len(food) > 2 and not food.isdigit()}\n",
        "\n",
        "        # Cập nhật danh sách món ăn\n",
        "        self.foods.update(filtered_foods)\n",
        "        logging.info(f\"Đã thêm {len(filtered_foods)} thực thể món ăn tiềm năng từ mở rộng thực thể\")\n",
        "\n",
        "        return filtered_foods\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Lỗi khi mở rộng thực thể: {e}\")\n",
        "        logging.error(traceback.format_exc())\n",
        "        return set()\n",
        "\n",
        "VietnameseTextProcessor.process_dataframe = process_dataframe\n",
        "VietnameseTextProcessor.bootstrap_entity_lists = bootstrap_entity_lists"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 227,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YI6pKb0WGWVC",
        "outputId": "174f413c-1aac-4c97-9d7b-a886256d1cf2"
      },
      "outputs": [],
      "source": [
        "def main():\n",
        "    try:\n",
        "        # Tạo một thể hiện của bộ xử lý văn bản\n",
        "        processor = VietnameseTextProcessor()\n",
        "\n",
        "        # Tải tập dữ liệu\n",
        "        logging.info(\"Đang tải tập dữ liệu...\")\n",
        "        try:\n",
        "            # df = pd.read_csv(\"/content/21KHDL-TikTok-Analytics/data/interim/small_video_transcription.csv\")\n",
        "            df = pd.read_csv(\"C:/Users/nguye/OneDrive/Tài liệu/GitHub/21KHDL-TikTok-Analytics/data/interim/small_video_transcription.csv\")\n",
        "            if df.empty:\n",
        "                logging.error(\"Tập dữ liệu được tải về trống\")\n",
        "                return\n",
        "            logging.info(f\"Tập dữ liệu đã tải có {len(df)} dòng\")\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Lỗi khi tải tập dữ liệu: {e}\")\n",
        "            logging.error(traceback.format_exc())\n",
        "            return\n",
        "\n",
        "        # Xử lý một mẫu nhỏ để kiểm thử (sử dụng .head(10) để thử nghiệm, xóa bỏ để xử lý toàn bộ)\n",
        "        sample_df = df.head(2)\n",
        "\n",
        "        # Xử lý dữ liệu văn bản\n",
        "        logging.info(\"Bắt đầu xử lý văn bản và trích xuất thực thể...\")\n",
        "        processed_df = processor.process_dataframe(sample_df, text_column='video_transcription')\n",
        "\n",
        "        # Mở rộng danh sách thực thể bằng phương pháp bootstrapping\n",
        "        logging.info(\"Thực hiện bootstrapping để mở rộng danh sách thực thể...\")\n",
        "        processor.bootstrap_entity_lists(processed_df)\n",
        "\n",
        "        # Lưu kết quả cuối cùng\n",
        "        processed_df.to_csv(\"extracted_data/fully_processed_data.csv\", index=False)\n",
        "        processor.save_entity_list(processor.foods, \"foods\")\n",
        "        processor.save_entity_list(processor.locations, \"locations\")\n",
        "\n",
        "        # Lưu kết quả có cấu trúc dưới dạng JSON gồm video_id, author_id và các thực thể trích xuất\n",
        "        structured_data = []\n",
        "        for _, row in processed_df.iterrows():\n",
        "            structured_data.append({\n",
        "                'video_id': row.get('video_id', ''),\n",
        "                'author_id': row.get('author_id', ''),\n",
        "                'extracted_entities': {\n",
        "                    'foods': row.get('extracted_foods', []),\n",
        "                    'locations': row.get('extracted_locations', []),\n",
        "                    'tastes': row.get('extracted_tastes', [])\n",
        "                }\n",
        "            })\n",
        "\n",
        "        with open(\"extracted_data/structured_entities.json\", 'w', encoding='utf-8') as f:\n",
        "            json.dump(structured_data, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "        logging.info(\"Quá trình xử lý hoàn tất. Kết quả đã được lưu trong thư mục 'extracted_data'.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Lỗi nghiêm trọng trong hàm main: {e}\")\n",
        "        logging.error(traceback.format_exc())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 228,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-03-09 00:22:05,955 - INFO - Không tìm thấy danh sách foods hiện có, bắt đầu với tập rỗng\n",
            "2025-03-09 00:22:05,956 - INFO - Không tìm thấy danh sách locations hiện có, bắt đầu với tập rỗng\n",
            "2025-03-09 00:22:05,957 - INFO - Đang tải tập dữ liệu...\n",
            "2025-03-09 00:22:06,203 - INFO - Tập dữ liệu đã tải có 10673 dòng\n",
            "2025-03-09 00:22:06,203 - INFO - Bắt đầu xử lý văn bản và trích xuất thực thể...\n",
            "C:\\Users\\nguye\\AppData\\Local\\Temp\\ipykernel_16656\\74775411.py:22: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df['preprocessed_text'] = \"\"\n",
            "C:\\Users\\nguye\\AppData\\Local\\Temp\\ipykernel_16656\\74775411.py:23: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df['extracted_foods'] = None\n",
            "C:\\Users\\nguye\\AppData\\Local\\Temp\\ipykernel_16656\\74775411.py:24: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df['extracted_locations'] = None\n",
            "C:\\Users\\nguye\\AppData\\Local\\Temp\\ipykernel_16656\\74775411.py:25: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df['extracted_tastes'] = None\n",
            "Đang xử lý batch:   0%|          | 0/1 [00:00<?, ?it/s]2025-03-09 00:22:06,208 - ERROR - Lỗi khi tiền xử lý văn bản: The provided dictionary language (vi) does not exist!\n",
            "2025-03-09 00:22:06,210 - ERROR - Lỗi khi tiền xử lý văn bản: The provided dictionary language (vi) does not exist!\n",
            "2025-03-09 00:22:08,643 - INFO - Đã lưu 23 foods vào extracted_data/foods_list.json\n",
            "2025-03-09 00:22:08,644 - INFO - Đã lưu 12 locations vào extracted_data/locations_list.json\n",
            "2025-03-09 00:22:08,648 - INFO - Đã lưu kết quả trung gian vào extracted_data/processed_data_batch_1.csv sau batch 1/1\n",
            "Đang xử lý batch: 100%|██████████| 1/1 [00:02<00:00,  2.44s/it]\n",
            "2025-03-09 00:22:08,649 - INFO - Trích xuất hoàn tất. Tìm thấy 23 thực thể món ăn và 12 thực thể địa điểm.\n",
            "2025-03-09 00:22:08,649 - INFO - Thực hiện bootstrapping để mở rộng danh sách thực thể...\n",
            "2025-03-09 00:22:08,779 - INFO - Đã thêm 33 thực thể món ăn tiềm năng từ mở rộng thực thể\n",
            "2025-03-09 00:22:08,782 - INFO - Đã lưu 53 foods vào extracted_data/foods_list.json\n",
            "2025-03-09 00:22:08,783 - INFO - Đã lưu 12 locations vào extracted_data/locations_list.json\n",
            "2025-03-09 00:22:08,784 - INFO - Quá trình xử lý hoàn tất. Kết quả đã được lưu trong thư mục 'extracted_data'.\n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "AppliedDataProject",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
