{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U91UVADr4Phh",
        "outputId": "f4a3bd1b-2a17-4e04-dbda-2bfbe125eeda"
      },
      "outputs": [],
      "source": [
        "# %%shell\n",
        "# git clone --branch TrggTin --single-branch https://github.com/vphuhan/21KHDL-TikTok-Analytics.git\n",
        "# cd 21KHDL-TikTok-Analytics\n",
        "# git sparse-checkout init --cone\n",
        "# git sparse-checkout set data/interim\n",
        "# git checkout"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0NZ7_Zrw5PFe",
        "outputId": "d4ed584f-a229-4763-aa43-af3871f0893e"
      },
      "outputs": [],
      "source": [
        "# pip install pandas nltk underthesea scikit-learn tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Imports and Initialization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "7GYwq-Tf4PTa"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\nguye\\.conda\\envs\\AppliedDataProject\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import unicodedata\n",
        "import nltk\n",
        "from underthesea import word_tokenize, pos_tag, ner\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from difflib import get_close_matches\n",
        "import logging\n",
        "import json\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import string\n",
        "import regex as re\n",
        "import traceback\n",
        "import jdc  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "pfZUR8vY4POT"
      },
      "outputs": [],
      "source": [
        "# Configure logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
        "    handlers=[\n",
        "        logging.FileHandler(\"extraction_log.log\"),\n",
        "        logging.StreamHandler()\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aDuHFYPU6NKm",
        "outputId": "2a87ddda-2e79-4d97-ed72-1a24c515c520"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt_tab to\n",
            "[nltk_data]     C:\\Users\\nguye\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# VietnameseTextProcessor Class Definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "class VietnameseTextProcessor:\n",
        "    def __init__(self, food_list_path=None, location_list_path=None):\n",
        "        \"\"\"\n",
        "        Initialize the Vietnamese Text Processor\n",
        "\n",
        "        Args:\n",
        "            food_list_path (str): Path to JSON file with known Vietnamese food items\n",
        "            location_list_path (str): Path to JSON file with known Vietnamese locations\n",
        "        \"\"\"\n",
        "        # Load or initialize food and location lists\n",
        "        self.foods = self._load_entity_list(food_list_path, \"foods\")\n",
        "        self.locations = self._load_entity_list(location_list_path, \"locations\")\n",
        "\n",
        "        # Common Vietnamese food and taste related words for bootstrapping\n",
        "        self.food_indicators = [\n",
        "            \"bánh\", \"phở\", \"bún\", \"xèo\", \"cơm\", \"gỏi\", \"chả\", \"xôi\", \"cao lầu\", \"cháo\",\n",
        "            \"mì\", \"hủ tiếu\", \"nem\", \"ram\", \"khọt\",\n",
        "            \"lẩu\", \"cá\", \"thịt\", \"canh\", \"rau\", \"đậu\", \"nướng\", \"ốc\", \"súp\", \"bắp\",\n",
        "            \"chuối\", \"nộm\", \"trà\", \"cà phê\", \"sinh tố\", \"kem\", \"tàu hủ\", \"chè\", \"yaourt\", \"nước mía\",\n",
        "            \"sữa\", \"kẹo\", \"đa\", \"nem chua\", \"gà\", \"món\", \"ăn\"\n",
        "        ]\n",
        "\n",
        "        self.taste_indicators = [\n",
        "            \"ngon\", \"ngọt\", \"chua\", \"cay\", \"đắng\", \"mặn\", \"bùi\", \"béo\", \"giòn\", \"mềm\",\n",
        "            \"thơm\", \"nồng\", \"đậm đà\", \"nhạt\", \"thanh\", \"tươi\", \"ướp\", \"rim\", \"kho\", \"xào\",\n",
        "            \"nướng\", \"luộc\", \"hấp\", \"chiên\", \"xốt\", \"tẩm\", \"ướt\", \"khô\", \"giòn tan\", \"dai\",\n",
        "            \"sần sật\", \"mọng nước\", \"đắng nghét\", \"chát\", \"cay xè\", \"tê\", \"mặn chát\", \"ngọt lịm\", \"béo ngậy\", \"thơm lừng\",\n",
        "            \"nồng nàn\", \"đậm vị\", \"nhạt nhẽo\", \"thanh mát\", \"tươi rói\", \"tươi ngon\", \"đậm đà hương vị\", \"vừa ăn\", \"hợp khẩu vị\"\n",
        "        ]\n",
        "\n",
        "        self.locations_indicators = [\n",
        "            \"Quận\", \"Huyện\", \"Phường\", \"Xã\", \"Thành phố\", \"TP\", \"Tỉnh\", \"đường\", \"phố\", \"chợ\", \"địa chỉ\", \"nằm ở\", \"tại\",\n",
        "            \"Quận 1\", \"Quận 2\", \"Quận 3\", \"Quận 4\", \"Quận 5\", \"Quận 6\", \"Quận 7\", \"Quận 8\", \"Quận 9\", \"Quận 10\",\n",
        "            \"Quận 11\", \"Quận 12\", \"Bình Thạnh\", \"Tân Bình\", \"Tân Phú\", \"Phú Nhuận\", \"Gò Vấp\", \"Bình Tân\", \"Thủ Đức\", \"Hóc Môn\",\n",
        "            \"Củ Chi\", \"Nhà Bè\", \"Cần Giờ\", \"Bình Chánh\", \"TP Thủ Đức\",\n",
        "            \"Hà Nội\", \"Hồ Chí Minh\", \"Đà Nẵng\", \"Hải Phòng\", \"Cần Thơ\", \"Huế\", \"Nha Trang\", \"Vũng Tàu\", \"Đà Lạt\",\n",
        "            \"Hạ Long\", \"Mỹ Tho\", \"Long Xuyên\", \"Rạch Giá\", \"Cà Mau\", \"Biên Hòa\", \"Buôn Ma Thuột\", \"Thái Nguyên\", \"Nam Định\"\n",
        "        ]\n",
        "\n",
        "        # Load NLTK resources if needed\n",
        "        try:\n",
        "            nltk.data.find('tokenizers/punkt')\n",
        "        except LookupError:\n",
        "            nltk.download('punkt')\n",
        "\n",
        "        # Create directory for output files\n",
        "        os.makedirs(\"extracted_data\", exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Helper Methods"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "def _load_entity_list(self, file_path, entity_type):\n",
        "    \"\"\"Load entity list from file or return default empty set\"\"\"\n",
        "    if file_path and os.path.exists(file_path):\n",
        "        try:\n",
        "            with open(file_path, 'r', encoding='utf-8') as f:\n",
        "                return set(json.load(f))\n",
        "        except Exception as e:\n",
        "            logging.warning(f\"Error loading {entity_type} list: {e}\")\n",
        "\n",
        "    logging.info(f\"No existing {entity_type} list found, starting with empty set\")\n",
        "    return set()\n",
        "\n",
        "def save_entity_list(self, entity_list, entity_type):\n",
        "    \"\"\"Save updated entity list to file\"\"\"\n",
        "    file_path = f\"extracted_data/{entity_type}_list.json\"\n",
        "    with open(file_path, 'w', encoding='utf-8') as f:\n",
        "        json.dump(list(entity_list), f, ensure_ascii=False, indent=2)\n",
        "    logging.info(f\"Saved {len(entity_list)} {entity_type} to {file_path}\")\n",
        "\n",
        "def normalize_vietnamese_text(self, text):\n",
        "    \"\"\"Normalize Vietnamese text by handling diacritics and case\"\"\"\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "\n",
        "    # Normalize Unicode characters\n",
        "    text = unicodedata.normalize('NFC', text)\n",
        "\n",
        "    # Remove extra whitespace\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    return text\n",
        "\n",
        "def clean_text(self, text):\n",
        "    \"\"\"Clean the text by removing special characters and normalizing\"\"\"\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "\n",
        "    # Normalize text\n",
        "    text = self.normalize_vietnamese_text(text)\n",
        "\n",
        "    # Remove URLs\n",
        "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
        "\n",
        "    # Remove emojis and special characters while preserving Vietnamese characters\n",
        "    # This regex keeps Vietnamese letters, numbers, punctuation and whitespace\n",
        "    vietnamese_pattern = r'[^\\p{L}\\p{N}\\p{P}\\s]+'\n",
        "    text = re.sub(vietnamese_pattern, '', text, flags=re.UNICODE)\n",
        "\n",
        "    # Fix spacing around punctuation\n",
        "    text = re.sub(r'\\s+([.,;:?!])', r'\\1', text)\n",
        "\n",
        "    return text\n",
        "\n",
        "def correct_common_misspellings(self, text):\n",
        "    \"\"\"Attempt to correct common misspellings in Vietnamese text\"\"\"\n",
        "    words = word_tokenize(text)\n",
        "    corrected_words = []\n",
        "\n",
        "    # Dictionary of common misspellings\n",
        "    common_corrections = {\n",
        "        \"pho\": \"phở\",\n",
        "        \"bun\": \"bún\",\n",
        "        \"banh\": \"bánh\",\n",
        "        \"com\": \"cơm\",\n",
        "        \"hu tieu\": \"hủ tiếu\",\n",
        "        \"goi\": \"gỏi\",\n",
        "        \"ca phe\": \"cà phê\",\n",
        "        \"nuoc mia\": \"nước mía\",\n",
        "        \"che\": \"chè\",\n",
        "    }\n",
        "\n",
        "    for word in words:\n",
        "        # Check if word is in common corrections\n",
        "        if word.lower() in common_corrections:\n",
        "            corrected_words.append(common_corrections[word.lower()])\n",
        "        else:\n",
        "            corrected_words.append(word)\n",
        "\n",
        "    return \" \".join(corrected_words)\n",
        "\n",
        "def preprocess_text(self, text):\n",
        "    \"\"\"Apply all preprocessing steps to the text\"\"\"\n",
        "    if not isinstance(text, str) or not text.strip():\n",
        "        return \"\"\n",
        "\n",
        "    try:\n",
        "        text = self.clean_text(text)\n",
        "        text = self.correct_common_misspellings(text)\n",
        "        return text\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error preprocessing text: {e}\")\n",
        "        logging.error(traceback.format_exc())\n",
        "        return text if isinstance(text, str) else \"\"\n",
        "    \n",
        "VietnameseTextProcessor._load_entity_list = _load_entity_list\n",
        "VietnameseTextProcessor.save_entity_list = save_entity_list \n",
        "VietnameseTextProcessor.normalize_vietnamese_text = normalize_vietnamese_text\n",
        "VietnameseTextProcessor.clean_text = clean_text\n",
        "VietnameseTextProcessor.correct_common_misspellings = correct_common_misspellings\n",
        "VietnameseTextProcessor.preprocess_text = preprocess_text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Entity Extraction Methods"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_entities_from_ner(self, text):\n",
        "    \"\"\"Extract entities using underthesea NER\"\"\"\n",
        "    locations = []\n",
        "\n",
        "    try:\n",
        "        ner_tags = ner(text)\n",
        "\n",
        "        # Check if ner_tags has the expected format\n",
        "        if not ner_tags:\n",
        "            return locations\n",
        "\n",
        "        # Extract locations from NER\n",
        "        current_loc = []\n",
        "\n",
        "        for item in ner_tags:\n",
        "            # Handle different output formats from NER\n",
        "            if isinstance(item, (list, tuple)) and len(item) == 2:\n",
        "                word, tag = item\n",
        "            else:\n",
        "                continue\n",
        "\n",
        "            if tag.startswith('B-LOC'):\n",
        "                if current_loc:\n",
        "                    locations.append(' '.join(current_loc))\n",
        "                    current_loc = []\n",
        "                current_loc.append(word)\n",
        "            elif tag.startswith('I-LOC') and current_loc:\n",
        "                current_loc.append(word)\n",
        "            elif current_loc:\n",
        "                locations.append(' '.join(current_loc))\n",
        "                current_loc = []\n",
        "\n",
        "        # Add the last location if it exists\n",
        "        if current_loc:\n",
        "            locations.append(' '.join(current_loc))\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error in NER extraction: {e}\")\n",
        "        logging.error(traceback.format_exc())\n",
        "\n",
        "    return locations\n",
        "\n",
        "def extract_entities_from_patterns(self, text, sentences, pos_tags):\n",
        "    \"\"\"Extract entities using pattern matching\"\"\"\n",
        "    foods = []\n",
        "    locations = []\n",
        "    tastes = []\n",
        "\n",
        "    # Process each sentence for entity extraction\n",
        "    for idx, sentence in enumerate(sentences):\n",
        "        words = word_tokenize(sentence)\n",
        "        sentence_pos_tags = pos_tags[idx] if idx < len(pos_tags) else []\n",
        "\n",
        "        # Find food entities\n",
        "        self._extract_food_entities(sentence, sentence_pos_tags, foods)\n",
        "\n",
        "        # Find location entities\n",
        "        self._extract_location_entities(sentence, sentence_pos_tags, locations)\n",
        "\n",
        "        # Find taste descriptions\n",
        "        self._extract_taste_descriptions(sentence, words, tastes)\n",
        "\n",
        "    return foods, locations, tastes\n",
        "\n",
        "def _extract_food_entities(self, sentence, pos_tags, foods):\n",
        "    \"\"\"Extract food entities from a sentence\"\"\"\n",
        "    # Look for direct matches from existing food list\n",
        "    for food in self.foods:\n",
        "        if food.lower() in sentence.lower():\n",
        "            foods.append(food)\n",
        "\n",
        "    # Look for food indicator words\n",
        "    for idx, (word, tag) in enumerate(pos_tags):\n",
        "        if word.lower() in self.food_indicators:\n",
        "            # Look ahead for potential food name (noun phrases)\n",
        "            noun_phrase = []\n",
        "            for i in range(1, 4):\n",
        "                if idx + i < len(pos_tags):\n",
        "                    next_word, next_tag = pos_tags[idx + i]\n",
        "                    if next_tag.startswith(('N', 'A')):  # Noun or Adjective\n",
        "                        noun_phrase.append(next_word)\n",
        "                    else:\n",
        "                        break\n",
        "\n",
        "            if noun_phrase:\n",
        "                food_name = \" \".join(noun_phrase)\n",
        "                foods.append(food_name)\n",
        "                self.foods.add(food_name)\n",
        "\n",
        "def _extract_location_entities(self, sentence, pos_tags, locations):\n",
        "    \"\"\"Extract location entities from a sentence\"\"\"\n",
        "    # Look for direct matches from existing location list\n",
        "    for location in self.locations:\n",
        "        if location.lower() in sentence.lower():\n",
        "            locations.append(location)\n",
        "\n",
        "    # Look for location indicator words\n",
        "    for idx, (word, tag) in enumerate(pos_tags):\n",
        "        if any(indicator.lower() in word.lower() for indicator in self.locations_indicators):\n",
        "            # Look ahead for potential location name (noun phrases)\n",
        "            noun_phrase = []\n",
        "            for i in range(1, 4):\n",
        "                if idx + i < len(pos_tags):\n",
        "                    next_word, next_tag = pos_tags[idx + i]\n",
        "                    if next_tag.startswith(('N', 'M', 'Np')):  # Noun, Number, Proper noun\n",
        "                        noun_phrase.append(next_word)\n",
        "                    else:\n",
        "                        break\n",
        "\n",
        "            if noun_phrase:\n",
        "                location_name = \" \".join(noun_phrase)\n",
        "                locations.append(location_name)\n",
        "                self.locations.add(location_name)\n",
        "\n",
        "def _extract_taste_descriptions(self, sentence, words, tastes):\n",
        "    \"\"\"Extract taste descriptions from a sentence\"\"\"\n",
        "    for taste_word in self.taste_indicators:\n",
        "        if taste_word in sentence.lower():\n",
        "            # Find the position of the taste word\n",
        "            taste_idx = -1\n",
        "            for idx, word in enumerate(words):\n",
        "                if taste_word in word.lower():\n",
        "                    taste_idx = idx\n",
        "                    break\n",
        "\n",
        "            if taste_idx >= 0:\n",
        "                # Extract surrounding context\n",
        "                start = max(0, taste_idx - 3)\n",
        "                end = min(len(words), taste_idx + 4)  # Increased range\n",
        "                taste_phrase = \" \".join(words[start:end])\n",
        "                tastes.append(taste_phrase)\n",
        "\n",
        "def extract_entities(self, text):\n",
        "    \"\"\"Extract food, location, and taste entities from text\"\"\"\n",
        "    if not text or not isinstance(text, str):\n",
        "        return {\"foods\": [], \"locations\": [], \"tastes\": []}\n",
        "\n",
        "    try:\n",
        "        results = {\n",
        "            \"foods\": [],\n",
        "            \"locations\": [],\n",
        "            \"tastes\": []\n",
        "        }\n",
        "\n",
        "        # Extract locations using NER\n",
        "        ner_locations = self.extract_entities_from_ner(text)\n",
        "        results[\"locations\"].extend(ner_locations)\n",
        "        self.locations.update(ner_locations)\n",
        "\n",
        "        # Extract entities using pattern matching\n",
        "        sentences = nltk.sent_tokenize(text)\n",
        "        pos_tags = [pos_tag(sent) for sent in sentences]\n",
        "\n",
        "        foods, locations, tastes = self.extract_entities_from_patterns(text, sentences, pos_tags)\n",
        "\n",
        "        results[\"foods\"].extend(foods)\n",
        "        results[\"locations\"].extend(locations)\n",
        "        results[\"tastes\"].extend(tastes)\n",
        "\n",
        "        # Update entity sets\n",
        "        self.foods.update(foods)\n",
        "        self.locations.update(locations)\n",
        "\n",
        "        # Remove duplicates and filter out empty strings\n",
        "        for key in results:\n",
        "            results[key] = list(set(filter(None, results[key])))\n",
        "\n",
        "        return results\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error extracting entities: {e}\")\n",
        "        logging.error(traceback.format_exc())\n",
        "        return {\"foods\": [], \"locations\": [], \"tastes\": []}\n",
        "    \n",
        "VietnameseTextProcessor.extract_entities_from_ner = extract_entities_from_ner\n",
        "VietnameseTextProcessor.extract_entities_from_patterns = extract_entities_from_patterns\n",
        "VietnameseTextProcessor._extract_food_entities = _extract_food_entities\n",
        "VietnameseTextProcessor._extract_location_entities = _extract_location_entities\n",
        "VietnameseTextProcessor._extract_taste_descriptions = _extract_taste_descriptions\n",
        "VietnameseTextProcessor.extract_entities = extract_entities"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# DataFrame Processing and Bootstrapping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "def process_dataframe(self, df, text_column=\"video_transcription\", batch_size=100):\n",
        "    \"\"\"Process the entire dataframe and extract entities\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): Dataframe with text data\n",
        "        text_column (str): Name of the column containing text\n",
        "        batch_size (int): Process in batches to save memory\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: Original dataframe with extracted entity columns\n",
        "    \"\"\"\n",
        "    # Check if dataframe is empty or text column doesn't exist\n",
        "    if df.empty or text_column not in df.columns:\n",
        "        logging.error(f\"Invalid dataframe or missing column '{text_column}'\")\n",
        "        return df\n",
        "\n",
        "    # Create output directory if it doesn't exist\n",
        "    os.makedirs(\"extracted_data\", exist_ok=True)\n",
        "\n",
        "    # Initialize columns for extracted entities\n",
        "    df['preprocessed_text'] = \"\"\n",
        "    df['extracted_foods'] = None\n",
        "    df['extracted_locations'] = None\n",
        "    df['extracted_tastes'] = None\n",
        "\n",
        "    total_batches = (len(df) + batch_size - 1) // batch_size\n",
        "\n",
        "    for i in tqdm(range(total_batches), desc=\"Processing batches\"):\n",
        "        start_idx = i * batch_size\n",
        "        end_idx = min((i + 1) * batch_size, len(df))\n",
        "\n",
        "        batch = df.iloc[start_idx:end_idx].copy()\n",
        "\n",
        "        # Preprocess texts\n",
        "        batch['preprocessed_text'] = batch[text_column].apply(self.preprocess_text)\n",
        "\n",
        "        # Extract entities\n",
        "        entities_list = []\n",
        "        for text in batch['preprocessed_text']:\n",
        "            entities_list.append(self.extract_entities(text))\n",
        "\n",
        "        # Update dataframe with extracted entities\n",
        "        batch['extracted_foods'] = [data['foods'] for data in entities_list]\n",
        "        batch['extracted_locations'] = [data['locations'] for data in entities_list]\n",
        "        batch['extracted_tastes'] = [data['tastes'] for data in entities_list]\n",
        "\n",
        "        # Update original dataframe\n",
        "        df.iloc[start_idx:end_idx] = batch\n",
        "\n",
        "        # Save intermediate results periodically\n",
        "        if (i + 1) % 5 == 0 or (i + 1) == total_batches:\n",
        "            self.save_entity_list(self.foods, \"foods\")\n",
        "            self.save_entity_list(self.locations, \"locations\")\n",
        "\n",
        "            # Save intermediate results\n",
        "            checkpoint_file = f\"extracted_data/processed_data_batch_{i+1}.csv\"\n",
        "            df.iloc[:end_idx].to_csv(checkpoint_file, index=False)\n",
        "            logging.info(f\"Saved intermediate results to {checkpoint_file} after batch {i+1}/{total_batches}\")\n",
        "\n",
        "    # Generate statistics\n",
        "    food_count = len(self.foods)\n",
        "    location_count = len(self.locations)\n",
        "\n",
        "    logging.info(f\"Extraction complete. Found {food_count} unique foods and {location_count} unique locations.\")\n",
        "\n",
        "    return df\n",
        "\n",
        "def bootstrap_entity_lists(self, df, text_column=\"preprocessed_text\", min_freq=3):\n",
        "    \"\"\"Bootstrap entity lists using TF-IDF to find potential new entities\"\"\"\n",
        "    if df.empty or text_column not in df.columns:\n",
        "        logging.error(f\"Cannot bootstrap: Invalid dataframe or missing column '{text_column}'\")\n",
        "        return set()\n",
        "\n",
        "    # Filter out empty texts\n",
        "    valid_texts = df[text_column].dropna().replace('', pd.NA).dropna().tolist()\n",
        "\n",
        "    if not valid_texts:\n",
        "        logging.warning(\"No valid texts found for bootstrapping\")\n",
        "        return set()\n",
        "\n",
        "    try:\n",
        "        # Use TF-IDF to find important n-grams\n",
        "        tfidf = TfidfVectorizer(\n",
        "            ngram_range=(1, 3),\n",
        "            min_df=min_freq,\n",
        "            max_df=0.9\n",
        "        )\n",
        "\n",
        "        tfidf_matrix = tfidf.fit_transform(valid_texts)\n",
        "        feature_names = tfidf.get_feature_names_out()\n",
        "\n",
        "        # Get high TF-IDF n-grams\n",
        "        important_ngrams = []\n",
        "        for i in range(min(tfidf_matrix.shape[0], 100)):  # Limit to first 100 docs for efficiency\n",
        "            feature_index = tfidf_matrix[i,:].nonzero()[1]\n",
        "            tfidf_scores = zip(feature_index, [tfidf_matrix[i, x] for x in feature_index])\n",
        "            # Sort by descending score\n",
        "            for idx, score in sorted(tfidf_scores, key=lambda x: x[1], reverse=True)[:20]:\n",
        "                important_ngrams.append(feature_names[idx])\n",
        "\n",
        "        # Filter to likely food names (those appearing near food indicators)\n",
        "        potential_foods = set()\n",
        "        for text in valid_texts:\n",
        "            for indicator in self.food_indicators:\n",
        "                if indicator in text:\n",
        "                    for ngram in important_ngrams:\n",
        "                        # Check if ngram is near a food indicator\n",
        "                        if ngram in text and re.search(r'\\b' + re.escape(indicator) + r'.{0,30}' + re.escape(ngram), text, re.IGNORECASE):\n",
        "                            potential_foods.add(ngram)\n",
        "\n",
        "                        # Also check for ngrams that appear before the indicator\n",
        "                        if ngram in text and re.search(r'\\b' + re.escape(ngram) + r'.{0,30}' + re.escape(indicator), text, re.IGNORECASE):\n",
        "                            potential_foods.add(ngram)\n",
        "\n",
        "        # Filter out unlikely food candidates (too short, numbers only, etc.)\n",
        "        filtered_foods = {food for food in potential_foods if len(food) > 2 and not food.isdigit()}\n",
        "\n",
        "        # Update food list\n",
        "        self.foods.update(filtered_foods)\n",
        "        logging.info(f\"Added {len(filtered_foods)} potential foods from bootstrapping\")\n",
        "\n",
        "        return filtered_foods\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error in bootstrapping: {e}\")\n",
        "        logging.error(traceback.format_exc())\n",
        "        return set()\n",
        "\n",
        "VietnameseTextProcessor.process_dataframe = process_dataframe\n",
        "VietnameseTextProcessor.bootstrap_entity_lists = bootstrap_entity_lists"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YI6pKb0WGWVC",
        "outputId": "174f413c-1aac-4c97-9d7b-a886256d1cf2"
      },
      "outputs": [],
      "source": [
        "def main():\n",
        "    try:\n",
        "        # Create processor instance\n",
        "        processor = VietnameseTextProcessor()\n",
        "\n",
        "        # Load dataset\n",
        "        logging.info(\"Loading dataset...\")\n",
        "        try:\n",
        "            # df = pd.read_csv(\"/content/21KHDL-TikTok-Analytics/data/interim/small_video_transcription.csv\")\n",
        "            df = pd.read_csv(\"C:/Users/nguye/OneDrive/Tài liệu/GitHub/21KHDL-TikTok-Analytics/data/interim/small_video_transcription.csv\")\n",
        "            if df.empty:\n",
        "                logging.error(\"Loaded dataset is empty\")\n",
        "                return\n",
        "            logging.info(f\"Loaded dataset with {len(df)} rows\")\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Error loading dataset: {e}\")\n",
        "            logging.error(traceback.format_exc())\n",
        "            return\n",
        "\n",
        "        # Process a sample for testing (use .head(10) for testing, remove for full processing)\n",
        "        sample_df = df.head(2)\n",
        "\n",
        "        # Process the dataframe\n",
        "        logging.info(\"Starting text processing and entity extraction...\")\n",
        "        processed_df = processor.process_dataframe(sample_df, text_column='video_transcription')\n",
        "\n",
        "        # Bootstrap to find more potential entities\n",
        "        logging.info(\"Bootstrapping to expand entity lists...\")\n",
        "        processor.bootstrap_entity_lists(processed_df)\n",
        "\n",
        "        # Save final results\n",
        "        processed_df.to_csv(\"extracted_data/fully_processed_data.csv\", index=False)\n",
        "        processor.save_entity_list(processor.foods, \"foods\")\n",
        "        processor.save_entity_list(processor.locations, \"locations\")\n",
        "\n",
        "        # Save a structured JSON with video_id, author_id and extracted entities\n",
        "        structured_data = []\n",
        "        for _, row in processed_df.iterrows():\n",
        "            structured_data.append({\n",
        "                'video_id': row.get('video_id', ''),\n",
        "                'author_id': row.get('author_id', ''),\n",
        "                'extracted_entities': {\n",
        "                    'foods': row.get('extracted_foods', []),\n",
        "                    'locations': row.get('extracted_locations', []),\n",
        "                    'tastes': row.get('extracted_tastes', [])\n",
        "                }\n",
        "            })\n",
        "\n",
        "        with open(\"extracted_data/structured_entities.json\", 'w', encoding='utf-8') as f:\n",
        "            json.dump(structured_data, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "        logging.info(\"Processing complete. Results saved to 'extracted_data' directory.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Fatal error in main function: {e}\")\n",
        "        logging.error(traceback.format_exc())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-03-08 16:34:10,367 - INFO - No existing foods list found, starting with empty set\n",
            "2025-03-08 16:34:10,367 - INFO - No existing locations list found, starting with empty set\n",
            "2025-03-08 16:34:10,370 - INFO - Loading dataset...\n",
            "2025-03-08 16:34:10,648 - INFO - Loaded dataset with 10673 rows\n",
            "2025-03-08 16:34:10,649 - INFO - Starting text processing and entity extraction...\n",
            "C:\\Users\\nguye\\AppData\\Local\\Temp\\ipykernel_6636\\763373256.py:21: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df['preprocessed_text'] = \"\"\n",
            "C:\\Users\\nguye\\AppData\\Local\\Temp\\ipykernel_6636\\763373256.py:22: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df['extracted_foods'] = None\n",
            "C:\\Users\\nguye\\AppData\\Local\\Temp\\ipykernel_6636\\763373256.py:23: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df['extracted_locations'] = None\n",
            "C:\\Users\\nguye\\AppData\\Local\\Temp\\ipykernel_6636\\763373256.py:24: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df['extracted_tastes'] = None\n",
            "Processing batches:   0%|          | 0/1 [00:00<?, ?it/s]2025-03-08 16:34:13,410 - INFO - Saved 12 foods to extracted_data/foods_list.json\n",
            "2025-03-08 16:34:13,412 - INFO - Saved 5 locations to extracted_data/locations_list.json\n",
            "2025-03-08 16:34:13,419 - INFO - Saved intermediate results to extracted_data/processed_data_batch_1.csv after batch 1/1\n",
            "Processing batches: 100%|██████████| 1/1 [00:02<00:00,  2.77s/it]\n",
            "2025-03-08 16:34:13,421 - INFO - Extraction complete. Found 12 unique foods and 5 unique locations.\n",
            "2025-03-08 16:34:13,421 - INFO - Bootstrapping to expand entity lists...\n",
            "2025-03-08 16:34:13,426 - ERROR - Error in bootstrapping: max_df corresponds to < documents than min_df\n",
            "2025-03-08 16:34:13,458 - ERROR - Traceback (most recent call last):\n",
            "  File \"C:\\Users\\nguye\\AppData\\Local\\Temp\\ipykernel_6636\\763373256.py\", line 89, in bootstrap_entity_lists\n",
            "    tfidf_matrix = tfidf.fit_transform(valid_texts)\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\nguye\\.conda\\envs\\AppliedDataProject\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 2104, in fit_transform\n",
            "    X = super().fit_transform(raw_documents)\n",
            "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\nguye\\.conda\\envs\\AppliedDataProject\\Lib\\site-packages\\sklearn\\base.py\", line 1389, in wrapper\n",
            "    return fit_method(estimator, *args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\nguye\\.conda\\envs\\AppliedDataProject\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1386, in fit_transform\n",
            "    raise ValueError(\"max_df corresponds to < documents than min_df\")\n",
            "ValueError: max_df corresponds to < documents than min_df\n",
            "\n",
            "2025-03-08 16:34:13,460 - INFO - Saved 12 foods to extracted_data/foods_list.json\n",
            "2025-03-08 16:34:13,461 - INFO - Saved 5 locations to extracted_data/locations_list.json\n",
            "2025-03-08 16:34:13,463 - INFO - Processing complete. Results saved to 'extracted_data' directory.\n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "AppliedDataProject",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
