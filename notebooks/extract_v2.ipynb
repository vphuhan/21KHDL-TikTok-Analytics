{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U91UVADr4Phh",
        "outputId": "f4a3bd1b-2a17-4e04-dbda-2bfbe125eeda"
      },
      "outputs": [],
      "source": [
        "# %%shell\n",
        "# git clone --branch TrggTin --single-branch https://github.com/vphuhan/21KHDL-TikTok-Analytics.git\n",
        "# cd 21KHDL-TikTok-Analytics\n",
        "# git sparse-checkout init --cone\n",
        "# git sparse-checkout set data/interim\n",
        "# git checkout"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0NZ7_Zrw5PFe",
        "outputId": "d4ed584f-a229-4763-aa43-af3871f0893e"
      },
      "outputs": [],
      "source": [
        "# pip install pandas nltk underthesea scikit-learn tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Imports and Initialization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "7GYwq-Tf4PTa"
      },
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "partially initialized module 'pandas' has no attribute '_pandas_parser_CAPI' (most likely due to a circular import)",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[12], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01municodedata\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\nguye\\.conda\\envs\\AppliedDataProject\\Lib\\site-packages\\pandas\\__init__.py:138\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcomputation\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;28meval\u001b[39m\n\u001b[0;32m    121\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mreshape\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m    122\u001b[0m     concat,\n\u001b[0;32m    123\u001b[0m     lreshape,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    135\u001b[0m     qcut,\n\u001b[0;32m    136\u001b[0m )\n\u001b[1;32m--> 138\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m api, arrays, errors, io, plotting, tseries\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m testing\n\u001b[0;32m    140\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_print_versions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m show_versions\n",
            "File \u001b[1;32mc:\\Users\\nguye\\.conda\\envs\\AppliedDataProject\\Lib\\site-packages\\pandas\\api\\__init__.py:2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\" public toolkit API \"\"\"\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      3\u001b[0m     extensions,\n\u001b[0;32m      4\u001b[0m     indexers,\n\u001b[0;32m      5\u001b[0m     interchange,\n\u001b[0;32m      6\u001b[0m     types,\n\u001b[0;32m      7\u001b[0m     typing,\n\u001b[0;32m      8\u001b[0m )\n\u001b[0;32m     10\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minterchange\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mextensions\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtyping\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     16\u001b[0m ]\n",
            "File \u001b[1;32mc:\\Users\\nguye\\.conda\\envs\\AppliedDataProject\\Lib\\site-packages\\pandas\\api\\typing\\__init__.py:31\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwindow\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     20\u001b[0m     Expanding,\n\u001b[0;32m     21\u001b[0m     ExpandingGroupby,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     26\u001b[0m     Window,\n\u001b[0;32m     27\u001b[0m )\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# TODO: Can't import Styler without importing jinja2\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# from pandas.io.formats.style import Styler\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mjson\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_json\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m JsonReader\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StataReader\n\u001b[0;32m     34\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataFrameGroupBy\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatetimeIndexResamplerGroupby\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWindow\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     55\u001b[0m ]\n",
            "File \u001b[1;32mc:\\Users\\nguye\\.conda\\envs\\AppliedDataProject\\Lib\\site-packages\\pandas\\io\\json\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mjson\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_json\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      2\u001b[0m     read_json,\n\u001b[0;32m      3\u001b[0m     to_json,\n\u001b[0;32m      4\u001b[0m     ujson_dumps,\n\u001b[0;32m      5\u001b[0m     ujson_loads,\n\u001b[0;32m      6\u001b[0m )\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mjson\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_table_schema\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m build_table_schema\n\u001b[0;32m      9\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mujson_dumps\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mujson_loads\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbuild_table_schema\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     15\u001b[0m ]\n",
            "File \u001b[1;32mc:\\Users\\nguye\\.conda\\envs\\AppliedDataProject\\Lib\\site-packages\\pandas\\io\\json\\_json.py:71\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mjson\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_normalize\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m convert_to_line_delimits\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mjson\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_table_schema\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     68\u001b[0m     build_table_schema,\n\u001b[0;32m     69\u001b[0m     parse_table_schema,\n\u001b[0;32m     70\u001b[0m )\n\u001b[1;32m---> 71\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparsers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mreaders\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m validate_integer\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n\u001b[0;32m     74\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcollections\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mabc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     75\u001b[0m         Hashable,\n\u001b[0;32m     76\u001b[0m         Mapping,\n\u001b[0;32m     77\u001b[0m     )\n",
            "File \u001b[1;32mc:\\Users\\nguye\\.conda\\envs\\AppliedDataProject\\Lib\\site-packages\\pandas\\io\\parsers\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparsers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mreaders\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      2\u001b[0m     TextFileReader,\n\u001b[0;32m      3\u001b[0m     TextParser,\n\u001b[0;32m      4\u001b[0m     read_csv,\n\u001b[0;32m      5\u001b[0m     read_fwf,\n\u001b[0;32m      6\u001b[0m     read_table,\n\u001b[0;32m      7\u001b[0m )\n\u001b[0;32m      9\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTextFileReader\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTextParser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread_csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread_fwf\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread_table\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
            "File \u001b[1;32mc:\\Users\\nguye\\.conda\\envs\\AppliedDataProject\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:32\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_config\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m using_copy_on_write\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m lib\n\u001b[1;32m---> 32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparsers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m STR_NA_VALUES\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01merrors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     34\u001b[0m     AbstractMethodError,\n\u001b[0;32m     35\u001b[0m     ParserWarning,\n\u001b[0;32m     36\u001b[0m )\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_decorators\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Appender\n",
            "File \u001b[1;32mparsers.pyx:1418\u001b[0m, in \u001b[0;36minit pandas._libs.parsers\u001b[1;34m()\u001b[0m\n",
            "\u001b[1;31mAttributeError\u001b[0m: partially initialized module 'pandas' has no attribute '_pandas_parser_CAPI' (most likely due to a circular import)"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import unicodedata\n",
        "import nltk\n",
        "from underthesea import word_tokenize, pos_tag, ner\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from difflib import get_close_matches\n",
        "import logging\n",
        "import json\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import string\n",
        "import regex as re\n",
        "import traceback\n",
        "import jdc  \n",
        "from spellchecker import SpellChecker\n",
        "from datetime import datetime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pfZUR8vY4POT"
      },
      "outputs": [],
      "source": [
        "# Configure logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
        "    handlers=[\n",
        "        logging.FileHandler(\"extraction_log.log\"),\n",
        "        logging.StreamHandler()\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aDuHFYPU6NKm",
        "outputId": "2a87ddda-2e79-4d97-ed72-1a24c515c520"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt_tab to\n",
            "[nltk_data]     C:\\Users\\nguye\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 207,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# VietnameseTextProcessor Class Definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class VietnameseTextProcessor:\n",
        "    def __init__(self, food_list_path=None, location_list_path=None):\n",
        "        \"\"\"\n",
        "        Kh·ªüi t·∫°o B·ªô x·ª≠ l√Ω vƒÉn b·∫£n ti·∫øng Vi·ªát\n",
        "\n",
        "        Tham s·ªë:\n",
        "            food_list_path (str): ƒê∆∞·ªùng d·∫´n ƒë·∫øn t·ªáp JSON ch·ª©a danh s√°ch m√≥n ƒÉn Vi·ªát Nam\n",
        "            location_list_path (str): ƒê∆∞·ªùng d·∫´n ƒë·∫øn t·ªáp JSON ch·ª©a danh s√°ch ƒë·ªãa ƒëi·ªÉm ·ªü Vi·ªát Nam\n",
        "        \"\"\"\n",
        "        # T·∫£i ho·∫∑c kh·ªüi t·∫°o danh s√°ch m√≥n ƒÉn v√† ƒë·ªãa ƒëi·ªÉm\n",
        "        self.foods = self._load_entity_list(food_list_path, \"foods\")\n",
        "        self.locations = self._load_entity_list(location_list_path, \"locations\")\n",
        "\n",
        "        # C√°c t·ª´ kh√≥a ph·ªï bi·∫øn li√™n quan ƒë·∫øn m√≥n ƒÉn v√† h∆∞∆°ng v·ªã trong ti·∫øng Vi·ªát ƒë·ªÉ h·ªó tr·ª£ nh·∫≠n di·ªán\n",
        "        self.food_indicators = [\n",
        "            \"b√°nh m√¨\", \"ph·ªü\", \"b√∫n\", \"x√®o\", \"c∆°m\", \"g·ªèi\", \"ch·∫£\", \"x√¥i\", \"cao l·∫ßu\", \"ch√°o\",\n",
        "            \"m√¨ g√≥i\", \"h·ªß ti·∫øu\", \"nem\", \"ch·∫£ ram\", \"b√°nh kh·ªçt\",\n",
        "            \"l·∫©u\", \"c√°\", \"th·ªãt\", \"canh\", \"rau\", \"ƒë·∫≠u\", \"·ªëc\", \"s√∫p\", \"b·∫Øp\", \"l∆∞∆°n\", \"mƒÉng\", \"n·∫•m\",\n",
        "            \"chu·ªëi\", \"n·ªôm\", \"tr√†\", \"c√† ph√™\", \"sinh t·ªë\", \"kem\", \"t√†u h·ªß\", \"ch√®\", \"yaourt\", \"n∆∞·ªõc m√≠a\",\n",
        "            \"s·ªØa\", \"k·∫πo\", \"ƒëa\", \"nem chua\", \"g√†\", \"b√≤\", \"heo\", \"v·ªãt\", \"c√°\", \"t√¥m\", \"m·ª±c, ·ªëc\", \"s√≤\", \"h√†u\",\n",
        "            \"b√∫n ri√™u\", \"b√∫n b√≤\", \"b√∫n m·∫Øm\", \"b√∫n m·ªçc\", \"b√∫n ch·∫£\", \"b√∫n ƒë·∫≠u\", \"b√∫n ·ªëc\"\n",
        "        ]\n",
        "\n",
        "        self.taste_indicators = [\n",
        "            \"ngon\", \"ng·ªçt\", \"chua\", \"cay\", \"ƒë·∫Øng\", \"m·∫∑n\", \"b√πi\", \"b√©o\", \"gi√≤n\", \"m·ªÅm\",\n",
        "            \"th∆°m\", \"n·ªìng\", \"ƒë·∫≠m ƒë√†\", \"nh·∫°t\", \"thanh\", \"t∆∞∆°i\", \"ch√°t\", \"cay n·ªìng\", \"cay nh·∫π\", \"cay v·ª´a\",\n",
        "            \"s·∫ßn s·∫≠t\", \"m·ªçng n∆∞·ªõc\", \"ƒë·∫Øng ngh√©t\", \"ch√°t\", \"cay x√®\", \"t√™\", \"m·∫∑n ch√°t\", \"ng·ªçt l·ªãm\", \"b√©o ng·∫≠y\", \"th∆°m l·ª´ng\",\n",
        "            \"n·ªìng n√†n\", \"ƒë·∫≠m v·ªã\", \"nh·∫°t nh·∫Ωo\", \"thanh m√°t\", \"t∆∞∆°i\", \"ƒë·∫≠m ƒë√† h∆∞∆°ng v·ªã\", \"v·ª´a ƒÉn\", \"h·ª£p kh·∫©u v·ªã\"\n",
        "        ]\n",
        "\n",
        "        self.locations_indicators = [ \n",
        "            \"qu·∫≠n 1\", \"qu·∫≠n 2\", \"qu·∫≠n 3\", \"qu·∫≠n 4\", \"qu·∫≠n 5\", \"qu·∫≠n 6\", \"qu·∫≠n 7\", \"qu·∫≠n 8\", \"qu·∫≠n 9\", \"qu·∫≠n 10\",\n",
        "            \"qu·∫≠n 11\", \"qu·∫≠n 12\", \"b√¨nh th·∫°nh\", \"t√¢n b√¨nh\", \"t√¢n ph√∫\", \"ph√∫ nhu·∫≠n\", \"g√≤ v·∫•p\", \"b√¨nh t√¢n\", \"th·ªß ƒë·ª©c\", \"h√≥c m√¥n\",\n",
        "            \"c·ªß chi\", \"nh√† b√®\", \"c·∫ßn gi·ªù\", \"b√¨nh ch√°nh\", \"tp th·ªß ƒë·ª©c\",\n",
        "            \"h√† n·ªôi\", \"h·ªì ch√≠ minh\", \"ƒë√† n·∫µng\", \"h·∫£i ph√≤ng\", \"c·∫ßn th∆°\", \"hu·∫ø\", \"nha trang\", \"v≈©ng t√†u\", \"ƒë√† l·∫°t\",\n",
        "            \"h·∫° long\", \"m·ªπ tho\", \"long xuy√™n\", \"r·∫°ch gi√°\", \"c√† mau\", \"bi√™n h√≤a\", \"bu√¥n ma thu·ªôt\", \"th√°i nguy√™n\", \"nam ƒë·ªãnh\"\n",
        "        ]\n",
        "\n",
        "\n",
        "        # T·∫£i c√°c t√†i nguy√™n c·ªßa NLTK n·∫øu c·∫ßn\n",
        "        try:\n",
        "            nltk.data.find('tokenizers/punkt')\n",
        "        except LookupError:\n",
        "            nltk.download('punkt')\n",
        "\n",
        "        # T·∫°o th∆∞ m·ª•c ƒë·ªÉ l∆∞u tr·ªØ c√°c t·ªáp d·ªØ li·ªáu ƒë∆∞·ª£c tr√≠ch xu·∫•t\n",
        "        os.makedirs(\"extracted_data\", exist_ok=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Helper Methods"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def _load_entity_list(self, file_path, entity_type):\n",
        "    \"\"\"T·∫£i danh s√°ch th·ª±c th·ªÉ t·ª´ t·ªáp ho·∫∑c tr·∫£ v·ªÅ t·∫≠p r·ªóng m·∫∑c ƒë·ªãnh\"\"\"\n",
        "    if file_path and os.path.exists(file_path):\n",
        "        try:\n",
        "            with open(file_path, 'r', encoding='utf-8') as f:\n",
        "                return set(json.load(f))\n",
        "        except Exception as e:\n",
        "            logging.warning(f\"L·ªói khi t·∫£i danh s√°ch {entity_type}: {e}\")\n",
        "\n",
        "    logging.info(f\"Kh√¥ng t√¨m th·∫•y danh s√°ch {entity_type} hi·ªán c√≥, b·∫Øt ƒë·∫ßu v·ªõi t·∫≠p r·ªóng\")\n",
        "    return set()\n",
        "\n",
        "def save_entity_list(self, entity_list, entity_type):\n",
        "    \"\"\"L∆∞u danh s√°ch th·ª±c th·ªÉ ƒë√£ c·∫≠p nh·∫≠t v√†o t·ªáp\"\"\"\n",
        "    file_path = f\"extracted_data/{entity_type}_list.json\"\n",
        "    with open(file_path, 'w', encoding='utf-8') as f:\n",
        "        json.dump(list(entity_list), f, ensure_ascii=False, indent=2)\n",
        "    logging.info(f\"ƒê√£ l∆∞u {len(entity_list)} {entity_type} v√†o {file_path}\")\n",
        "\n",
        "def normalize_vietnamese_text(self, text):\n",
        "    \"\"\"Chu·∫©n h√≥a vƒÉn b·∫£n ti·∫øng Vi·ªát b·∫±ng c√°ch x·ª≠ l√Ω d·∫•u v√† ch·ªØ hoa/th∆∞·ªùng\"\"\"\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "\n",
        "    # Chu·∫©n h√≥a k√Ω t·ª± Unicode\n",
        "    text = unicodedata.normalize('NFC', text)\n",
        "\n",
        "    # Lo·∫°i b·ªè kho·∫£ng tr·∫Øng th·ª´a\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    return text\n",
        "\n",
        "def clean_text(self, text):\n",
        "    \"\"\"L√†m s·∫°ch vƒÉn b·∫£n b·∫±ng c√°ch lo·∫°i b·ªè k√Ω t·ª± ƒë·∫∑c bi·ªát v√† chu·∫©n h√≥a\"\"\"\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "\n",
        "    # Chu·∫©n h√≥a vƒÉn b·∫£n\n",
        "    text = self.normalize_vietnamese_text(text)\n",
        "    text = text.lower()\n",
        "\n",
        "    # Lo·∫°i b·ªè ƒë∆∞·ªùng d·∫´n URL\n",
        "    text = re.sub(r'https?://\\S+|www\\.\\S+|\\S+@\\S+\\.\\S+', '', text)\n",
        "    text = re.sub(r'@\\w+|#\\w+', '', text)\n",
        "\n",
        "    # Lo·∫°i b·ªè bi·ªÉu t∆∞·ª£ng c·∫£m x√∫c v√† k√Ω t·ª± ƒë·∫∑c bi·ªát trong khi gi·ªØ l·∫°i ch·ªØ ti·∫øng Vi·ªát\n",
        "    emoji_pattern = re.compile(\"[\"\n",
        "            u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "            u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "            u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "            u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "            u\"\\U00002702-\\U000027B0\"  # dingbats\n",
        "            u\"\\U000024C2-\\U0001F251\" \n",
        "            \"]+\", flags=re.UNICODE)\n",
        "    text = emoji_pattern.sub('', text)\n",
        "\n",
        "    symbols_to_remove = [\n",
        "            '!', '\"', '#', '$', '%', '&', \"'\", '*', '+', ',', \n",
        "            '-', '.', '/', ':', ';', '<', '=', '>', '?', '@', \n",
        "            '[', '\\\\', ']', '^', '_', '`', '{', '|', '}', '~',\n",
        "            '\"', '\"', ''', ''', '‚Ä¶', '‚Äì', '‚Äî', '‚Ä¢', '‚Ä≤', '‚Ä≥',\n",
        "            '‚Äû', '¬´', '¬ª', '‚Äπ', '‚Ä∫', '‚ü®', '‚ü©', '„Äà', '„Äâ'\n",
        "    ]\n",
        "    \n",
        "    # Create a pattern that excludes Vietnamese diacritics\n",
        "    pattern = f'[{\"\".join(map(re.escape, symbols_to_remove))}]'\n",
        "    text = re.sub(pattern, ' ', text)\n",
        "\n",
        "    # Handle ellipsis and multiple dots\n",
        "    text = re.sub(r'\\.{2,}', ' ', text)\n",
        "\n",
        "    # Handle multiple spaces and normalize whitespace\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "\n",
        "    # Handle parentheses and brackets\n",
        "    text = re.sub(r'[\\(\\)\\[\\]\\{\\}‚ü®‚ü©„Äà„Äâ]', ' ', text)\n",
        "\n",
        "    # Clean up extra spaces around Vietnamese words\n",
        "    text = re.sub(r'\\s+([^\\w\\s])|([^\\w\\s])\\s+', r'\\1\\2', text)\n",
        "\n",
        "    # Final whitespace cleanup\n",
        "    text = text.strip()\n",
        "\n",
        "    return text\n",
        "\n",
        "def auto_correct_text(self, text):\n",
        "    \"\"\"T·ª± ƒë·ªông s·ª≠a l·ªói ch√≠nh t·∫£ b·∫±ng b·ªô ki·ªÉm tra ch√≠nh t·∫£\"\"\"\n",
        "    spell = SpellChecker(language='vi')\n",
        "    words = word_tokenize(text)\n",
        "    corrected_words = [spell.correction(word) for word in words]\n",
        "    return \" \".join(corrected_words)\n",
        "\n",
        "def load_stopwords(self, file_path):\n",
        "    \"\"\"T·∫£i danh s√°ch t·ª´ d·ª´ng t·ª´ t·ªáp\"\"\"\n",
        "    if file_path and os.path.exists(file_path):\n",
        "        try:\n",
        "            with open(file_path, 'r', encoding='utf-8') as f:\n",
        "                return set(f.read().splitlines())\n",
        "        except Exception as e:\n",
        "            logging.warning(f\"L·ªói khi t·∫£i danh s√°ch t·ª´ d·ª´ng: {e}\")\n",
        "    logging.info(\"Kh√¥ng t√¨m th·∫•y t·ªáp t·ª´ d·ª´ng, b·∫Øt ƒë·∫ßu v·ªõi t·∫≠p r·ªóng\")\n",
        "    return set()\n",
        "\n",
        "def remove_stopwords(self, text, stopwords):\n",
        "    \"\"\"Lo·∫°i b·ªè t·ª´ d·ª´ng kh·ªèi vƒÉn b·∫£n\"\"\"\n",
        "    words = word_tokenize(text)\n",
        "    filtered_words = [word for word in words if word.lower() not in stopwords]\n",
        "    return \" \".join(filtered_words)\n",
        "\n",
        "def preprocess_text(self, text):\n",
        "    \"\"\"√Åp d·ª•ng t·∫•t c·∫£ c√°c b∆∞·ªõc ti·ªÅn x·ª≠ l√Ω l√™n vƒÉn b·∫£n\"\"\"\n",
        "    try:\n",
        "        text = self.clean_text(text)\n",
        "        text = self.auto_correct_text(text)  # ƒê√£ s·ª≠a l·ªói t·∫°i ƒë√¢y\n",
        "        stopwords = self.load_stopwords('vietnamese-stopwords.txt')\n",
        "        text = self.remove_stopwords(text, stopwords)\n",
        "        return text\n",
        "    except Exception as e:\n",
        "        logging.error(f\"L·ªói khi ti·ªÅn x·ª≠ l√Ω vƒÉn b·∫£n: {e}\")\n",
        "        return text if isinstance(text, str) else \"\"\n",
        "\n",
        "# G√°n c√°c ph∆∞∆°ng th·ª©c v√†o l·ªõp VietnameseTextProcessor\n",
        "VietnameseTextProcessor._load_entity_list = _load_entity_list\n",
        "VietnameseTextProcessor.save_entity_list = save_entity_list \n",
        "VietnameseTextProcessor.normalize_vietnamese_text = normalize_vietnamese_text\n",
        "VietnameseTextProcessor.clean_text = clean_text\n",
        "VietnameseTextProcessor.auto_correct_text = auto_correct_text\n",
        "VietnameseTextProcessor.load_stopwords = load_stopwords\n",
        "VietnameseTextProcessor.preprocess_text = preprocess_text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def test_text_processing():\n",
        "    \"\"\"Test function to demonstrate all text preprocessing and cleaning steps\"\"\"\n",
        "    \n",
        "    # Initialize the processor\n",
        "    processor = VietnameseTextProcessor()\n",
        "    \n",
        "    # Test text with various cases to check\n",
        "    test_text = \"\"\"\n",
        "    üî• Qu√°n Ph·ªü ngon ·ªü Qu·∫≠n 1 TPHCM! https://example.com\n",
        "    M√≥n ph·ªü b√≤ t√°i n·∫°m g·∫ßu c·ª±c k·ª≥ ngon, n∆∞·ªõc d√πng ƒë·∫≠m ƒë√†...\n",
        "    ƒê·ªãa ch·ªâ: 123 L√™ L·ª£i, P. B·∫øn Ngh√©, Qu·∫≠n 1, TP.HCM\n",
        "    #pho #amthuc #reviewdoan @foodblogger\n",
        "    \"\"\"\n",
        "    \n",
        "    print(\"Original Text:\")\n",
        "    print(\"-\" * 50)\n",
        "    print(test_text)\n",
        "    print(\"\\n\")\n",
        "\n",
        "    # Test normalize_vietnamese_text\n",
        "    print(\"1. After Vietnamese Text Normalization:\")\n",
        "    print(\"-\" * 50)\n",
        "    normalized = processor.normalize_vietnamese_text(test_text)\n",
        "    print(normalized)\n",
        "    print(\"\\n\")\n",
        "\n",
        "    # Test clean_text\n",
        "    print(\"2. After Text Cleaning:\")\n",
        "    print(\"-\" * 50)\n",
        "    cleaned = processor.clean_text(test_text)\n",
        "    print(cleaned)\n",
        "    print(\"\\n\")\n",
        "\n",
        "    # Test stopwords removal\n",
        "    print(\"3. After Stopwords Removal:\")\n",
        "    print(\"-\" * 50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-03-09 23:28:59,300 - INFO - Kh√¥ng t√¨m th·∫•y danh s√°ch foods hi·ªán c√≥, b·∫Øt ƒë·∫ßu v·ªõi t·∫≠p r·ªóng\n",
            "2025-03-09 23:28:59,300 - INFO - Kh√¥ng t√¨m th·∫•y danh s√°ch locations hi·ªán c√≥, b·∫Øt ƒë·∫ßu v·ªõi t·∫≠p r·ªóng\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original Text:\n",
            "--------------------------------------------------\n",
            "\n",
            "    üî• Qu√°n Ph·ªü ngon ·ªü Q.1 TPHCM! https://example.com\n",
            "    M√≥n ph·ªü b√≤ t√°i n·∫°m g·∫ßu c·ª±c k·ª≥ ngon, n∆∞·ªõc d√πng ƒë·∫≠m ƒë√†...\n",
            "    ƒê·ªãa ch·ªâ: 123 L√™ L·ª£i, P. B·∫øn Ngh√©, Qu·∫≠n 1, TP.HCM\n",
            "    #pho #amthuc #reviewdoan @foodblogger\n",
            "    \n",
            "\n",
            "\n",
            "1. After Vietnamese Text Normalization:\n",
            "--------------------------------------------------\n",
            "üî• Qu√°n Ph·ªü ngon ·ªü Q.1 TPHCM! https://example.com M√≥n ph·ªü b√≤ t√°i n·∫°m g·∫ßu c·ª±c k·ª≥ ngon, n∆∞·ªõc d√πng ƒë·∫≠m ƒë√†... ƒê·ªãa ch·ªâ: 123 L√™ L·ª£i, P. B·∫øn Ngh√©, Qu·∫≠n 1, TP.HCM #pho #amthuc #reviewdoan @foodblogger\n",
            "\n",
            "\n",
            "2. After Text Cleaning:\n",
            "--------------------------------------------------\n",
            "qu√°n ph·ªü ngon ·ªü q 1 tphcm m√≥n ph·ªü b√≤ t√°i n·∫°m g·∫ßu c·ª±c k·ª≥ ngon n∆∞·ªõc d√πng ƒë·∫≠m ƒë√† ƒë·ªãa ch·ªâ 123 l√™ l·ª£i p b·∫øn ngh√© qu·∫≠n 1 tp hcm\n",
            "\n",
            "\n",
            "3. After Stopwords Removal:\n",
            "--------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "test_text_processing()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Entity Extraction Methods"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_entities_from_ner(self, text):\n",
        "    \"\"\"Tr√≠ch xu·∫•t th·ª±c th·ªÉ t·ª´ vƒÉn b·∫£n b·∫±ng Named Entity Recognition (NER) c·ªßa underthesea.\"\"\"\n",
        "    locations = []\n",
        "\n",
        "    try:\n",
        "        ner_tags = ner(text)  # Th·ª±c hi·ªán nh·∫≠n d·∫°ng th·ª±c th·ªÉ c√≥ t√™n (NER)\n",
        "\n",
        "        # Ki·ªÉm tra n·∫øu k·∫øt qu·∫£ t·ª´ NER c√≥ ƒë·ªãnh d·∫°ng mong ƒë·ª£i\n",
        "        if not isinstance(ner_tags, list):\n",
        "            return locations\n",
        "\n",
        "        # Tr√≠ch xu·∫•t c√°c ƒë·ªãa ƒëi·ªÉm t·ª´ NER\n",
        "        current_loc = []\n",
        "\n",
        "        for item in ner_tags:\n",
        "            # X·ª≠ l√Ω c√°c ƒë·ªãnh d·∫°ng ƒë·∫ßu ra kh√°c nhau t·ª´ NER\n",
        "            if isinstance(item, (list, tuple)) and len(item) == 2:\n",
        "                word, tag = item\n",
        "            else:\n",
        "                continue\n",
        "\n",
        "            if tag.startswith('B-LOC'):\n",
        "                if current_loc:\n",
        "                    locations.append(' '.join(current_loc))\n",
        "                    current_loc = []\n",
        "                current_loc.append(word)\n",
        "            elif tag.startswith('I-LOC') and current_loc:\n",
        "                current_loc.append(word)\n",
        "            elif current_loc:\n",
        "                locations.append(' '.join(current_loc))\n",
        "                current_loc = []\n",
        "\n",
        "        # Th√™m th·ª±c th·ªÉ ƒë·ªãa ƒëi·ªÉm cu·ªëi c√πng n·∫øu c√≥\n",
        "        if current_loc:\n",
        "            locations.append(' '.join(current_loc))\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"L·ªói khi tr√≠ch xu·∫•t th·ª±c th·ªÉ b·∫±ng NER: {e}\")\n",
        "        logging.error(traceback.format_exc())\n",
        "\n",
        "    return locations\n",
        "\n",
        "def extract_entities_from_patterns(self, text, sentences, pos_tags):\n",
        "    \"\"\"Tr√≠ch xu·∫•t th·ª±c th·ªÉ b·∫±ng c√°ch s·ª≠ d·ª•ng ph∆∞∆°ng ph√°p d·ª±a tr√™n m·∫´u (Pattern Matching).\"\"\"\n",
        "    foods = []\n",
        "    locations = []\n",
        "    tastes = []\n",
        "\n",
        "    # X·ª≠ l√Ω t·ª´ng c√¢u ƒë·ªÉ tr√≠ch xu·∫•t th·ª±c th·ªÉ\n",
        "    for idx, sentence in enumerate(sentences):\n",
        "        words = word_tokenize(sentence)\n",
        "        sentence_pos_tags = pos_tags[idx] if idx < len(pos_tags) else []\n",
        "\n",
        "        # T√¨m th·ª±c th·ªÉ v·ªÅ th·ª±c ph·∫©m\n",
        "        self._extract_food_entities(sentence, sentence_pos_tags, foods)\n",
        "\n",
        "        # T√¨m th·ª±c th·ªÉ v·ªÅ ƒë·ªãa ƒëi·ªÉm\n",
        "        self._extract_location_entities(sentence, sentence_pos_tags, locations)\n",
        "\n",
        "        # T√¨m m√¥ t·∫£ v·ªÅ h∆∞∆°ng v·ªã\n",
        "        self._extract_taste_descriptions(sentence, words, tastes)\n",
        "\n",
        "    return foods, locations, tastes\n",
        "\n",
        "def _extract_food_entities(self, sentence, pos_tags, foods):\n",
        "    \"\"\"Tr√≠ch xu·∫•t th·ª±c th·ªÉ th·ª±c ph·∫©m t·ª´ m·ªôt c√¢u.\"\"\"\n",
        "    # Ki·ªÉm tra danh s√°ch th·ª±c ph·∫©m c√≥ s·∫µn\n",
        "    for food in self.foods:\n",
        "        if food.lower() in sentence.lower():\n",
        "            foods.append(food)\n",
        "\n",
        "    # T√¨m c√°c t·ª´ ch·ªâ th·ª±c ph·∫©m\n",
        "    for idx, (word, tag) in enumerate(pos_tags):\n",
        "        if word.lower() in self.food_indicators:\n",
        "            noun_phrase = [word]\n",
        "            for i in range(1, 4):\n",
        "                if idx + i < len(pos_tags):\n",
        "                    next_word, next_tag = pos_tags[idx + i]\n",
        "                    if next_tag.startswith(('N', 'A')):  # Danh t·ª´ ho·∫∑c T√≠nh t·ª´\n",
        "                        noun_phrase.append(next_word)\n",
        "                    else:\n",
        "                        break\n",
        "            \n",
        "            if noun_phrase:\n",
        "                food_name = \" \".join(noun_phrase)\n",
        "                foods.append(food_name)\n",
        "                self.foods.add(food_name)\n",
        "\n",
        "def _extract_location_entities(self, sentence, pos_tags, locations):\n",
        "    \"\"\"Tr√≠ch xu·∫•t th·ª±c th·ªÉ ƒë·ªãa ƒëi·ªÉm t·ª´ m·ªôt c√¢u.\"\"\"\n",
        "    # Ki·ªÉm tra danh s√°ch ƒë·ªãa ƒëi·ªÉm c√≥ s·∫µn\n",
        "    for location in self.locations:\n",
        "        if location.lower() in sentence.lower():\n",
        "            locations.append(location)\n",
        "\n",
        "    # T√¨m c√°c t·ª´ ch·ªâ ƒë·ªãa ƒëi·ªÉm\n",
        "    for idx, (word, tag) in enumerate(pos_tags):\n",
        "        if any(indicator.lower() in word.lower() for indicator in self.locations_indicators):\n",
        "            noun_phrase = [word]\n",
        "            for i in range(1, 4):\n",
        "                if idx + i < len(pos_tags):\n",
        "                    next_word, next_tag = pos_tags[idx + i]\n",
        "                    if next_tag.startswith(('N', 'M', 'Np')):  # Danh t·ª´, S·ªë, Danh t·ª´ ri√™ng\n",
        "                        noun_phrase.append(next_word)\n",
        "                    else:\n",
        "                        break\n",
        "            \n",
        "            if noun_phrase:\n",
        "                location_name = \" \".join(noun_phrase)\n",
        "                locations.append(location_name)\n",
        "                self.locations.add(location_name)\n",
        "\n",
        "def _extract_taste_descriptions(self, sentence, words, tastes):\n",
        "    \"\"\"Tr√≠ch xu·∫•t m√¥ t·∫£ v·ªÅ h∆∞∆°ng v·ªã t·ª´ m·ªôt c√¢u.\"\"\"\n",
        "    for taste_word in self.taste_indicators:\n",
        "        if taste_word in sentence.lower():\n",
        "            taste_idx = -1\n",
        "            for idx, word in enumerate(words):\n",
        "                if taste_word in word.lower():\n",
        "                    taste_idx = idx\n",
        "                    break\n",
        "            \n",
        "            if taste_idx >= 0:\n",
        "                start = max(0, taste_idx - 1)  # L·∫•y t·ªëi ƒëa 1 t·ª´ tr∆∞·ªõc\n",
        "                end = min(len(words), taste_idx + 2)  # L·∫•y t·ªëi ƒëa 2 t·ª´ sau\n",
        "                taste_phrase = \" \".join(words[start:end])\n",
        "\n",
        "                # ƒê·∫£m b·∫£o c·ª•m t·ª´ c√≥ √≠t nh·∫•t 2 t·ª´ v√† t·ªëi ƒëa 3 t·ª´\n",
        "                if 2 <= len(taste_phrase.split()) <= 3:\n",
        "                    tastes.append(taste_phrase)\n",
        "\n",
        "def extract_entities(self, text):\n",
        "    \"\"\"Tr√≠ch xu·∫•t c√°c th·ª±c th·ªÉ v·ªÅ th·ª±c ph·∫©m, ƒë·ªãa ƒëi·ªÉm v√† h∆∞∆°ng v·ªã t·ª´ vƒÉn b·∫£n.\"\"\"\n",
        "    if not text or not isinstance(text, str):\n",
        "        return {\"foods\": [], \"locations\": [], \"tastes\": []}\n",
        "\n",
        "    try:\n",
        "        results = {\"foods\": [], \"locations\": [], \"tastes\": []}\n",
        "\n",
        "        # Tr√≠ch xu·∫•t ƒë·ªãa ƒëi·ªÉm b·∫±ng NER\n",
        "        ner_locations = self.extract_entities_from_ner(text)\n",
        "        results[\"locations\"].extend(ner_locations)\n",
        "        self.locations.update(ner_locations)\n",
        "\n",
        "        # Tr√≠ch xu·∫•t th·ª±c th·ªÉ b·∫±ng ph∆∞∆°ng ph√°p d·ª±a tr√™n m·∫´u\n",
        "        sentences = nltk.sent_tokenize(text)\n",
        "        pos_tags = [pos_tag(sent) for sent in sentences]\n",
        "\n",
        "        foods, locations, tastes = self.extract_entities_from_patterns(text, sentences, pos_tags)\n",
        "\n",
        "        results[\"foods\"].extend(foods)\n",
        "        results[\"locations\"].extend(locations)\n",
        "        results[\"tastes\"].extend(tastes)\n",
        "\n",
        "        # C·∫≠p nh·∫≠t danh s√°ch th·ª±c th·ªÉ\n",
        "        self.foods.update(foods)\n",
        "        self.locations.update(locations)\n",
        "\n",
        "        # Lo·∫°i b·ªè tr√πng l·∫∑p v√† l·ªçc b·ªè chu·ªói r·ªóng\n",
        "        for key in results:\n",
        "            results[key] = list(set(filter(None, results[key])))\n",
        "\n",
        "        return results\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"L·ªói khi tr√≠ch xu·∫•t th·ª±c th·ªÉ: {e}\")\n",
        "        logging.error(traceback.format_exc())\n",
        "        return {\"foods\": [], \"locations\": [], \"tastes\": []}\n",
        "    \n",
        "VietnameseTextProcessor.extract_entities_from_ner = extract_entities_from_ner\n",
        "VietnameseTextProcessor.extract_entities_from_patterns = extract_entities_from_patterns\n",
        "VietnameseTextProcessor._extract_food_entities = _extract_food_entities\n",
        "VietnameseTextProcessor._extract_location_entities = _extract_location_entities\n",
        "VietnameseTextProcessor._extract_taste_descriptions = _extract_taste_descriptions\n",
        "VietnameseTextProcessor.extract_entities = extract_entities\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# DataFrame Processing and Bootstrapping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def process_dataframe(self, df, text_column=\"video_transcription\", batch_size=100):\n",
        "    \"\"\"\n",
        "    X·ª≠ l√Ω to√†n b·ªô DataFrame v√† tr√≠ch xu·∫•t c√°c th·ª±c th·ªÉ.\n",
        "\n",
        "    Tham s·ªë:\n",
        "        df (pd.DataFrame): DataFrame ch·ª©a d·ªØ li·ªáu vƒÉn b·∫£n.\n",
        "        text_column (str): T√™n c·ªôt ch·ª©a vƒÉn b·∫£n.\n",
        "        batch_size (int): K√≠ch th∆∞·ªõc batch ƒë·ªÉ x·ª≠ l√Ω nh·∫±m ti·∫øt ki·ªám b·ªô nh·ªõ.\n",
        "\n",
        "    Tr·∫£ v·ªÅ:\n",
        "        pd.DataFrame: DataFrame g·ªëc v·ªõi c√°c c·ªôt ch·ª©a th·ª±c th·ªÉ ƒë∆∞·ª£c tr√≠ch xu·∫•t.\n",
        "    \"\"\"\n",
        "    # Ki·ªÉm tra n·∫øu DataFrame tr·ªëng ho·∫∑c kh√¥ng c√≥ c·ªôt vƒÉn b·∫£n\n",
        "    if df.empty or text_column not in df.columns:\n",
        "        logging.error(f\"DataFrame kh√¥ng h·ª£p l·ªá ho·∫∑c thi·∫øu c·ªôt '{text_column}'\")\n",
        "        return df\n",
        "\n",
        "    # T·∫°o th∆∞ m·ª•c l∆∞u tr·ªØ n·∫øu ch∆∞a t·ªìn t·∫°i\n",
        "    os.makedirs(\"extracted_data\", exist_ok=True)\n",
        "\n",
        "    # Kh·ªüi t·∫°o c√°c c·ªôt ƒë·ªÉ l∆∞u th·ª±c th·ªÉ tr√≠ch xu·∫•t\n",
        "    df['preprocessed_text'] = \"\"\n",
        "    df['extracted_foods'] = None\n",
        "    df['extracted_locations'] = None\n",
        "    df['extracted_tastes'] = None\n",
        "\n",
        "    total_batches = (len(df) + batch_size - 1) // batch_size  # T√≠nh s·ªë batch c·∫ßn x·ª≠ l√Ω\n",
        "\n",
        "    for i in tqdm(range(total_batches), desc=\"ƒêang x·ª≠ l√Ω batch\"):\n",
        "        start_idx = i * batch_size\n",
        "        end_idx = min((i + 1) * batch_size, len(df))\n",
        "\n",
        "        batch = df.iloc[start_idx:end_idx].copy()\n",
        "\n",
        "        # Ti·ªÅn x·ª≠ l√Ω vƒÉn b·∫£n\n",
        "        batch['preprocessed_text'] = batch[text_column].apply(self.preprocess_text)\n",
        "\n",
        "        # Tr√≠ch xu·∫•t th·ª±c th·ªÉ\n",
        "        entities_list = []\n",
        "        for text in batch['preprocessed_text']:\n",
        "            entities_list.append(self.extract_entities(text))\n",
        "\n",
        "        # C·∫≠p nh·∫≠t DataFrame v·ªõi th·ª±c th·ªÉ tr√≠ch xu·∫•t\n",
        "        batch['extracted_foods'] = [data['foods'] for data in entities_list]\n",
        "        batch['extracted_locations'] = [data['locations'] for data in entities_list]\n",
        "        batch['extracted_tastes'] = [data['tastes'] for data in entities_list]\n",
        "\n",
        "        # C·∫≠p nh·∫≠t v√†o DataFrame g·ªëc\n",
        "        df.iloc[start_idx:end_idx] = batch\n",
        "\n",
        "        # L∆∞u k·∫øt qu·∫£ t·∫°m th·ªùi theo t·ª´ng batch\n",
        "        if (i + 1) % 5 == 0 or (i + 1) == total_batches:\n",
        "            self.save_entity_list(self.foods, \"foods\")\n",
        "            self.save_entity_list(self.locations, \"locations\")\n",
        "\n",
        "            # L∆∞u k·∫øt qu·∫£ trung gian\n",
        "            checkpoint_file = f\"extracted_data/processed_data_batch_{i+1}.csv\"\n",
        "            df.iloc[:end_idx].to_csv(checkpoint_file, index=False)\n",
        "            logging.info(f\"ƒê√£ l∆∞u k·∫øt qu·∫£ trung gian v√†o {checkpoint_file} sau batch {i+1}/{total_batches}\")\n",
        "\n",
        "    # Th·ªëng k√™ s·ªë l∆∞·ª£ng th·ª±c th·ªÉ ƒë√£ t√¨m th·∫•y\n",
        "    food_count = len(self.foods)\n",
        "    location_count = len(self.locations)\n",
        "\n",
        "    logging.info(f\"Tr√≠ch xu·∫•t ho√†n t·∫•t. T√¨m th·∫•y {food_count} th·ª±c th·ªÉ m√≥n ƒÉn v√† {location_count} th·ª±c th·ªÉ ƒë·ªãa ƒëi·ªÉm.\")\n",
        "\n",
        "    return df\n",
        "\n",
        "def bootstrap_entity_lists(self, df, text_column=\"preprocessed_text\", min_freq=3):\n",
        "    \"\"\"\n",
        "    M·ªü r·ªông danh s√°ch th·ª±c th·ªÉ b·∫±ng TF-IDF ƒë·ªÉ t√¨m c√°c th·ª±c th·ªÉ ti·ªÅm nƒÉng.\n",
        "    \n",
        "    Tham s·ªë:\n",
        "        df (pd.DataFrame): DataFrame ch·ª©a d·ªØ li·ªáu vƒÉn b·∫£n.\n",
        "        text_column (str): T√™n c·ªôt ch·ª©a vƒÉn b·∫£n ƒë√£ ti·ªÅn x·ª≠ l√Ω.\n",
        "        min_freq (int): S·ªë l·∫ßn xu·∫•t hi·ªán t·ªëi thi·ªÉu ƒë·ªÉ xem x√©t m·ªôt th·ª±c th·ªÉ.\n",
        "\n",
        "    Tr·∫£ v·ªÅ:\n",
        "        set: T·∫≠p h·ª£p c√°c th·ª±c th·ªÉ m√≥n ƒÉn m·ªõi ƒë∆∞·ª£c nh·∫≠n di·ªán.\n",
        "    \"\"\"\n",
        "    if df.empty or text_column not in df.columns:\n",
        "        logging.error(f\"Kh√¥ng th·ªÉ m·ªü r·ªông th·ª±c th·ªÉ: DataFrame kh√¥ng h·ª£p l·ªá ho·∫∑c thi·∫øu c·ªôt '{text_column}'\")\n",
        "        return set()\n",
        "\n",
        "    # L·ªçc ra c√°c vƒÉn b·∫£n h·ª£p l·ªá\n",
        "    valid_texts = df[text_column].dropna().replace('', pd.NA).dropna().tolist()\n",
        "\n",
        "    if not valid_texts:\n",
        "        logging.warning(\"Kh√¥ng t√¨m th·∫•y vƒÉn b·∫£n h·ª£p l·ªá ƒë·ªÉ m·ªü r·ªông th·ª±c th·ªÉ\")\n",
        "        return set()\n",
        "\n",
        "    try:\n",
        "        min_df_val = max(1, min(min_freq, len(valid_texts) // 2))\n",
        "        \n",
        "        tfidf = TfidfVectorizer(\n",
        "            ngram_range=(1, 3),  # X√©t c√°c n-gram t·ª´ 1 ƒë·∫øn 3 t·ª´\n",
        "            min_df=min_df_val,  # ƒêi·ªÅu ch·ªânh min_df\n",
        "            max_df=0.7  # Lo·∫°i b·ªè c√°c c·ª•m t·ª´ qu√° ph·ªï bi·∫øn\n",
        "        )\n",
        "\n",
        "        tfidf_matrix = tfidf.fit_transform(valid_texts)\n",
        "        feature_names = tfidf.get_feature_names_out()\n",
        "\n",
        "        # L·∫•y danh s√°ch n-gram c√≥ gi√° tr·ªã TF-IDF cao\n",
        "        important_ngrams = []\n",
        "        for i in range(min(tfidf_matrix.shape[0], 100)):\n",
        "            feature_index = tfidf_matrix[i,:].nonzero()[1]\n",
        "            tfidf_scores = zip(feature_index, [tfidf_matrix[i, x] for x in feature_index])\n",
        "            # S·∫Øp x·∫øp theo ƒëi·ªÉm TF-IDF gi·∫£m d·∫ßn\n",
        "            for idx, score in sorted(tfidf_scores, key=lambda x: x[1], reverse=True)[:20]:\n",
        "                important_ngrams.append(feature_names[idx])\n",
        "\n",
        "        # L·ªçc c√°c c·ª•m t·ª´ c√≥ th·ªÉ l√† t√™n m√≥n ƒÉn (d·ª±a v√†o t·ª´ ch·ªâ m√≥n ƒÉn)\n",
        "        potential_foods = set()\n",
        "        for text in valid_texts:\n",
        "            for indicator in self.food_indicators:\n",
        "                if indicator in text:\n",
        "                    for ngram in important_ngrams:\n",
        "                        # Ki·ªÉm tra n·∫øu ngram xu·∫•t hi·ªán g·∫ßn t·ª´ ch·ªâ m√≥n ƒÉn\n",
        "                        if ngram in text and re.search(r'\\b' + re.escape(indicator) + r'.{0,30}' + re.escape(ngram), text, re.IGNORECASE):\n",
        "                            potential_foods.add(ngram)\n",
        "                        if ngram in text and re.search(r'\\b' + re.escape(ngram) + r'.{0,30}' + re.escape(indicator), text, re.IGNORECASE):\n",
        "                            potential_foods.add(ngram)\n",
        "\n",
        "        # L·ªçc b·ªè c√°c th·ª±c th·ªÉ kh√¥ng h·ª£p l·ªá (qu√° ng·∫Øn, ch·ªâ ch·ª©a s·ªë, v.v.)\n",
        "        filtered_foods = {food for food in potential_foods if len(food) > 2 and not food.isdigit()}\n",
        "\n",
        "        # C·∫≠p nh·∫≠t danh s√°ch m√≥n ƒÉn\n",
        "        self.foods.update(filtered_foods)\n",
        "        logging.info(f\"ƒê√£ th√™m {len(filtered_foods)} th·ª±c th·ªÉ m√≥n ƒÉn ti·ªÅm nƒÉng t·ª´ m·ªü r·ªông th·ª±c th·ªÉ\")\n",
        "\n",
        "        return filtered_foods\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"L·ªói khi m·ªü r·ªông th·ª±c th·ªÉ: {e}\")\n",
        "        logging.error(traceback.format_exc())\n",
        "        return set()\n",
        "\n",
        "VietnameseTextProcessor.process_dataframe = process_dataframe\n",
        "VietnameseTextProcessor.bootstrap_entity_lists = bootstrap_entity_lists"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YI6pKb0WGWVC",
        "outputId": "174f413c-1aac-4c97-9d7b-a886256d1cf2"
      },
      "outputs": [],
      "source": [
        "def main():\n",
        "    try:\n",
        "        # T·∫°o m·ªôt th·ªÉ hi·ªán c·ªßa b·ªô x·ª≠ l√Ω vƒÉn b·∫£n\n",
        "        processor = VietnameseTextProcessor()\n",
        "\n",
        "        # T·∫£i t·∫≠p d·ªØ li·ªáu\n",
        "        logging.info(\"ƒêang t·∫£i t·∫≠p d·ªØ li·ªáu...\")\n",
        "        try:\n",
        "            # df = pd.read_csv(\"/content/21KHDL-TikTok-Analytics/data/interim/small_video_transcription.csv\")\n",
        "            df = pd.read_csv(\"C:/Users/nguye/OneDrive/TaÃÄi li√™Ã£u/GitHub/21KHDL-TikTok-Analytics/data/interim/small_video_transcription.csv\")\n",
        "            if df.empty:\n",
        "                logging.error(\"T·∫≠p d·ªØ li·ªáu ƒë∆∞·ª£c t·∫£i v·ªÅ tr·ªëng\")\n",
        "                return\n",
        "            logging.info(f\"T·∫≠p d·ªØ li·ªáu ƒë√£ t·∫£i c√≥ {len(df)} d√≤ng\")\n",
        "        except Exception as e:\n",
        "            logging.error(f\"L·ªói khi t·∫£i t·∫≠p d·ªØ li·ªáu: {e}\")\n",
        "            logging.error(traceback.format_exc())\n",
        "            return\n",
        "\n",
        "        # X·ª≠ l√Ω m·ªôt m·∫´u nh·ªè ƒë·ªÉ ki·ªÉm th·ª≠ (s·ª≠ d·ª•ng .head(10) ƒë·ªÉ th·ª≠ nghi·ªám, x√≥a b·ªè ƒë·ªÉ x·ª≠ l√Ω to√†n b·ªô)\n",
        "        sample_df = df.head(20)\n",
        "\n",
        "        # X·ª≠ l√Ω d·ªØ li·ªáu vƒÉn b·∫£n\n",
        "        logging.info(\"B·∫Øt ƒë·∫ßu x·ª≠ l√Ω vƒÉn b·∫£n v√† tr√≠ch xu·∫•t th·ª±c th·ªÉ...\")\n",
        "        processed_df = processor.process_dataframe(sample_df, text_column='video_transcription')\n",
        "\n",
        "        # M·ªü r·ªông danh s√°ch th·ª±c th·ªÉ b·∫±ng ph∆∞∆°ng ph√°p bootstrapping\n",
        "        logging.info(\"Th·ª±c hi·ªán bootstrapping ƒë·ªÉ m·ªü r·ªông danh s√°ch th·ª±c th·ªÉ...\")\n",
        "        processor.bootstrap_entity_lists(processed_df)\n",
        "\n",
        "        # L∆∞u k·∫øt qu·∫£ cu·ªëi c√πng\n",
        "        processed_df.to_csv(\"extracted_data/fully_processed_data.csv\", index=False)\n",
        "        processor.save_entity_list(processor.foods, \"foods\")\n",
        "        processor.save_entity_list(processor.locations, \"locations\")\n",
        "\n",
        "        # L∆∞u k·∫øt qu·∫£ c√≥ c·∫•u tr√∫c d∆∞·ªõi d·∫°ng JSON g·ªìm video_id, author_id v√† c√°c th·ª±c th·ªÉ tr√≠ch xu·∫•t\n",
        "        structured_data = []\n",
        "        for _, row in processed_df.iterrows():\n",
        "            structured_data.append({\n",
        "                'video_id': row.get('video_id', ''),\n",
        "                'author_id': row.get('author_id', ''),\n",
        "                'extracted_entities': {\n",
        "                    'foods': row.get('extracted_foods', []),\n",
        "                    'locations': row.get('extracted_locations', []),\n",
        "                    'tastes': row.get('extracted_tastes', [])\n",
        "                }\n",
        "            })\n",
        "\n",
        "        with open(\"extracted_data/structured_entities.json\", 'w', encoding='utf-8') as f:\n",
        "            json.dump(structured_data, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "        logging.info(\"Qu√° tr√¨nh x·ª≠ l√Ω ho√†n t·∫•t. K·∫øt qu·∫£ ƒë√£ ƒë∆∞·ª£c l∆∞u trong th∆∞ m·ª•c 'extracted_data'.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"L·ªói nghi√™m tr·ªçng trong h√†m main: {e}\")\n",
        "        logging.error(traceback.format_exc())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-03-09 23:28:59,374 - INFO - Kh√¥ng t√¨m th·∫•y danh s√°ch foods hi·ªán c√≥, b·∫Øt ƒë·∫ßu v·ªõi t·∫≠p r·ªóng\n",
            "2025-03-09 23:28:59,375 - INFO - Kh√¥ng t√¨m th·∫•y danh s√°ch locations hi·ªán c√≥, b·∫Øt ƒë·∫ßu v·ªõi t·∫≠p r·ªóng\n",
            "2025-03-09 23:28:59,377 - INFO - ƒêang t·∫£i t·∫≠p d·ªØ li·ªáu...\n",
            "2025-03-09 23:28:59,651 - INFO - T·∫≠p d·ªØ li·ªáu ƒë√£ t·∫£i c√≥ 10673 d√≤ng\n",
            "2025-03-09 23:28:59,652 - INFO - B·∫Øt ƒë·∫ßu x·ª≠ l√Ω vƒÉn b·∫£n v√† tr√≠ch xu·∫•t th·ª±c th·ªÉ...\n",
            "C:\\Users\\nguye\\AppData\\Local\\Temp\\ipykernel_13796\\74775411.py:22: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df['preprocessed_text'] = \"\"\n",
            "C:\\Users\\nguye\\AppData\\Local\\Temp\\ipykernel_13796\\74775411.py:23: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df['extracted_foods'] = None\n",
            "C:\\Users\\nguye\\AppData\\Local\\Temp\\ipykernel_13796\\74775411.py:24: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df['extracted_locations'] = None\n",
            "C:\\Users\\nguye\\AppData\\Local\\Temp\\ipykernel_13796\\74775411.py:25: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df['extracted_tastes'] = None\n",
            "ƒêang x·ª≠ l√Ω batch:   0%|          | 0/1 [00:00<?, ?it/s]2025-03-09 23:28:59,657 - ERROR - L·ªói khi ti·ªÅn x·ª≠ l√Ω vƒÉn b·∫£n: The provided dictionary language (vi) does not exist!\n",
            "2025-03-09 23:28:59,659 - ERROR - L·ªói khi ti·ªÅn x·ª≠ l√Ω vƒÉn b·∫£n: The provided dictionary language (vi) does not exist!\n",
            "2025-03-09 23:28:59,662 - ERROR - L·ªói khi ti·ªÅn x·ª≠ l√Ω vƒÉn b·∫£n: The provided dictionary language (vi) does not exist!\n",
            "2025-03-09 23:28:59,664 - ERROR - L·ªói khi ti·ªÅn x·ª≠ l√Ω vƒÉn b·∫£n: The provided dictionary language (vi) does not exist!\n",
            "2025-03-09 23:28:59,667 - ERROR - L·ªói khi ti·ªÅn x·ª≠ l√Ω vƒÉn b·∫£n: The provided dictionary language (vi) does not exist!\n",
            "2025-03-09 23:28:59,669 - ERROR - L·ªói khi ti·ªÅn x·ª≠ l√Ω vƒÉn b·∫£n: The provided dictionary language (vi) does not exist!\n",
            "2025-03-09 23:28:59,670 - ERROR - L·ªói khi ti·ªÅn x·ª≠ l√Ω vƒÉn b·∫£n: The provided dictionary language (vi) does not exist!\n",
            "2025-03-09 23:28:59,672 - ERROR - L·ªói khi ti·ªÅn x·ª≠ l√Ω vƒÉn b·∫£n: The provided dictionary language (vi) does not exist!\n",
            "2025-03-09 23:28:59,674 - ERROR - L·ªói khi ti·ªÅn x·ª≠ l√Ω vƒÉn b·∫£n: The provided dictionary language (vi) does not exist!\n",
            "2025-03-09 23:28:59,676 - ERROR - L·ªói khi ti·ªÅn x·ª≠ l√Ω vƒÉn b·∫£n: The provided dictionary language (vi) does not exist!\n",
            "2025-03-09 23:29:12,573 - INFO - ƒê√£ l∆∞u 74 foods v√†o extracted_data/foods_list.json\n",
            "2025-03-09 23:29:12,574 - INFO - ƒê√£ l∆∞u 7 locations v√†o extracted_data/locations_list.json\n",
            "2025-03-09 23:29:12,577 - INFO - ƒê√£ l∆∞u k·∫øt qu·∫£ trung gian v√†o extracted_data/processed_data_batch_1.csv sau batch 1/1\n",
            "ƒêang x·ª≠ l√Ω batch: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:12<00:00, 12.92s/it]\n",
            "2025-03-09 23:29:12,578 - INFO - Tr√≠ch xu·∫•t ho√†n t·∫•t. T√¨m th·∫•y 74 th·ª±c th·ªÉ m√≥n ƒÉn v√† 7 th·ª±c th·ªÉ ƒë·ªãa ƒëi·ªÉm.\n",
            "2025-03-09 23:29:12,579 - INFO - Th·ª±c hi·ªán bootstrapping ƒë·ªÉ m·ªü r·ªông danh s√°ch th·ª±c th·ªÉ...\n",
            "2025-03-09 23:29:14,933 - INFO - ƒê√£ th√™m 101 th·ª±c th·ªÉ m√≥n ƒÉn ti·ªÅm nƒÉng t·ª´ m·ªü r·ªông th·ª±c th·ªÉ\n",
            "2025-03-09 23:29:14,937 - INFO - ƒê√£ l∆∞u 170 foods v√†o extracted_data/foods_list.json\n",
            "2025-03-09 23:29:14,938 - INFO - ƒê√£ l∆∞u 7 locations v√†o extracted_data/locations_list.json\n",
            "2025-03-09 23:29:14,940 - INFO - Qu√° tr√¨nh x·ª≠ l√Ω ho√†n t·∫•t. K·∫øt qu·∫£ ƒë√£ ƒë∆∞·ª£c l∆∞u trong th∆∞ m·ª•c 'extracted_data'.\n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_food_prompts():\n",
        "    food_prompt_template = \"\"\"\n",
        "    H√£y ph√¢n t√≠ch th√¥ng tin v·ªÅ m√≥n ƒÉn sau ƒë√¢y:\n",
        "    \n",
        "    Danh s√°ch m√≥n ƒÉn: {foods}\n",
        "    \n",
        "    Y√™u c·∫ßu:\n",
        "    1. Ph√¢n lo·∫°i c√°c m√≥n ƒÉn th√†nh c√°c nh√≥m (v√≠ d·ª•: m√≥n n∆∞·ªõc, m√≥n n∆∞·ªõng, ƒë·ªì u·ªëng, etc.)\n",
        "    2. X√°c ƒë·ªãnh c√°c m√≥n ƒë·∫∑c tr∆∞ng nh·∫•t\n",
        "    3. ƒê·ªÅ xu·∫•t m√≥n ƒÉn ph·ªï bi·∫øn nh·∫•t d·ª±a tr√™n t·∫ßn su·∫•t xu·∫•t hi·ªán\n",
        "    4. Li√™n k·∫øt m√≥n ƒÉn v·ªõi vƒÉn h√≥a ·∫©m th·ª±c Vi·ªát Nam\n",
        "    5. ƒê·ªÅ xu·∫•t c√°c k·∫øt h·ª£p m√≥n ƒÉn ph√π h·ª£p\n",
        "    \n",
        "    Vui l√≤ng tr√¨nh b√†y k·∫øt qu·∫£ m·ªôt c√°ch chi ti·∫øt v√† c√≥ c·∫•u tr√∫c.\n",
        "    \"\"\"\n",
        "    return food_prompt_template\n",
        "\n",
        "def create_location_prompts():\n",
        "    location_prompt_template = \"\"\"\n",
        "    H√£y ph√¢n t√≠ch th√¥ng tin v·ªÅ ƒë·ªãa ƒëi·ªÉm ·∫©m th·ª±c sau ƒë√¢y:\n",
        "    \n",
        "    Danh s√°ch ƒë·ªãa ƒëi·ªÉm: {locations}\n",
        "    \n",
        "    Y√™u c·∫ßu:\n",
        "    1. Nh√≥m c√°c ƒë·ªãa ƒëi·ªÉm theo khu v·ª±c (qu·∫≠n/huy·ªán)\n",
        "    2. X√°c ƒë·ªãnh c√°c khu v·ª±c ·∫©m th·ª±c n·ªïi ti·∫øng\n",
        "    3. ƒê·ªÅ xu·∫•t tuy·∫øn ƒë∆∞·ªùng kh√°m ph√° ·∫©m th·ª±c\n",
        "    4. Li√™n k·∫øt ƒë·ªãa ƒëi·ªÉm v·ªõi ƒë·∫∑c tr∆∞ng ·∫©m th·ª±c\n",
        "    5. X√°c ƒë·ªãnh c√°c ƒëi·ªÉm ·∫©m th·ª±c c√≥ m·∫≠t ƒë·ªô cao\n",
        "    \n",
        "    Vui l√≤ng ph√¢n t√≠ch v√† ƒë∆∞a ra c√°c g·ª£i √Ω chi ti·∫øt cho ng∆∞·ªùi d√πng.\n",
        "    \"\"\"\n",
        "    return location_prompt_template\n",
        "\n",
        "def analyze_food_locations(structured_data):\n",
        "    \"\"\"Analyze food and location data using Gemini API\"\"\"\n",
        "    \n",
        "    import google.generativeai as genai\n",
        "    from collections import Counter\n",
        "    \n",
        "    # Configure API\n",
        "    genai.configure(api_key='AIzaSyCtSe_5iLidRs0CaVSIOGgRrLK7H29jZfY')\n",
        "    model = genai.GenerativeModel('models/gemini-2.0-flash-thinking-exp-1219')\n",
        "    \n",
        "    # Extract unique foods and locations\n",
        "    all_foods = []\n",
        "    all_locations = []\n",
        "    \n",
        "    for item in structured_data:\n",
        "        all_foods.extend(item['extracted_entities']['foods'])\n",
        "        all_locations.extend(item['extracted_entities']['locations'])\n",
        "    \n",
        "    # Count frequencies\n",
        "    food_counts = Counter(all_foods)\n",
        "    location_counts = Counter(all_locations)\n",
        "    \n",
        "    # Create prompts\n",
        "    food_prompt = create_food_prompts().format(\n",
        "        foods=\"\\n\".join(f\"- {food} (xu·∫•t hi·ªán {count} l·∫ßn)\" \n",
        "                       for food, count in food_counts.most_common())\n",
        "    )\n",
        "    \n",
        "    location_prompt = create_location_prompts().format(\n",
        "        locations=\"\\n\".join(f\"- {loc} (xu·∫•t hi·ªán {count} l·∫ßn)\"\n",
        "                           for loc, count in location_counts.most_common())\n",
        "    )\n",
        "    \n",
        "    # Get responses from Gemini\n",
        "    food_analysis = model.generate_content(food_prompt)\n",
        "    location_analysis = model.generate_content(location_prompt)\n",
        "    \n",
        "    return {\n",
        "        'food_analysis': food_analysis.text,\n",
        "        'location_analysis': location_analysis.text,\n",
        "        'statistics': {\n",
        "            'total_unique_foods': len(set(all_foods)),\n",
        "            'total_unique_locations': len(set(all_locations)),\n",
        "            'most_common_foods': dict(food_counts.most_common(10)),\n",
        "            'most_common_locations': dict(location_counts.most_common(10))\n",
        "        }\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Ph√¢n t√≠ch m√≥n ƒÉn:\n",
            "D·ª±a tr√™n danh s√°ch m√≥n ƒÉn v√† t·∫ßn su·∫•t xu·∫•t hi·ªán c·ªßa ch√∫ng, ch√∫ng ta c√≥ th·ªÉ ti·∫øn h√†nh ph√¢n t√≠ch chi ti·∫øt nh∆∞ sau:\n",
            "\n",
            "**1. Ph√¢n lo·∫°i c√°c m√≥n ƒÉn th√†nh c√°c nh√≥m:**\n",
            "\n",
            "ƒê·ªÉ d·ªÖ d√†ng ph√¢n t√≠ch v√† hi·ªÉu r√µ h∆°n v·ªÅ danh s√°ch m√≥n ƒÉn n√†y, ch√∫ng ta c√≥ th·ªÉ ph√¢n lo·∫°i ch√∫ng v√†o c√°c nh√≥m d·ª±a tr√™n ƒë·∫∑c ƒëi·ªÉm chung. D∆∞·ªõi ƒë√¢y l√† m·ªôt c√°ch ph√¢n lo·∫°i chi ti·∫øt:\n",
            "\n",
            "*   **Nh√≥m Th·ªãt Gia C·∫ßm:**\n",
            "    *   G√† (xu·∫•t hi·ªán 9 l·∫ßn): Bao g·ªìm \"g√†\", \"g√† t√†u\", \"th·ªãt g√†\".\n",
            "    *   V·ªãt (xu·∫•t hi·ªán 1 l·∫ßn): \"v·ªãt l·∫≠u\".\n",
            "*   **Nh√≥m H·∫£i S·∫£n:**\n",
            "    *   C√° (xu·∫•t hi·ªán 8 l·∫ßn): Bao g·ªìm \"c√°\", \"c√° tai t∆∞·ª£ng\".\n",
            "    *   T√¥m (xu·∫•t hi·ªán 4 l·∫ßn): Bao g·ªìm \"t√¥m\", \"t√¥m t∆∞∆°i\", \"t√¥m ƒë·∫°i d∆∞∆°ng\", \"t√¥m th·ªãt\", \"t√¥m ƒë·∫°i d∆∞∆°ng ch√¢n g√†\", \"t√¥m h√¥m\", \"c∆°m h√πm\".\n",
            "    *   ·ªêc/S√≤ (xu·∫•t hi·ªán 23 l·∫ßn): Bao g·ªìm \"·ªëc\", \"·ªëc t∆∞∆°i\", \"·ªëc h∆∞∆°ng\", \"·ªëc h∆∞∆°ng h·∫•p\", \"·ªëc kh·∫ø\", \"·ªëc mu·ªôn n∆∞·ªõng ti√™u\", \"·ªëc b∆∞u n∆∞·ªõng\", \"·ªëc tr·ª©ng mu·ªëi\", \"·ªëc tr·ª©ng mu·ªëi b·∫°n\", \"·ªëc h∆∞∆°ng x·ªët tr·ª©ng mu·ªëi\", \"·ªëc tr·ª©ng mu·ªëi tr·ª©ng\", \"·ªëc boulot\", \"s√≤ l√¥ng m·ª°\", \"s√≤ ƒë·ªìng gi√°\", \"s√≤ ·ªëc\".  *L∆∞u √Ω: Nh√≥m n√†y c√≥ s·ªë l∆∞·ª£ng m√≥n v√† bi·∫øn th·ªÉ r·∫•t ƒëa d·∫°ng, cho th·∫•y s·ª± ph·ªï bi·∫øn c·ªßa ·ªëc v√† s√≤.*\n",
            "*   **Nh√≥m Th·ªãt ƒê·ªè:**\n",
            "    *   Th·ªãt (xu·∫•t hi·ªán 7 l·∫ßn): Bao g·ªìm \"th·ªãt\", \"th·ªãt s√¥ng kh√≥i\", \"th·ªãt ngu·ªôi s√¥ng kh√≥i\", \"th·ªãt ng·ªçt n∆∞·ªõc\", \"th·ªãt t∆∞∆°i\", \"th·ªãt ƒë√¥ng l·∫°nh\". *L∆∞u √Ω: \"Th·ªãt\" ·ªü ƒë√¢y c√≥ th·ªÉ hi·ªÉu l√† th·ªãt heo ho·∫∑c th·ªãt n√≥i chung.*\n",
            "    *   B√≤ (xu·∫•t hi·ªán 18 l·∫ßn): Bao g·ªìm \"b√≤\", \"b√≤ t∆∞∆°i\", \"th·ªãt b√≤ t∆∞∆°i\", \"th·ªãt b√≤\", \"b√≤ b√≤\", \"b√≤ v√†ng\", \"b√≤ b√≥p\", \"g·ªèi b√≤ b√≥p\", \"b√≤ m·∫Øm ru·ªët\", \"b√≤ m·ªÅm\", \"b√≤ t∆∞∆°i th√°i\", \"b√≤ thai\", \"b√≤ b√™n qu√°n\", \"b√≤ n∆∞·ªõng t·∫£ng\", \"c∆°m mi·∫øng b√≤ thai\", \"th·ªãt b√≤ t∆∞∆°i th√°i\". *L∆∞u √Ω: Nh√≥m b√≤ c√≥ nhi·ªÅu bi·∫øn th·ªÉ v√† c√°ch ch·∫ø bi·∫øn kh√°c nhau.*\n",
            "    *   Heo (xu·∫•t hi·ªán 1 l·∫ßn): \"heo\".\n",
            "*   **Nh√≥m Rau c·ªß qu·∫£:**\n",
            "    *   Rau (xu·∫•t hi·ªán 2 l·∫ßn): Bao g·ªìm \"rau\", \"rau tr∆∞·ªõc\".\n",
            "    *   MƒÉng (xu·∫•t hi·ªán 1 l·∫ßn): \"mƒÉng\".\n",
            "    *   N·∫•m (xu·∫•t hi·ªán 1 l·∫ßn): \"n·∫•m\".\n",
            "*   **Nh√≥m ƒê·∫≠u:**\n",
            "    *   ƒê·∫≠u h·ªß (xu·∫•t hi·ªán 2 l·∫ßn): Bao g·ªìm \"ƒë·∫≠u h·ªß\", \"ƒë·∫≠u h·ªß gi√≤n b√©o\", \"ƒë·∫≠u h·ªß k√¨\".\n",
            "*   **Nh√≥m S·ªØa v√† S·∫£n ph·∫©m t·ª´ S·ªØa:**\n",
            "    *   S·ªØa (xu·∫•t hi·ªán 3 l·∫ßn): Bao g·ªìm \"s·ªØa\", \"s·ªØa ph∆∞·ªùng\".\n",
            "    *   S·ªØa chua (xu·∫•t hi·ªán 2 l·∫ßn): Bao g·ªìm \"s·ªØa chua\", \"s·ªØa chua n·∫øp c·∫©m\".\n",
            "    *   Kem (xu·∫•t hi·ªán 2 l·∫ßn): Bao g·ªìm \"kem s·ªØa\", \"kem\".\n",
            "*   **Nh√≥m Tinh b·ªôt:**\n",
            "    *   C∆°m (xu·∫•t hi·ªán 2 l·∫ßn): Bao g·ªìm \"c∆°m\", \"c∆°m h√πm\", \"c∆°m mi·∫øng b√≤ thai\".\n",
            "    *   B√°nh m√¨ (xu·∫•t hi·ªán 1 l·∫ßn): \"b√°nh m√¨\".\n",
            "    *   X√¥i (xu·∫•t hi·ªán 1 l·∫ßn): \"x√¥i quanh\".\n",
            "    *   H·ªß ti·∫øu m√¨ (xu·∫•t hi·ªán 4 l·∫ßn): Bao g·ªìm \"h·ªß ti·∫øu m√¨ b√≤\", \"h·ªß ti·∫øu m√¨ s∆∞·ªùn\", \"h·ªß ti·∫øu m√¨ h·∫£i s·∫£n\", \"h·ªß ti·∫øu m√¨ s∆∞·ªùn h√°\".\n",
            "*   **Nh√≥m L·∫©u:**\n",
            "    *   L·∫©u (xu·∫•t hi·ªán 3 l·∫ßn): Bao g·ªìm \"l·∫©u mƒÉng\", \"l·∫©u b√≤\", \"l·∫©u t√°i nh√∫ng\".\n",
            "\n",
            "**2. X√°c ƒë·ªãnh c√°c m√≥n ƒë·∫∑c tr∆∞ng nh·∫•t:**\n",
            "\n",
            "*   **·ªêc/S√≤:** V·ªõi s·ªë l∆∞·ª£ng xu·∫•t hi·ªán v√† bi·∫øn th·ªÉ ƒëa d·∫°ng, c√°c m√≥n ·ªëc v√† s√≤ l√† m·ªôt trong nh·ªØng m√≥n ƒë·∫∑c tr∆∞ng nh·∫•t trong danh s√°ch n√†y. S·ª± ƒëa d·∫°ng trong t√™n g·ªçi (·ªëc h∆∞∆°ng, ·ªëc kh·∫ø, ·ªëc b∆∞u, ·ªëc tr·ª©ng mu·ªëi, ·ªëc boulot, s√≤ l√¥ng, s√≤ ƒë·ªìng) cho th·∫•y s·ª± phong ph√∫ c·ªßa c√°c lo·∫°i ·ªëc v√† s√≤ ƒë∆∞·ª£c ∆∞a chu·ªông.\n",
            "*   **B√≤:** C√°c m√≥n b√≤ c≈©ng r·∫•t ƒëa d·∫°ng v√† phong ph√∫, t·ª´ b√≤ t∆∞∆°i, b√≤ b√≥p, b√≤ n∆∞·ªõng t·∫£ng ƒë·∫øn c√°c m√≥n l·∫©u b√≤, c∆°m b√≤, h·ªß ti·∫øu b√≤. ƒêi·ªÅu n√†y cho th·∫•y th·ªãt b√≤ l√† m·ªôt nguy√™n li·ªáu ph·ªï bi·∫øn v√† ƒë∆∞·ª£c ch·∫ø bi·∫øn th√†nh nhi·ªÅu m√≥n ƒÉn kh√°c nhau.\n",
            "*   **H·ªß ti·∫øu m√¨:**  S·ª± xu·∫•t hi·ªán c·ªßa nhi·ªÅu lo·∫°i h·ªß ti·∫øu m√¨ (b√≤, s∆∞·ªùn, h·∫£i s·∫£n, s∆∞·ªùn h√°) cho th·∫•y ƒë√¢y l√† m·ªôt m√≥n ƒÉn ƒë·∫∑c tr∆∞ng v√† ph·ªï bi·∫øn trong danh s√°ch n√†y, c√≥ th·ªÉ l√† m√≥n ƒÉn s√°ng, tr∆∞a ho·∫∑c t·ªëi.\n",
            "*   **L·∫©u:**  C√°c m√≥n l·∫©u nh∆∞ l·∫©u mƒÉng, l·∫©u b√≤, l·∫©u t√°i nh√∫ng c≈©ng l√† nh·ªØng m√≥n ƒÉn ƒë·∫∑c tr∆∞ng, th∆∞·ªùng ƒë∆∞·ª£c d√πng trong c√°c b·ªØa ƒÉn gia ƒë√¨nh ho·∫∑c nh√≥m b·∫°n b√®, ƒë·∫∑c bi·ªát v√†o th·ªùi ti·∫øt m√°t m·∫ª.\n",
            "*   **C√°c m√≥n ch·∫ø bi·∫øn t·ª´ s·ªØa:** S·ªØa chua n·∫øp c·∫©m, kem s·ªØa, kem cho th·∫•y s·ª± quan t√¢m ƒë·∫øn c√°c m√≥n tr√°ng mi·ªáng ho·∫∑c ƒë·ªì u·ªëng t·ª´ s·ªØa.\n",
            "\n",
            "**3. ƒê·ªÅ xu·∫•t m√≥n ƒÉn ph·ªï bi·∫øn nh·∫•t d·ª±a tr√™n t·∫ßn su·∫•t xu·∫•t hi·ªán:**\n",
            "\n",
            "D·ª±a tr√™n t·∫ßn su·∫•t xu·∫•t hi·ªán, c√≥ th·ªÉ ƒë·ªÅ xu·∫•t c√°c m√≥n ƒÉn ph·ªï bi·∫øn nh·∫•t nh∆∞ sau:\n",
            "\n",
            "*   **G√† (9 l·∫ßn):**  G√† l√† m√≥n ƒÉn ph·ªï bi·∫øn nh·∫•t trong danh s√°ch, cho th·∫•y s·ª± ∆∞a chu·ªông c√°c m√≥n ƒÉn t·ª´ g√†.\n",
            "*   **C√° (8 l·∫ßn):** C√° ƒë·ª©ng th·ª© hai v·ªÅ t·∫ßn su·∫•t, cho th·∫•y c√°c m√≥n c√° c≈©ng r·∫•t ƒë∆∞·ª£c y√™u th√≠ch.\n",
            "*   **Th·ªãt v√† ·ªêc (c√πng 7 l·∫ßn):**  Th·ªãt (c√≥ th·ªÉ hi·ªÉu l√† th·ªãt heo ho·∫∑c th·ªãt n√≥i chung) v√† ·ªëc c√≥ t·∫ßn su·∫•t xu·∫•t hi·ªán t∆∞∆°ng ƒë∆∞∆°ng nhau, cho th·∫•y s·ª± ph·ªï bi·∫øn c·ªßa c·∫£ hai lo·∫°i nguy√™n li·ªáu n√†y.\n",
            "*   **B√≤ (6 l·∫ßn):**  B√≤ c≈©ng l√† m·ªôt m√≥n ƒÉn ph·ªï bi·∫øn, ƒë·ª©ng th·ª© nƒÉm v·ªÅ t·∫ßn su·∫•t.\n",
            "*   **T√¥m (4 l·∫ßn):** T√¥m c≈©ng l√† m·ªôt l·ª±a ch·ªçn h·∫£i s·∫£n ƒë∆∞·ª£c ∆∞a chu·ªông.\n",
            "*   **S·ªØa (3 l·∫ßn):** S·ªØa v√† c√°c s·∫£n ph·∫©m t·ª´ s·ªØa c≈©ng xu·∫•t hi·ªán nhi·ªÅu l·∫ßn, cho th·∫•y s·ª± quan t√¢m ƒë·∫øn ƒë·ªì u·ªëng v√† tr√°ng mi·ªáng.\n",
            "\n",
            "**4. Li√™n k·∫øt m√≥n ƒÉn v·ªõi vƒÉn h√≥a ·∫©m th·ª±c Vi·ªát Nam:**\n",
            "\n",
            "Danh s√°ch m√≥n ƒÉn n√†y ph·∫£n √°nh nhi·ªÅu n√©t ƒë·∫∑c tr∆∞ng c·ªßa vƒÉn h√≥a ·∫©m th·ª±c Vi·ªát Nam:\n",
            "\n",
            "*   **S·ª± ƒëa d·∫°ng c·ªßa nguy√™n li·ªáu:** Danh s√°ch s·ª≠ d·ª•ng nhi·ªÅu lo·∫°i nguy√™n li·ªáu kh√°c nhau t·ª´ th·ªãt gia c·∫ßm (g√†, v·ªãt), h·∫£i s·∫£n (c√°, t√¥m, ·ªëc, s√≤), th·ªãt ƒë·ªè (th·ªãt, b√≤, heo), rau c·ªß (rau, mƒÉng, n·∫•m), ƒë·∫≠u h·ªß, s·ªØa, tinh b·ªôt (c∆°m, b√°nh m√¨, x√¥i, h·ªß ti·∫øu m√¨). ƒêi·ªÅu n√†y th·ªÉ hi·ªán s·ª± phong ph√∫ v√† ƒëa d·∫°ng trong ngu·ªìn nguy√™n li·ªáu c·ªßa ·∫©m th·ª±c Vi·ªát Nam.\n",
            "*   **∆Øu ti√™n h·∫£i s·∫£n v√† c√°c m√≥n t·ª´ ·ªëc/s√≤:** S·ªë l∆∞·ª£ng m√≥n ·ªëc/s√≤ v√† h·∫£i s·∫£n xu·∫•t hi·ªán nhi·ªÅu cho th·∫•y s·ª± ∆∞a chu·ªông c√°c m√≥n ƒÉn t·ª´ bi·ªÉn, ƒë·∫∑c bi·ªát l√† ·ªëc v√† s√≤, v·ªën l√† nh·ªØng ƒë·∫∑c s·∫£n ph·ªï bi·∫øn ·ªü nhi·ªÅu v√πng ven bi·ªÉn Vi·ªát Nam.\n",
            "*   **S·ª± ph·ªï bi·∫øn c·ªßa c√°c m√≥n th·ªãt:** G√†, th·ªãt, b√≤ l√† nh·ªØng lo·∫°i th·ªãt ƒë∆∞·ª£c ti√™u th·ª• nhi·ªÅu nh·∫•t, ph·∫£n √°nh th√≥i quen ƒÉn u·ªëng c·ªßa ng∆∞·ªùi Vi·ªát Nam.\n",
            "*   **C√°c m√≥n n∆∞·ªõc v√† l·∫©u:** H·ªß ti·∫øu m√¨ v√† l·∫©u l√† nh·ªØng m√≥n ƒÉn ph·ªï bi·∫øn trong ·∫©m th·ª±c Vi·ªát Nam, th∆∞·ªùng ƒë∆∞·ª£c d√πng trong c√°c b·ªØa ƒÉn gia ƒë√¨nh ho·∫∑c khi t·ª• t·∫≠p b·∫°n b√®. H·ªß ti·∫øu m√¨ l√† m√≥n ƒÉn s√°ng, tr∆∞a, t·ªëi quen thu·ªôc, c√≤n l·∫©u th∆∞·ªùng ƒë∆∞·ª£c ∆∞a chu·ªông trong th·ªùi ti·∫øt m√°t m·∫ª ho·∫∑c c√°c d·ªãp ƒë·∫∑c bi·ªát.\n",
            "*   **S·ª≠ d·ª•ng rau c·ªß t∆∞∆°i:** S·ª± xu·∫•t hi·ªán c·ªßa \"rau\" v√† \"rau tr∆∞·ªõc\" cho th·∫•y rau xanh l√† m·ªôt ph·∫ßn kh√¥ng th·ªÉ thi·∫øu trong b·ªØa ƒÉn Vi·ªát Nam, ƒë·∫£m b·∫£o s·ª± c√¢n b·∫±ng dinh d∆∞·ª°ng v√† h∆∞∆°ng v·ªã t∆∞∆°i ngon.\n",
            "*   **S·ª± k·∫øt h·ª£p gi·ªØa truy·ªÅn th·ªëng v√† hi·ªán ƒë·∫°i:** B√™n c·∫°nh c√°c m√≥n ƒÉn truy·ªÅn th·ªëng nh∆∞ g√†, c√°, th·ªãt, ·ªëc, b√≤, danh s√°ch c≈©ng c√≥ nh·ªØng m√≥n mang h∆°i h∆∞·ªõng hi·ªán ƒë·∫°i h∆°n nh∆∞ \"ƒë·∫≠u h·ªß gi√≤n b√©o\", \"kem s·ªØa\", \"s·ªØa chua n·∫øp c·∫©m\", th·ªÉ hi·ªán s·ª± ph√°t tri·ªÉn v√† giao thoa c·ªßa ·∫©m th·ª±c Vi·ªát Nam.\n",
            "\n",
            "**5. ƒê·ªÅ xu·∫•t c√°c k·∫øt h·ª£p m√≥n ƒÉn ph√π h·ª£p:**\n",
            "\n",
            "D·ª±a tr√™n danh s√°ch v√† vƒÉn h√≥a ·∫©m th·ª±c Vi·ªát Nam, c√≥ th·ªÉ ƒë·ªÅ xu·∫•t m·ªôt s·ªë k·∫øt h·ª£p m√≥n ƒÉn ph√π h·ª£p nh∆∞ sau:\n",
            "\n",
            "*   **B·ªØa ƒÉn gia ƒë√¨nh truy·ªÅn th·ªëng:**\n",
            "    *   **M√≥n ch√≠nh:** G√† lu·ªôc ho·∫∑c g√† n∆∞·ªõng, c√° chi√™n ho·∫∑c c√° kho, th·ªãt kho t√†u ho·∫∑c th·ªãt ram, b√≤ x√†o ho·∫∑c b√≤ n∆∞·ªõng.\n",
            "    *   **M√≥n rau:** Rau lu·ªôc ho·∫∑c rau x√†o.\n",
            "    *   **M√≥n canh:** L·∫©u mƒÉng ho·∫∑c l·∫©u b√≤ (n·∫øu ph√π h·ª£p th·ªùi ti·∫øt).\n",
            "    *   **C∆°m tr·∫Øng.**\n",
            "*   **B·ªØa ƒÉn h·∫£i s·∫£n:**\n",
            "    *   **M√≥n khai v·ªã:** G·ªèi b√≤ b√≥p (n·∫øu mu·ªën k·∫øt h·ª£p th·ªãt), ho·∫∑c ch·ªâ t·∫≠p trung h·∫£i s·∫£n nh∆∞ s√≤ l√¥ng m·ª° h√†nh, ·ªëc h∆∞∆°ng x·ªët tr·ª©ng mu·ªëi.\n",
            "    *   **M√≥n ch√≠nh:** T√¥m n∆∞·ªõng, c√° tai t∆∞·ª£ng chi√™n x√π, ·ªëc h∆∞∆°ng h·∫•p, ·ªëc b∆∞u n∆∞·ªõng, c√°c m√≥n s√≤ ·ªëc kh√°c.\n",
            "    *   **M√≥n rau:** Rau s·ªëng ƒÉn k√®m h·∫£i s·∫£n ho·∫∑c rau x√†o.\n",
            "    *   **B√∫n ho·∫∑c c∆°m.**\n",
            "*   **B·ªØa ƒÉn nhanh bu·ªïi s√°ng/tr∆∞a:**\n",
            "    *   H·ªß ti·∫øu m√¨ b√≤, h·ªß ti·∫øu m√¨ s∆∞·ªùn, h·ªß ti·∫øu m√¨ h·∫£i s·∫£n.\n",
            "    *   B√°nh m√¨ th·ªãt.\n",
            "    *   X√¥i g√†.\n",
            "*   **M√≥n tr√°ng mi·ªáng/ƒë·ªì u·ªëng:**\n",
            "    *   S·ªØa chua n·∫øp c·∫©m.\n",
            "    *   Kem s·ªØa ho·∫∑c kem.\n",
            "    *   S·ªØa t∆∞∆°i.\n",
            "\n",
            "**K·∫øt lu·∫≠n:**\n",
            "\n",
            "Danh s√°ch m√≥n ƒÉn n√†y th·ªÉ hi·ªán s·ª± ƒëa d·∫°ng v√† phong ph√∫ c·ªßa ·∫©m th·ª±c Vi·ªát Nam, v·ªõi s·ª± ∆∞u ti√™n c√°c m√≥n t·ª´ h·∫£i s·∫£n (ƒë·∫∑c bi·ªát l√† ·ªëc/s√≤), th·ªãt gia c·∫ßm v√† th·ªãt ƒë·ªè. C√°c m√≥n n∆∞·ªõc nh∆∞ h·ªß ti·∫øu m√¨ v√† l·∫©u c≈©ng r·∫•t ph·ªï bi·∫øn. Ph√¢n t√≠ch t·∫ßn su·∫•t xu·∫•t hi·ªán gi√∫p nh·∫≠n di·ªán ƒë∆∞·ª£c nh·ªØng m√≥n ƒÉn ƒë∆∞·ª£c ∆∞a chu·ªông nh·∫•t, ƒë·ªìng th·ªùi li√™n k·∫øt v·ªõi vƒÉn h√≥a ·∫©m th·ª±c Vi·ªát Nam gi√∫p hi·ªÉu r√µ h∆°n v·ªÅ th√≥i quen ƒÉn u·ªëng v√† s·ªü th√≠ch ·∫©m th·ª±c c·ªßa ng∆∞·ªùi Vi·ªát. C√°c ƒë·ªÅ xu·∫•t k·∫øt h·ª£p m√≥n ƒÉn mang t√≠nh g·ª£i √Ω, c√≥ th·ªÉ t√πy ch·ªânh theo s·ªü th√≠ch v√† ho√†n c·∫£nh c·ª• th·ªÉ.\n",
            "\n",
            "Ph√¢n t√≠ch ƒë·ªãa ƒëi·ªÉm:\n",
            "D·ª±a tr√™n th√¥ng tin b·∫°n cung c·∫•p v·ªÅ danh s√°ch ƒë·ªãa ƒëi·ªÉm ·∫©m th·ª±c, ch√∫ng ta c√≥ th·ªÉ ph√¢n t√≠ch v√† ƒë∆∞a ra c√°c g·ª£i √Ω nh∆∞ sau:\n",
            "\n",
            "**1. Nh√≥m c√°c ƒë·ªãa ƒëi·ªÉm theo khu v·ª±c (qu·∫≠n/huy·ªán):**\n",
            "\n",
            "* **Qu·∫≠n B√¨nh Th·∫°nh:** b√¨nh th·∫°nh (xu·∫•t hi·ªán 2 l·∫ßn)\n",
            "* **Qu·∫≠n Th·ªß ƒê·ª©c:** th·ªß ƒë·ª©c (xu·∫•t hi·ªán 1 l·∫ßn)\n",
            "* **Huy·ªán C·ªß Chi:** c·ªß chi (xu·∫•t hi·ªán 1 l·∫ßn)\n",
            "* **Qu·∫≠n T√¢n B√¨nh:** t√¢n b√¨nh (xu·∫•t hi·ªán 1 l·∫ßn)\n",
            "* **Qu·∫≠n Ph√∫ Nhu·∫≠n:** ph√∫ nhu·∫≠n (xu·∫•t hi·ªán 1 l·∫ßn), ph√∫ nhu·∫≠n g√≤ (xu·∫•t hi·ªán 1 l·∫ßn) - *L∆∞u √Ω: \"ph√∫ nhu·∫≠n g√≤\" c√≥ th·ªÉ l√† c√°ch g·ªçi kh√¥ng ch√≠nh x√°c, c√≥ th·ªÉ ch·ªâ khu v·ª±c Ph√∫ Nhu·∫≠n g·∫ßn G√≤ V·∫•p ho·∫∑c l√† l·ªói g√µ ph√≠m. ƒê·ªÉ ƒë∆°n gi·∫£n, t·∫°m th·ªùi xem x√©t l√† khu v·ª±c thu·ªôc Ph√∫ Nhu·∫≠n ho·∫∑c l√¢n c·∫≠n.*\n",
            "* **\"khuy·∫øn\":**  *ƒê·ªãa ƒëi·ªÉm n√†y kh√¥ng r√µ r√†ng v√† kh√¥ng x√°c ƒë·ªãnh ƒë∆∞·ª£c qu·∫≠n/huy·ªán. C√≥ th·ªÉ l√† l·ªói ch√≠nh t·∫£ ho·∫∑c th√¥ng tin kh√¥ng ƒë·∫ßy ƒë·ªß. C·∫ßn lo·∫°i tr·ª´ kh·ªèi ph√¢n t√≠ch khu v·ª±c n·∫øu kh√¥ng c√≥ th√™m th√¥ng tin.*\n",
            "\n",
            "**Danh s√°ch khu v·ª±c sau khi nh√≥m:**\n",
            "\n",
            "* **Qu·∫≠n B√¨nh Th·∫°nh:** (2 l·∫ßn)\n",
            "* **Qu·∫≠n Ph√∫ Nhu·∫≠n (bao g·ªìm c·∫£ \"ph√∫ nhu·∫≠n g√≤\" kh√¥ng r√µ r√†ng):** (2 l·∫ßn)\n",
            "* **Qu·∫≠n Th·ªß ƒê·ª©c:** (1 l·∫ßn)\n",
            "* **Huy·ªán C·ªß Chi:** (1 l·∫ßn)\n",
            "* **Qu·∫≠n T√¢n B√¨nh:** (1 l·∫ßn)\n",
            "\n",
            "**2. X√°c ƒë·ªãnh c√°c khu v·ª±c ·∫©m th·ª±c n·ªïi ti·∫øng:**\n",
            "\n",
            "D·ª±a tr√™n s·ªë l·∫ßn xu·∫•t hi·ªán trong danh s√°ch (m·∫∑c d√π s·ªë l∆∞·ª£ng ƒë·ªãa ƒëi·ªÉm c√≤n h·∫°n ch·∫ø) v√† ki·∫øn th·ª©c chung v·ªÅ ·∫©m th·ª±c TP.HCM, c√≥ th·ªÉ nh·∫≠n ƒë·ªãnh:\n",
            "\n",
            "* **Qu·∫≠n B√¨nh Th·∫°nh v√† Qu·∫≠n Ph√∫ Nhu·∫≠n:**  C√≥ v·∫ª l√† nh·ªØng khu v·ª±c ƒë∆∞·ª£c nh·∫Øc ƒë·∫øn nhi·ªÅu h∆°n trong danh s√°ch n√†y, cho th·∫•y c√≥ th·ªÉ ƒë√¢y l√† nh·ªØng ƒëi·ªÉm ƒë·∫øn ·∫©m th·ª±c ƒë∆∞·ª£c quan t√¢m. C·∫£ hai qu·∫≠n n√†y ƒë·ªÅu n·ªïi ti·∫øng v·ªõi s·ª± ƒëa d·∫°ng ·∫©m th·ª±c, t·ª´ qu√°n ƒÉn ƒë∆∞·ªùng ph·ªë, nh√† h√†ng gia ƒë√¨nh ƒë·∫øn c√°c ƒë·ªãa ƒëi·ªÉm sang tr·ªçng h∆°n.\n",
            "* **C√°c khu v·ª±c kh√°c (Th·ªß ƒê·ª©c, C·ªß Chi, T√¢n B√¨nh):**  C≈©ng l√† nh·ªØng khu v·ª±c c√≥ ·∫©m th·ª±c ri√™ng, nh∆∞ng trong danh s√°ch n√†y xu·∫•t hi·ªán √≠t h∆°n. ƒêi·ªÅu n√†y kh√¥ng c√≥ nghƒ©a l√† ch√∫ng k√©m n·ªïi ti·∫øng v·ªÅ ·∫©m th·ª±c, m√† c√≥ th·ªÉ ch·ªâ ƒë∆°n gi·∫£n l√† √≠t ƒë∆∞·ª£c ƒë·ªÅ c·∫≠p trong ngu·ªìn th√¥ng tin b·∫°n cung c·∫•p.\n",
            "\n",
            "**L∆∞u √Ω quan tr·ªçng:**  Danh s√°ch ƒë·ªãa ƒëi·ªÉm n√†y c√≤n r·∫•t h·∫°n ch·∫ø v√† kh√¥ng ƒë·∫°i di·ªán cho to√†n b·ªô b·∫£n ƒë·ªì ·∫©m th·ª±c TP.HCM.  Vi·ªác ƒë√°nh gi√° ƒë·ªô n·ªïi ti·∫øng ch·ªâ d·ª±a tr√™n s·ªë l·∫ßn xu·∫•t hi·ªán trong danh s√°ch n√†y l√† ch∆∞a ƒë·ªß. C·∫ßn tham kh·∫£o th√™m nhi·ªÅu ngu·ªìn th√¥ng tin kh√°c ƒë·ªÉ c√≥ c√°i nh√¨n to√†n di·ªán h∆°n.\n",
            "\n",
            "**3. ƒê·ªÅ xu·∫•t tuy·∫øn ƒë∆∞·ªùng kh√°m ph√° ·∫©m th·ª±c:**\n",
            "\n",
            "D·ª±a tr√™n c√°c khu v·ª±c ƒë√£ x√°c ƒë·ªãnh, c√≥ th·ªÉ ƒë·ªÅ xu·∫•t m·ªôt s·ªë tuy·∫øn ƒë∆∞·ªùng kh√°m ph√° ·∫©m th·ª±c nh∆∞ sau:\n",
            "\n",
            "* **Tuy·∫øn 1: Kh√°m ph√° ·∫©m th·ª±c B√¨nh Th·∫°nh - Ph√∫ Nhu·∫≠n:**\n",
            "    * **B·∫Øt ƒë·∫ßu t·ª´ B√¨nh Th·∫°nh:** Kh√°m ph√° c√°c khu v·ª±c ·∫©m th·ª±c ƒë∆∞·ªùng ph·ªë n·ªïi ti·∫øng nh∆∞ ƒë∆∞·ªùng Phan VƒÉn H√¢n, khu v·ª±c ch·ª£ B√† Chi·ªÉu, ƒë∆∞·ªùng X√¥ Vi·∫øt Ngh·ªá Tƒ©nh. B√¨nh Th·∫°nh n·ªïi ti·∫øng v·ªõi h·∫£i s·∫£n ven s√¥ng, l·∫©u, n∆∞·ªõng, v√† c√°c m√≥n ƒÉn v·∫∑t ƒëa d·∫°ng.\n",
            "    * **Di chuy·ªÉn sang Ph√∫ Nhu·∫≠n:**  Kh√°m ph√° c√°c con ƒë∆∞·ªùng ·∫©m th·ª±c nh∆∞ Phan X√≠ch Long, Hu·ª≥nh VƒÉn B√°nh, Nguy·ªÖn Tr·ªçng Tuy·ªÉn. Ph√∫ Nhu·∫≠n ƒëa d·∫°ng t·ª´ cafe, b√°nh ng·ªçt, m√≥n ƒÉn gia ƒë√¨nh, ƒë·∫øn c√°c nh√† h√†ng phong c√°ch.\n",
            "    * **∆Øu ƒëi·ªÉm:** Hai qu·∫≠n n√†y li·ªÅn k·ªÅ, d·ªÖ d√†ng di chuy·ªÉn, t·∫≠p trung nhi·ªÅu ƒë·ªãa ƒëi·ªÉm ·∫©m th·ª±c ƒëa d·∫°ng.\n",
            "    * **Ph√π h·ª£p:** V·ªõi ng∆∞·ªùi mu·ªën kh√°m ph√° nhi·ªÅu lo·∫°i h√¨nh ·∫©m th·ª±c trong m·ªôt ng√†y ho·∫∑c cu·ªëi tu·∫ßn, t·ª´ b√¨nh d√¢n ƒë·∫øn t·∫ßm trung.\n",
            "\n",
            "* **Tuy·∫øn 2:  ·∫®m th·ª±c ƒë∆∞·ªùng ph·ªë v√† sinh vi√™n Th·ªß ƒê·ª©c:**\n",
            "    * **T·∫≠p trung t·∫°i Th·ªß ƒê·ª©c:**  Kh√°m ph√° c√°c khu v·ª±c xung quanh c√°c tr∆∞·ªùng ƒë·∫°i h·ªçc (L√†ng ƒê·∫°i H·ªçc Th·ªß ƒê·ª©c), c√°c tuy·∫øn ƒë∆∞·ªùng nh∆∞ V√µ VƒÉn Ng√¢n, Ho√†ng Di·ªáu 2. Th·ªß ƒê·ª©c n·ªïi ti·∫øng v·ªõi ·∫©m th·ª±c sinh vi√™n gi√° r·∫ª, qu√°n ƒÉn v·∫∑t, qu√°n nh·∫≠u b√¨nh d√¢n.\n",
            "    * **∆Øu ƒëi·ªÉm:** Gi√° c·∫£ ph·∫£i chƒÉng, tr·∫£i nghi·ªám ·∫©m th·ª±c ƒë∆∞·ªùng ph·ªë s√¥i ƒë·ªông.\n",
            "    * **Ph√π h·ª£p:** V·ªõi sinh vi√™n, gi·ªõi tr·∫ª, ho·∫∑c ng∆∞·ªùi mu·ªën ti·∫øt ki·ªám chi ph√≠ v√† th√≠ch kh√¥ng kh√≠ n√°o nhi·ªát.\n",
            "\n",
            "* **Tuy·∫øn 3:  ·∫®m th·ª±c d√¢n d√£ v√† ƒë·∫∑c s·∫£n C·ªß Chi (d√†nh cho m·ªôt ng√†y):**\n",
            "    * **Di chuy·ªÉn ƒë·∫øn C·ªß Chi:**  Kh√°m ph√° c√°c nh√† h√†ng, qu√°n ƒÉn chuy√™n v·ªÅ ƒë·∫∑c s·∫£n C·ªß Chi nh∆∞ b√≤ t∆°, rau r·ª´ng, b√°nh x√®o, g·ªèi g√†.\n",
            "    * **∆Øu ƒëi·ªÉm:** Tr·∫£i nghi·ªám ·∫©m th·ª±c v√πng qu√™, kh√¥ng gian tho√°ng ƒë√£ng, g·∫ßn g≈©i thi√™n nhi√™n.\n",
            "    * **Ph√π h·ª£p:** V·ªõi ng∆∞·ªùi mu·ªën ƒë·ªïi gi√≥, t√¨m ki·∫øm kh√¥ng gian y√™n tƒ©nh v√† th∆∞·ªüng th·ª©c c√°c m√≥n ƒÉn ƒë·∫∑c tr∆∞ng v√πng mi·ªÅn.\n",
            "\n",
            "* **Tuy·∫øn 4:  ·∫®m th·ª±c ƒëa d·∫°ng T√¢n B√¨nh (g·∫ßn s√¢n bay):**\n",
            "    * **Kh√°m ph√° T√¢n B√¨nh:**  T·∫≠p trung v√†o c√°c khu v·ª±c xung quanh s√¢n bay T√¢n S∆°n Nh·∫•t, ƒë∆∞·ªùng Tr∆∞·ªùng S∆°n, khu v·ª±c ch·ª£ Ph·∫°m VƒÉn Hai. T√¢n B√¨nh c√≥ nhi·ªÅu qu√°n ƒÉn ph·ª•c v·ª• kh√°ch du l·ªãch, nh√† h√†ng ƒëa d·∫°ng m√≥n √Å - √Çu, qu√°n cafe.\n",
            "    * **∆Øu ƒëi·ªÉm:**  Ti·ªán l·ª£i cho du kh√°ch, nhi·ªÅu l·ª±a ch·ªçn ·∫©m th·ª±c qu·ªëc t·∫ø v√† ƒë·∫∑c s·∫£n v√πng mi·ªÅn.\n",
            "    * **Ph√π h·ª£p:** V·ªõi du kh√°ch, ng∆∞·ªùi c√≥ th·ªùi gian h·∫°n ch·∫ø g·∫ßn s√¢n bay, ho·∫∑c mu·ªën th∆∞·ªüng th·ª©c ·∫©m th·ª±c ƒëa d·∫°ng.\n",
            "\n",
            "**4. Li√™n k·∫øt ƒë·ªãa ƒëi·ªÉm v·ªõi ƒë·∫∑c tr∆∞ng ·∫©m th·ª±c:**\n",
            "\n",
            "* **B√¨nh Th·∫°nh:**  ·∫®m th·ª±c ƒë∆∞·ªùng ph·ªë s√¥i ƒë·ªông, h·∫£i s·∫£n ven s√¥ng, qu√°n nh·∫≠u b√¨nh d√¢n v√† nh√† h√†ng gia ƒë√¨nh. ƒê·∫∑c bi·ªát n·ªïi ti·∫øng v·ªõi c√°c m√≥n l·∫©u, n∆∞·ªõng, h·∫£i s·∫£n t∆∞∆°i s·ªëng.\n",
            "* **Ph√∫ Nhu·∫≠n:**  ƒêa d·∫°ng t·ª´ cafe, b√°nh ng·ªçt, m√≥n ƒÉn v·∫∑t, qu√°n ƒÉn gia ƒë√¨nh ƒë·∫øn nh√† h√†ng sang tr·ªçng.  Phong ph√∫ v·ªÅ ·∫©m th·ª±c qu·ªëc t·∫ø v√† c√°c m√≥n ƒÉn healthy, cafe ƒë·∫πp.\n",
            "* **Th·ªß ƒê·ª©c:**  ·∫®m th·ª±c sinh vi√™n gi√° r·∫ª, qu√°n ƒÉn v·∫∑t, tr√† s·ªØa, l·∫©u n∆∞·ªõng b√¨nh d√¢n, qu√°n nh·∫≠u. Gi√° c·∫£ ph·∫£i chƒÉng, kh√¥ng kh√≠ n√°o nhi·ªát.\n",
            "* **C·ªß Chi:**  ·∫®m th·ª±c d√¢n d√£, ƒë·∫∑c s·∫£n v√πng qu√™ nh∆∞ b√≤ t∆° C·ªß Chi, rau r·ª´ng, b√°nh x√®o, g·ªèi g√†, c√°c m√≥n ƒÉn ƒë·ªìng qu√™. Kh√¥ng gian tho√°ng ƒë√£ng, g·∫ßn g≈©i thi√™n nhi√™n.\n",
            "* **T√¢n B√¨nh:**  ·∫®m th·ª±c ƒëa d·∫°ng, ph·ª•c v·ª• kh√°ch du l·ªãch, nh√† h√†ng gia ƒë√¨nh, qu√°n ƒÉn ƒë·∫∑c s·∫£n v√πng mi·ªÅn, ·∫©m th·ª±c qu·ªëc t·∫ø. Do g·∫ßn s√¢n bay n√™n c√≥ nhi·ªÅu l·ª±a ch·ªçn ƒëa d·∫°ng.\n",
            "\n",
            "**5. X√°c ƒë·ªãnh c√°c ƒëi·ªÉm ·∫©m th·ª±c c√≥ m·∫≠t ƒë·ªô cao:**\n",
            "\n",
            "D·ª±a tr√™n danh s√°ch c·ªßa b·∫°n, **Qu·∫≠n B√¨nh Th·∫°nh** l√† khu v·ª±c xu·∫•t hi·ªán nhi·ªÅu nh·∫•t (2 l·∫ßn).  ƒêi·ªÅu n√†y c√≥ th·ªÉ g·ª£i √Ω r·∫±ng B√¨nh Th·∫°nh l√† m·ªôt trong nh·ªØng khu v·ª±c c√≥ m·∫≠t ƒë·ªô ƒë·ªãa ƒëi·ªÉm ·∫©m th·ª±c ƒë∆∞·ª£c nh·∫Øc ƒë·∫øn cao h∆°n trong ngu·ªìn th√¥ng tin n√†y.\n",
            "\n",
            "Tuy nhi√™n, c·∫ßn nh·∫•n m·∫°nh l·∫°i r·∫±ng:\n",
            "\n",
            "* **D·ªØ li·ªáu c√≤n qu√° √≠t:** Ch·ªâ c√≥ 7 ƒë·ªãa ƒëi·ªÉm ƒë∆∞·ª£c li·ªát k√™, v√† \"b√¨nh th·∫°nh\" l·∫∑p l·∫°i 2 l·∫ßn.  Kh√¥ng th·ªÉ k·∫øt lu·∫≠n ch·∫Øc ch·∫Øn v·ªÅ m·∫≠t ƒë·ªô cao d·ª±a tr√™n d·ªØ li·ªáu n√†y.\n",
            "* **\"M·∫≠t ƒë·ªô cao\" c·∫ßn ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a r√µ h∆°n:**  \"M·∫≠t ƒë·ªô cao\" c√≥ th·ªÉ hi·ªÉu l√† s·ªë l∆∞·ª£ng ƒë·ªãa ƒëi·ªÉm ·∫©m th·ª±c tr√™n m·ªôt ƒë∆°n v·ªã di·ªán t√≠ch, ho·∫∑c m·ª©c ƒë·ªô t·∫≠p trung c√°c ƒë·ªãa ƒëi·ªÉm n·ªïi ti·∫øng, ho·∫∑c t·∫ßn su·∫•t ƒë∆∞·ª£c nh·∫Øc ƒë·∫øn trong c√°c ngu·ªìn th√¥ng tin.  C·∫ßn c√≥ ƒë·ªãnh nghƒ©a r√µ r√†ng h∆°n ƒë·ªÉ ƒë√°nh gi√° ch√≠nh x√°c.\n",
            "\n",
            "**G·ª£i √Ω th√™m cho ng∆∞·ªùi d√πng:**\n",
            "\n",
            "* **M·ªü r·ªông ngu·ªìn th√¥ng tin:**  ƒê·ªÉ c√≥ c√°i nh√¨n to√†n di·ªán h∆°n, b·∫°n n√™n tham kh·∫£o th√™m nhi·ªÅu ngu·ªìn th√¥ng tin kh√°c v·ªÅ ƒë·ªãa ƒëi·ªÉm ·∫©m th·ª±c TP.HCM, v√≠ d·ª•:\n",
            "    * C√°c trang web v√† ·ª©ng d·ª•ng ƒë√°nh gi√° ·∫©m th·ª±c (Foody, Lozi, GrabFood, Baemin...).\n",
            "    * C√°c blog, review ·∫©m th·ª±c tr√™n m·∫°ng x√£ h·ªôi (Facebook, Instagram, TikTok...).\n",
            "    * C√°c b√†i b√°o, t·∫°p ch√≠ v·ªÅ ·∫©m th·ª±c ƒë·ªãa ph∆∞∆°ng.\n",
            "    * H·ªèi √Ω ki·∫øn b·∫°n b√®, ng∆∞·ªùi th√¢n, ng∆∞·ªùi d√¢n ƒë·ªãa ph∆∞∆°ng.\n",
            "* **X√°c ƒë·ªãnh r√µ nhu c·∫ßu v√† s·ªü th√≠ch:**  B·∫°n mu·ªën kh√°m ph√° lo·∫°i h√¨nh ·∫©m th·ª±c n√†o? Ng√¢n s√°ch bao nhi√™u?  Th√≠ch kh√¥ng gian n√†o (ƒë∆∞·ªùng ph·ªë, nh√† h√†ng, qu√°n cafe...)?  Khi x√°c ƒë·ªãnh r√µ nhu c·∫ßu, b·∫°n s·∫Ω d·ªÖ d√†ng l·ª±a ch·ªçn ƒë∆∞·ª£c khu v·ª±c v√† tuy·∫øn ƒë∆∞·ªùng kh√°m ph√° ·∫©m th·ª±c ph√π h·ª£p.\n",
            "* **L∆∞u √Ω v·ªÅ \"ph√∫ nhu·∫≠n g√≤\" v√† \"khuy·∫øn\":**  C·∫ßn l√†m r√µ th√¥ng tin v·ªÅ \"ph√∫ nhu·∫≠n g√≤\" (c√≥ th·ªÉ l√† Ph√∫ Nhu·∫≠n ho·∫∑c G√≤ V·∫•p, ho·∫∑c khu v·ª±c l√¢n c·∫≠n) v√† lo·∫°i b·ªè \"khuy·∫øn\" v√¨ kh√¥ng x√°c ƒë·ªãnh ƒë∆∞·ª£c ƒë·ªãa ƒëi·ªÉm. N·∫øu c√≥ th√™m th√¥ng tin, h√£y c·∫≠p nh·∫≠t ƒë·ªÉ ph√¢n t√≠ch ch√≠nh x√°c h∆°n.\n",
            "\n",
            "Hy v·ªçng ph√¢n t√≠ch n√†y s·∫Ω h·ªØu √≠ch cho b·∫°n trong vi·ªác kh√°m ph√° ·∫©m th·ª±c TP.HCM!\n"
          ]
        }
      ],
      "source": [
        "# Example usage\n",
        "import json\n",
        "\n",
        "# Load structured data\n",
        "with open('C:/Users/nguye/OneDrive/TaÃÄi li√™Ã£u/GitHub/21KHDL-TikTok-Analytics/notebooks/extracted_data/structured_entities.json', 'r', encoding='utf-8') as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "# Run analysis\n",
        "results = analyze_food_locations(data)\n",
        "\n",
        "# Save results\n",
        "with open('food_location_analysis.json', 'w', encoding='utf-8') as f:\n",
        "    json.dump(results, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(\"\\nPh√¢n t√≠ch m√≥n ƒÉn:\")\n",
        "print(results['food_analysis'])\n",
        "print(\"\\nPh√¢n t√≠ch ƒë·ªãa ƒëi·ªÉm:\")\n",
        "print(results['location_analysis'])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "AppliedDataProject",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
