{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 172,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U91UVADr4Phh",
        "outputId": "f4a3bd1b-2a17-4e04-dbda-2bfbe125eeda"
      },
      "outputs": [],
      "source": [
        "# %%shell\n",
        "# git clone --branch TrggTin --single-branch https://github.com/vphuhan/21KHDL-TikTok-Analytics.git\n",
        "# cd 21KHDL-TikTok-Analytics\n",
        "# git sparse-checkout init --cone\n",
        "# git sparse-checkout set data/interim\n",
        "# git checkout"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 173,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0NZ7_Zrw5PFe",
        "outputId": "d4ed584f-a229-4763-aa43-af3871f0893e"
      },
      "outputs": [],
      "source": [
        "# pip install pandas nltk underthesea scikit-learn tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Imports and Initialization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 174,
      "metadata": {
        "id": "7GYwq-Tf4PTa"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import unicodedata\n",
        "import nltk\n",
        "from underthesea import word_tokenize, pos_tag, ner\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from difflib import get_close_matches\n",
        "import logging\n",
        "import json\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import string\n",
        "import regex as re\n",
        "import traceback\n",
        "import jdc  \n",
        "from spellchecker import SpellChecker\n",
        "from datetime import datetime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 175,
      "metadata": {
        "id": "pfZUR8vY4POT"
      },
      "outputs": [],
      "source": [
        "# Configure logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
        "    handlers=[\n",
        "        logging.FileHandler(\"extraction_log.log\"),\n",
        "        logging.StreamHandler()\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 176,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aDuHFYPU6NKm",
        "outputId": "2a87ddda-2e79-4d97-ed72-1a24c515c520"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt_tab to\n",
            "[nltk_data]     C:\\Users\\nguye\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 176,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# VietnameseTextProcessor Class Definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class VietnameseTextProcessor:\n",
        "    def __init__(self, food_list_path=None, location_list_path=None):\n",
        "        \"\"\"\n",
        "        Khởi tạo Bộ xử lý văn bản tiếng Việt\n",
        "\n",
        "        Tham số:\n",
        "            food_list_path (str): Đường dẫn đến tệp JSON chứa danh sách món ăn Việt Nam\n",
        "            location_list_path (str): Đường dẫn đến tệp JSON chứa danh sách địa điểm ở Việt Nam\n",
        "        \"\"\"\n",
        "        # Tải hoặc khởi tạo danh sách món ăn và địa điểm\n",
        "        self.foods = self._load_entity_list(food_list_path, \"foods\")\n",
        "        self.locations = self._load_entity_list(location_list_path, \"locations\")\n",
        "\n",
        "        # Các từ khóa phổ biến liên quan đến món ăn và hương vị trong tiếng Việt để hỗ trợ nhận diện\n",
        "        self.food_indicators = [\n",
        "            \"bánh mì\", \"phở\", \"bún\", \"xèo\", \"cơm\", \"gỏi\", \"chả\", \"xôi\", \"cao lầu\", \"cháo\",\n",
        "            \"mì gói\", \"hủ tiếu\", \"nem\", \"chả ram\", \"bánh khọt\",\n",
        "            \"lẩu\", \"cá\", \"thịt\", \"canh\", \"rau\", \"đậu\", \"ốc\", \"súp\", \"bắp\", \"lươn\", \"măng\", \"nấm\",\n",
        "            \"chuối\", \"nộm\", \"trà\", \"cà phê\", \"sinh tố\", \"kem\", \"tàu hủ\", \"chè\", \"yaourt\", \"nước mía\",\n",
        "            \"sữa\", \"kẹo\", \"đa\", \"nem chua\", \"gà\", \"bò\", \"heo\", \"vịt\", \"cá\", \"tôm\", \"mực, ốc\", \"sò\", \"hàu\",\n",
        "            \"bún riêu\", \"bún bò\", \"bún mắm\", \"bún mọc\", \"bún chả\", \"bún đậu\", \"bún ốc\"\n",
        "        ]\n",
        "\n",
        "        self.taste_indicators = [\n",
        "            \"ngon\", \"ngọt\", \"chua\", \"cay\", \"đắng\", \"mặn\", \"bùi\", \"béo\", \"giòn\", \"mềm\",\n",
        "            \"thơm\", \"nồng\", \"đậm đà\", \"nhạt\", \"thanh\", \"tươi\", \"chát\", \"cay nồng\", \"cay nhẹ\", \"cay vừa\",\n",
        "            \"sần sật\", \"mọng nước\", \"đắng nghét\", \"chát\", \"cay xè\", \"tê\", \"mặn chát\", \"ngọt lịm\", \"béo ngậy\", \"thơm lừng\",\n",
        "            \"nồng nàn\", \"đậm vị\", \"nhạt nhẽo\", \"thanh mát\", \"tươi\", \"đậm đà hương vị\", \"vừa ăn\", \"hợp khẩu vị\"\n",
        "        ]\n",
        "\n",
        "        self.locations_indicators = [ \n",
        "            \"quận 1\", \"quận 2\", \"quận 3\", \"quận 4\", \"quận 5\", \"quận 6\", \"quận 7\", \"quận 8\", \"quận 9\", \"quận 10\",\n",
        "            \"quận 11\", \"quận 12\", \"bình thạnh\", \"tân bình\", \"tân phú\", \"phú nhuận\", \"gò vấp\", \"bình tân\", \"thủ đức\", \"hóc môn\",\n",
        "            \"củ chi\", \"nhà bè\", \"cần giờ\", \"bình chánh\", \"tp thủ đức\",\n",
        "            \"hà nội\", \"hồ chí minh\", \"đà nẵng\", \"hải phòng\", \"cần thơ\", \"huế\", \"nha trang\", \"vũng tàu\", \"đà lạt\",\n",
        "            \"hạ long\", \"mỹ tho\", \"long xuyên\", \"rạch giá\", \"cà mau\", \"biên hòa\", \"buôn ma thuột\", \"thái nguyên\", \"nam định\"\n",
        "        ]\n",
        "\n",
        "\n",
        "        # Tải các tài nguyên của NLTK nếu cần\n",
        "        try:\n",
        "            nltk.data.find('tokenizers/punkt')\n",
        "        except LookupError:\n",
        "            nltk.download('punkt')\n",
        "\n",
        "        # Tạo thư mục để lưu trữ các tệp dữ liệu được trích xuất\n",
        "        os.makedirs(\"extracted_data\", exist_ok=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Helper Methods"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 178,
      "metadata": {},
      "outputs": [],
      "source": [
        "def _load_entity_list(self, file_path, entity_type):\n",
        "    \"\"\"Tải danh sách thực thể từ tệp hoặc trả về tập rỗng mặc định\"\"\"\n",
        "    if file_path and os.path.exists(file_path):\n",
        "        try:\n",
        "            with open(file_path, 'r', encoding='utf-8') as f:\n",
        "                return set(json.load(f))\n",
        "        except Exception as e:\n",
        "            logging.warning(f\"Lỗi khi tải danh sách {entity_type}: {e}\")\n",
        "\n",
        "    logging.info(f\"Không tìm thấy danh sách {entity_type} hiện có, bắt đầu với tập rỗng\")\n",
        "    return set()\n",
        "\n",
        "def save_entity_list(self, entity_list, entity_type):\n",
        "    \"\"\"Lưu danh sách thực thể đã cập nhật vào tệp\"\"\"\n",
        "    file_path = f\"extracted_data/{entity_type}_list.json\"\n",
        "    with open(file_path, 'w', encoding='utf-8') as f:\n",
        "        json.dump(list(entity_list), f, ensure_ascii=False, indent=2)\n",
        "    logging.info(f\"Đã lưu {len(entity_list)} {entity_type} vào {file_path}\")\n",
        "\n",
        "def normalize_vietnamese_text(self, text):\n",
        "    \"\"\"Chuẩn hóa văn bản tiếng Việt bằng cách xử lý dấu và chữ hoa/thường\"\"\"\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "\n",
        "    # Chuẩn hóa ký tự Unicode\n",
        "    text = unicodedata.normalize('NFC', text)\n",
        "\n",
        "    # Loại bỏ khoảng trắng thừa\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    return text\n",
        "\n",
        "def clean_text(self, text):\n",
        "    \"\"\"Làm sạch văn bản bằng cách loại bỏ ký tự đặc biệt và chuẩn hóa\"\"\"\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "\n",
        "    # Chuẩn hóa văn bản\n",
        "    text = self.normalize_vietnamese_text(text)\n",
        "    text = text.lower()\n",
        "\n",
        "    # Loại bỏ đường dẫn URL\n",
        "    text = re.sub(r'https?://\\S+|www\\.\\S+|\\S+@\\S+\\.\\S+', '', text)\n",
        "    text = re.sub(r'@\\w+|#\\w+', '', text)\n",
        "\n",
        "    # Loại bỏ biểu tượng cảm xúc và ký tự đặc biệt trong khi giữ lại chữ tiếng Việt\n",
        "    emoji_pattern = re.compile(\"[\"\n",
        "            u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "            u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "            u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "            u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "            u\"\\U00002702-\\U000027B0\"  # dingbats\n",
        "            u\"\\U000024C2-\\U0001F251\" \n",
        "            \"]+\", flags=re.UNICODE)\n",
        "    text = emoji_pattern.sub('', text)\n",
        "\n",
        "    symbols_to_remove = [\n",
        "            '!', '\"', '#', '$', '%', '&', \"'\", '*', '+', ',', \n",
        "            '-', '.', '/', ':', ';', '<', '=', '>', '?', '@', \n",
        "            '[', '\\\\', ']', '^', '_', '`', '{', '|', '}', '~',\n",
        "            '\"', '\"', ''', ''', '…', '–', '—', '•', '′', '″',\n",
        "            '„', '«', '»', '‹', '›', '⟨', '⟩', '〈', '〉'\n",
        "    ]\n",
        "    \n",
        "    # Create a pattern that excludes Vietnamese diacritics\n",
        "    pattern = f'[{\"\".join(map(re.escape, symbols_to_remove))}]'\n",
        "    text = re.sub(pattern, ' ', text)\n",
        "\n",
        "    # Handle ellipsis and multiple dots\n",
        "    text = re.sub(r'\\.{2,}', ' ', text)\n",
        "\n",
        "    # Handle multiple spaces and normalize whitespace\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "\n",
        "    # Handle parentheses and brackets\n",
        "    text = re.sub(r'[\\(\\)\\[\\]\\{\\}⟨⟩〈〉]', ' ', text)\n",
        "\n",
        "    # Clean up extra spaces around Vietnamese words\n",
        "    text = re.sub(r'\\s+([^\\w\\s])|([^\\w\\s])\\s+', r'\\1\\2', text)\n",
        "\n",
        "    # Final whitespace cleanup\n",
        "    text = text.strip()\n",
        "\n",
        "    return text\n",
        "\n",
        "def auto_correct_text(self, text):\n",
        "    \"\"\"Tự động sửa lỗi chính tả bằng bộ kiểm tra chính tả\"\"\"\n",
        "    spell = SpellChecker(language='vi')\n",
        "    words = word_tokenize(text)\n",
        "    corrected_words = [spell.correction(word) for word in words]\n",
        "    return \" \".join(corrected_words)\n",
        "\n",
        "def load_stopwords(self, file_path):\n",
        "    \"\"\"Tải danh sách từ dừng từ tệp\"\"\"\n",
        "    if file_path and os.path.exists(file_path):\n",
        "        try:\n",
        "            with open(file_path, 'r', encoding='utf-8') as f:\n",
        "                return set(f.read().splitlines())\n",
        "        except Exception as e:\n",
        "            logging.warning(f\"Lỗi khi tải danh sách từ dừng: {e}\")\n",
        "    logging.info(\"Không tìm thấy tệp từ dừng, bắt đầu với tập rỗng\")\n",
        "    return set()\n",
        "\n",
        "def remove_stopwords(self, text, stopwords):\n",
        "    \"\"\"Loại bỏ từ dừng khỏi văn bản\"\"\"\n",
        "    words = word_tokenize(text)\n",
        "    filtered_words = [word for word in words if word.lower() not in stopwords]\n",
        "    return \" \".join(filtered_words)\n",
        "\n",
        "def preprocess_text(self, text):\n",
        "    \"\"\"Áp dụng tất cả các bước tiền xử lý lên văn bản\"\"\"\n",
        "    try:\n",
        "        text = self.clean_text(text)\n",
        "        text = self.auto_correct_text(text)  # Đã sửa lỗi tại đây\n",
        "        stopwords = self.load_stopwords('vietnamese-stopwords.txt')\n",
        "        text = self.remove_stopwords(text, stopwords)\n",
        "        return text\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Lỗi khi tiền xử lý văn bản: {e}\")\n",
        "        return text if isinstance(text, str) else \"\"\n",
        "\n",
        "# Gán các phương thức vào lớp VietnameseTextProcessor\n",
        "VietnameseTextProcessor._load_entity_list = _load_entity_list\n",
        "VietnameseTextProcessor.save_entity_list = save_entity_list \n",
        "VietnameseTextProcessor.normalize_vietnamese_text = normalize_vietnamese_text\n",
        "VietnameseTextProcessor.clean_text = clean_text\n",
        "VietnameseTextProcessor.auto_correct_text = auto_correct_text\n",
        "VietnameseTextProcessor.load_stopwords = load_stopwords\n",
        "VietnameseTextProcessor.preprocess_text = preprocess_text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 179,
      "metadata": {},
      "outputs": [],
      "source": [
        "def test_text_processing():\n",
        "    \"\"\"Test function to demonstrate all text preprocessing and cleaning steps\"\"\"\n",
        "    \n",
        "    # Initialize the processor\n",
        "    processor = VietnameseTextProcessor()\n",
        "    \n",
        "    # Test text with various cases to check\n",
        "    test_text = \"\"\"\n",
        "    🔥 Quán Phở ngon ở Q.1 TPHCM! https://example.com\n",
        "    Món phở bò tái nạm gầu cực kỳ ngon, nước dùng đậm đà...\n",
        "    Địa chỉ: 123 Lê Lợi, P. Bến Nghé, Quận 1, TP.HCM\n",
        "    #pho #amthuc #reviewdoan @foodblogger\n",
        "    \"\"\"\n",
        "    \n",
        "    print(\"Original Text:\")\n",
        "    print(\"-\" * 50)\n",
        "    print(test_text)\n",
        "    print(\"\\n\")\n",
        "\n",
        "    # Test normalize_vietnamese_text\n",
        "    print(\"1. After Vietnamese Text Normalization:\")\n",
        "    print(\"-\" * 50)\n",
        "    normalized = processor.normalize_vietnamese_text(test_text)\n",
        "    print(normalized)\n",
        "    print(\"\\n\")\n",
        "\n",
        "    # Test clean_text\n",
        "    print(\"2. After Text Cleaning:\")\n",
        "    print(\"-\" * 50)\n",
        "    cleaned = processor.clean_text(test_text)\n",
        "    print(cleaned)\n",
        "    print(\"\\n\")\n",
        "\n",
        "    # Test stopwords removal\n",
        "    print(\"3. After Stopwords Removal:\")\n",
        "    print(\"-\" * 50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 180,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-03-09 23:18:39,988 - INFO - Không tìm thấy danh sách foods hiện có, bắt đầu với tập rỗng\n",
            "2025-03-09 23:18:39,989 - INFO - Không tìm thấy danh sách locations hiện có, bắt đầu với tập rỗng\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original Text:\n",
            "--------------------------------------------------\n",
            "\n",
            "    🔥 Quán Phở ngon ở Q.1 TPHCM! https://example.com\n",
            "    Món phở bò tái nạm gầu cực kỳ ngon, nước dùng đậm đà...\n",
            "    Địa chỉ: 123 Lê Lợi, P. Bến Nghé, Quận 1, TP.HCM\n",
            "    #pho #amthuc #reviewdoan @foodblogger\n",
            "    \n",
            "\n",
            "\n",
            "1. After Vietnamese Text Normalization:\n",
            "--------------------------------------------------\n",
            "🔥 Quán Phở ngon ở Q.1 TPHCM! https://example.com Món phở bò tái nạm gầu cực kỳ ngon, nước dùng đậm đà... Địa chỉ: 123 Lê Lợi, P. Bến Nghé, Quận 1, TP.HCM #pho #amthuc #reviewdoan @foodblogger\n",
            "\n",
            "\n",
            "2. After Text Cleaning:\n",
            "--------------------------------------------------\n",
            "quán phở ngon ở q 1 tphcm món phở bò tái nạm gầu cực kỳ ngon nước dùng đậm đà địa chỉ 123 lê lợi p bến nghé quận 1 tp hcm\n",
            "\n",
            "\n",
            "3. After Stopwords Removal:\n",
            "--------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "test_text_processing()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Entity Extraction Methods"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_entities_from_ner(self, text):\n",
        "    \"\"\"Trích xuất thực thể từ văn bản bằng Named Entity Recognition (NER) của underthesea.\"\"\"\n",
        "    locations = []\n",
        "\n",
        "    try:\n",
        "        ner_tags = ner(text)  # Thực hiện nhận dạng thực thể có tên (NER)\n",
        "\n",
        "        # Kiểm tra nếu kết quả từ NER có định dạng mong đợi\n",
        "        if not isinstance(ner_tags, list):\n",
        "            return locations\n",
        "\n",
        "        # Trích xuất các địa điểm từ NER\n",
        "        current_loc = []\n",
        "\n",
        "        for item in ner_tags:\n",
        "            # Xử lý các định dạng đầu ra khác nhau từ NER\n",
        "            if isinstance(item, (list, tuple)) and len(item) == 2:\n",
        "                word, tag = item\n",
        "            else:\n",
        "                continue\n",
        "\n",
        "            if tag.startswith('B-LOC'):\n",
        "                if current_loc:\n",
        "                    locations.append(' '.join(current_loc))\n",
        "                    current_loc = []\n",
        "                current_loc.append(word)\n",
        "            elif tag.startswith('I-LOC') and current_loc:\n",
        "                current_loc.append(word)\n",
        "            elif current_loc:\n",
        "                locations.append(' '.join(current_loc))\n",
        "                current_loc = []\n",
        "\n",
        "        # Thêm thực thể địa điểm cuối cùng nếu có\n",
        "        if current_loc:\n",
        "            locations.append(' '.join(current_loc))\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Lỗi khi trích xuất thực thể bằng NER: {e}\")\n",
        "        logging.error(traceback.format_exc())\n",
        "\n",
        "    return locations\n",
        "\n",
        "def extract_entities_from_patterns(self, text, sentences, pos_tags):\n",
        "    \"\"\"Trích xuất thực thể bằng cách sử dụng phương pháp dựa trên mẫu (Pattern Matching).\"\"\"\n",
        "    foods = []\n",
        "    locations = []\n",
        "    tastes = []\n",
        "\n",
        "    # Xử lý từng câu để trích xuất thực thể\n",
        "    for idx, sentence in enumerate(sentences):\n",
        "        words = word_tokenize(sentence)\n",
        "        sentence_pos_tags = pos_tags[idx] if idx < len(pos_tags) else []\n",
        "\n",
        "        # Tìm thực thể về thực phẩm\n",
        "        self._extract_food_entities(sentence, sentence_pos_tags, foods)\n",
        "\n",
        "        # Tìm thực thể về địa điểm\n",
        "        self._extract_location_entities(sentence, sentence_pos_tags, locations)\n",
        "\n",
        "        # Tìm mô tả về hương vị\n",
        "        self._extract_taste_descriptions(sentence, words, tastes)\n",
        "\n",
        "    return foods, locations, tastes\n",
        "\n",
        "def _validate_entity(self, phrase, indicators):\n",
        "    \"\"\"Validate if a phrase contains at least one indicator\"\"\"\n",
        "    phrase_lower = phrase.lower()\n",
        "    return any(indicator.lower() in phrase_lower for indicator in indicators)\n",
        "\n",
        "def _extract_food_entities(self, sentence, pos_tags, foods):\n",
        "    \"\"\"Extract food entities with improved indicator matching\"\"\"\n",
        "    # Check existing food list\n",
        "    for food in self.foods:\n",
        "        if food.lower() in sentence.lower() and self._validate_entity(food, self.food_indicators):\n",
        "            foods.append(food)\n",
        "\n",
        "    # Find food indicators\n",
        "    for idx, (word, tag) in enumerate(pos_tags):\n",
        "        if word.lower() in self.food_indicators:\n",
        "            noun_phrase = [word]\n",
        "            max_look_ahead = 4\n",
        "            \n",
        "            # Look ahead for related words\n",
        "            for i in range(1, max_look_ahead):\n",
        "                if idx + i < len(pos_tags):\n",
        "                    next_word, next_tag = pos_tags[idx + i]\n",
        "                    # Accept nouns, adjectives, and numbers for quantities\n",
        "                    if next_tag.startswith(('N', 'A', 'M')):\n",
        "                        noun_phrase.append(next_word)\n",
        "                    else:\n",
        "                        # Check if we should continue based on common food patterns\n",
        "                        if len(noun_phrase) < 2 or not self._validate_entity(\" \".join(noun_phrase), self.food_indicators):\n",
        "                            continue\n",
        "                        break\n",
        "            \n",
        "            if noun_phrase and self._validate_entity(\" \".join(noun_phrase), self.food_indicators):\n",
        "                food_name = \" \".join(noun_phrase)\n",
        "                foods.append(food_name)\n",
        "                self.foods.add(food_name)\n",
        "\n",
        "def _extract_location_entities(self, sentence, pos_tags, locations):\n",
        "    \"\"\"Extract location entities with improved indicator matching\"\"\"\n",
        "    # Check existing location list\n",
        "    for location in self.locations:\n",
        "        if location.lower() in sentence.lower() and self._validate_entity(location, self.locations_indicators):\n",
        "            locations.append(location)\n",
        "\n",
        "    # Find location indicators\n",
        "    for idx, (word, tag) in enumerate(pos_tags):\n",
        "        if any(indicator.lower() in word.lower() for indicator in self.locations_indicators):\n",
        "            noun_phrase = [word]\n",
        "            max_look_ahead = 4\n",
        "            \n",
        "            # Look ahead for related words\n",
        "            for i in range(1, max_look_ahead):\n",
        "                if idx + i < len(pos_tags):\n",
        "                    next_word, next_tag = pos_tags[idx + i]\n",
        "                    # Accept proper nouns, numbers, and regular nouns\n",
        "                    if next_tag.startswith(('N', 'M', 'Np', 'Nu')):\n",
        "                        noun_phrase.append(next_word)\n",
        "                    else:\n",
        "                        # Check if we should continue based on location patterns\n",
        "                        if len(noun_phrase) < 2 or not self._validate_entity(\" \".join(noun_phrase), self.locations_indicators):\n",
        "                            continue\n",
        "                        break\n",
        "            \n",
        "            if noun_phrase and self._validate_entity(\" \".join(noun_phrase), self.locations_indicators):\n",
        "                location_name = \" \".join(noun_phrase)\n",
        "                locations.append(location_name)\n",
        "                self.locations.add(location_name)\n",
        "\n",
        "def _extract_taste_descriptions(self, sentence, words, tastes):\n",
        "    \"\"\"Extract taste descriptions with improved matching\"\"\"\n",
        "    for taste_word in self.taste_indicators:\n",
        "        if taste_word in sentence.lower():\n",
        "            taste_idx = -1\n",
        "            for idx, word in enumerate(words):\n",
        "                if taste_word in word.lower():\n",
        "                    taste_idx = idx\n",
        "                    break\n",
        "            \n",
        "            if taste_idx >= 0:\n",
        "                # Look for a wider context\n",
        "                start = max(0, taste_idx - 2)\n",
        "                end = min(len(words), taste_idx + 3)\n",
        "                taste_phrase = \" \".join(words[start:end])\n",
        "                \n",
        "                # Validate the taste phrase\n",
        "                if 2 <= len(taste_phrase.split()) <= 4 and self._validate_entity(taste_phrase, self.taste_indicators):\n",
        "                    tastes.append(taste_phrase)\n",
        "\n",
        "def extract_entities(self, text):\n",
        "    \"\"\"Extract entities with improved validation and matching\"\"\"\n",
        "    if not text or not isinstance(text, str):\n",
        "        return {\"foods\": [], \"locations\": [], \"tastes\": []}\n",
        "\n",
        "    try:\n",
        "        results = {\"foods\": [], \"locations\": [], \"tastes\": []}\n",
        "\n",
        "        # Extract locations using NER\n",
        "        ner_locations = self.extract_entities_from_ner(text)\n",
        "        validated_locations = [loc for loc in ner_locations if self._validate_entity(loc, self.locations_indicators)]\n",
        "        results[\"locations\"].extend(validated_locations)\n",
        "        self.locations.update(validated_locations)\n",
        "\n",
        "        # Extract entities using pattern matching\n",
        "        sentences = nltk.sent_tokenize(text)\n",
        "        pos_tags = [pos_tag(sent) for sent in sentences]\n",
        "\n",
        "        foods, locations, tastes = self.extract_entities_from_patterns(text, sentences, pos_tags)\n",
        "\n",
        "        # Validate and extend results\n",
        "        results[\"foods\"].extend([f for f in foods if self._validate_entity(f, self.food_indicators)])\n",
        "        results[\"locations\"].extend([l for l in locations if self._validate_entity(l, self.locations_indicators)])\n",
        "        results[\"tastes\"].extend([t for t in tastes if self._validate_entity(t, self.taste_indicators)])\n",
        "\n",
        "        # Update entity lists\n",
        "        self.foods.update(results[\"foods\"])\n",
        "        self.locations.update(results[\"locations\"])\n",
        "\n",
        "        # Remove duplicates and empty strings\n",
        "        for key in results:\n",
        "            results[key] = list(set(filter(None, results[key])))\n",
        "\n",
        "        return results\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error extracting entities: {e}\")\n",
        "        logging.error(traceback.format_exc())\n",
        "        return {\"foods\": [], \"locations\": [], \"tastes\": []}\n",
        "\n",
        "# Gán các phương thức vào lớp VietnameseTextProcessor\n",
        "VietnameseTextProcessor.validate_entity = _validate_entity\n",
        "VietnameseTextProcessor.extract_entities_from_ner = extract_entities_from_ner\n",
        "VietnameseTextProcessor.extract_entities_from_patterns = extract_entities_from_patterns\n",
        "VietnameseTextProcessor._extract_food_entities = _extract_food_entities\n",
        "VietnameseTextProcessor._extract_location_entities = _extract_location_entities\n",
        "VietnameseTextProcessor._extract_taste_descriptions = _extract_taste_descriptions\n",
        "VietnameseTextProcessor.extract_entities = extract_entities\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# DataFrame Processing and Bootstrapping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 182,
      "metadata": {},
      "outputs": [],
      "source": [
        "def process_dataframe(self, df, text_column=\"video_transcription\", batch_size=100):\n",
        "    \"\"\"\n",
        "    Xử lý toàn bộ DataFrame và trích xuất các thực thể.\n",
        "\n",
        "    Tham số:\n",
        "        df (pd.DataFrame): DataFrame chứa dữ liệu văn bản.\n",
        "        text_column (str): Tên cột chứa văn bản.\n",
        "        batch_size (int): Kích thước batch để xử lý nhằm tiết kiệm bộ nhớ.\n",
        "\n",
        "    Trả về:\n",
        "        pd.DataFrame: DataFrame gốc với các cột chứa thực thể được trích xuất.\n",
        "    \"\"\"\n",
        "    # Kiểm tra nếu DataFrame trống hoặc không có cột văn bản\n",
        "    if df.empty or text_column not in df.columns:\n",
        "        logging.error(f\"DataFrame không hợp lệ hoặc thiếu cột '{text_column}'\")\n",
        "        return df\n",
        "\n",
        "    # Tạo thư mục lưu trữ nếu chưa tồn tại\n",
        "    os.makedirs(\"extracted_data\", exist_ok=True)\n",
        "\n",
        "    # Khởi tạo các cột để lưu thực thể trích xuất\n",
        "    df['preprocessed_text'] = \"\"\n",
        "    df['extracted_foods'] = None\n",
        "    df['extracted_locations'] = None\n",
        "    df['extracted_tastes'] = None\n",
        "\n",
        "    total_batches = (len(df) + batch_size - 1) // batch_size  # Tính số batch cần xử lý\n",
        "\n",
        "    for i in tqdm(range(total_batches), desc=\"Đang xử lý batch\"):\n",
        "        start_idx = i * batch_size\n",
        "        end_idx = min((i + 1) * batch_size, len(df))\n",
        "\n",
        "        batch = df.iloc[start_idx:end_idx].copy()\n",
        "\n",
        "        # Tiền xử lý văn bản\n",
        "        batch['preprocessed_text'] = batch[text_column].apply(self.preprocess_text)\n",
        "\n",
        "        # Trích xuất thực thể\n",
        "        entities_list = []\n",
        "        for text in batch['preprocessed_text']:\n",
        "            entities_list.append(self.extract_entities(text))\n",
        "\n",
        "        # Cập nhật DataFrame với thực thể trích xuất\n",
        "        batch['extracted_foods'] = [data['foods'] for data in entities_list]\n",
        "        batch['extracted_locations'] = [data['locations'] for data in entities_list]\n",
        "        batch['extracted_tastes'] = [data['tastes'] for data in entities_list]\n",
        "\n",
        "        # Cập nhật vào DataFrame gốc\n",
        "        df.iloc[start_idx:end_idx] = batch\n",
        "\n",
        "        # Lưu kết quả tạm thời theo từng batch\n",
        "        if (i + 1) % 5 == 0 or (i + 1) == total_batches:\n",
        "            self.save_entity_list(self.foods, \"foods\")\n",
        "            self.save_entity_list(self.locations, \"locations\")\n",
        "\n",
        "            # Lưu kết quả trung gian\n",
        "            checkpoint_file = f\"extracted_data/processed_data_batch_{i+1}.csv\"\n",
        "            df.iloc[:end_idx].to_csv(checkpoint_file, index=False)\n",
        "            logging.info(f\"Đã lưu kết quả trung gian vào {checkpoint_file} sau batch {i+1}/{total_batches}\")\n",
        "\n",
        "    # Thống kê số lượng thực thể đã tìm thấy\n",
        "    food_count = len(self.foods)\n",
        "    location_count = len(self.locations)\n",
        "\n",
        "    logging.info(f\"Trích xuất hoàn tất. Tìm thấy {food_count} thực thể món ăn và {location_count} thực thể địa điểm.\")\n",
        "\n",
        "    return df\n",
        "\n",
        "def bootstrap_entity_lists(self, df, text_column=\"preprocessed_text\", min_freq=3):\n",
        "    \"\"\"\n",
        "    Mở rộng danh sách thực thể bằng TF-IDF để tìm các thực thể tiềm năng.\n",
        "    \n",
        "    Tham số:\n",
        "        df (pd.DataFrame): DataFrame chứa dữ liệu văn bản.\n",
        "        text_column (str): Tên cột chứa văn bản đã tiền xử lý.\n",
        "        min_freq (int): Số lần xuất hiện tối thiểu để xem xét một thực thể.\n",
        "\n",
        "    Trả về:\n",
        "        set: Tập hợp các thực thể món ăn mới được nhận diện.\n",
        "    \"\"\"\n",
        "    if df.empty or text_column not in df.columns:\n",
        "        logging.error(f\"Không thể mở rộng thực thể: DataFrame không hợp lệ hoặc thiếu cột '{text_column}'\")\n",
        "        return set()\n",
        "\n",
        "    # Lọc ra các văn bản hợp lệ\n",
        "    valid_texts = df[text_column].dropna().replace('', pd.NA).dropna().tolist()\n",
        "\n",
        "    if not valid_texts:\n",
        "        logging.warning(\"Không tìm thấy văn bản hợp lệ để mở rộng thực thể\")\n",
        "        return set()\n",
        "\n",
        "    try:\n",
        "        min_df_val = max(1, min(min_freq, len(valid_texts) // 2))\n",
        "        \n",
        "        tfidf = TfidfVectorizer(\n",
        "            ngram_range=(1, 3),  # Xét các n-gram từ 1 đến 3 từ\n",
        "            min_df=min_df_val,  # Điều chỉnh min_df\n",
        "            max_df=0.9  # Loại bỏ các cụm từ quá phổ biến\n",
        "        )\n",
        "\n",
        "        tfidf_matrix = tfidf.fit_transform(valid_texts)\n",
        "        feature_names = tfidf.get_feature_names_out()\n",
        "\n",
        "        # Lấy danh sách n-gram có giá trị TF-IDF cao\n",
        "        important_ngrams = []\n",
        "        for i in range(min(tfidf_matrix.shape[0], 100)):\n",
        "            feature_index = tfidf_matrix[i,:].nonzero()[1]\n",
        "            tfidf_scores = zip(feature_index, [tfidf_matrix[i, x] for x in feature_index])\n",
        "            # Sắp xếp theo điểm TF-IDF giảm dần\n",
        "            for idx, score in sorted(tfidf_scores, key=lambda x: x[1], reverse=True)[:20]:\n",
        "                important_ngrams.append(feature_names[idx])\n",
        "\n",
        "        # Lọc các cụm từ có thể là tên món ăn (dựa vào từ chỉ món ăn)\n",
        "        potential_foods = set()\n",
        "        for text in valid_texts:\n",
        "            for indicator in self.food_indicators:\n",
        "                if indicator in text:\n",
        "                    for ngram in important_ngrams:\n",
        "                        # Kiểm tra nếu ngram xuất hiện gần từ chỉ món ăn\n",
        "                        if ngram in text and re.search(r'\\b' + re.escape(indicator) + r'.{0,30}' + re.escape(ngram), text, re.IGNORECASE):\n",
        "                            potential_foods.add(ngram)\n",
        "                        if ngram in text and re.search(r'\\b' + re.escape(ngram) + r'.{0,30}' + re.escape(indicator), text, re.IGNORECASE):\n",
        "                            potential_foods.add(ngram)\n",
        "\n",
        "        # Lọc bỏ các thực thể không hợp lệ (quá ngắn, chỉ chứa số, v.v.)\n",
        "        filtered_foods = {food for food in potential_foods if len(food) > 2 and not food.isdigit()}\n",
        "\n",
        "        # Cập nhật danh sách món ăn\n",
        "        self.foods.update(filtered_foods)\n",
        "        logging.info(f\"Đã thêm {len(filtered_foods)} thực thể món ăn tiềm năng từ mở rộng thực thể\")\n",
        "\n",
        "        return filtered_foods\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Lỗi khi mở rộng thực thể: {e}\")\n",
        "        logging.error(traceback.format_exc())\n",
        "        return set()\n",
        "\n",
        "VietnameseTextProcessor.process_dataframe = process_dataframe\n",
        "VietnameseTextProcessor.bootstrap_entity_lists = bootstrap_entity_lists"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 183,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YI6pKb0WGWVC",
        "outputId": "174f413c-1aac-4c97-9d7b-a886256d1cf2"
      },
      "outputs": [],
      "source": [
        "def main():\n",
        "    try:\n",
        "        # Tạo một thể hiện của bộ xử lý văn bản\n",
        "        processor = VietnameseTextProcessor()\n",
        "\n",
        "        # Tải tập dữ liệu\n",
        "        logging.info(\"Đang tải tập dữ liệu...\")\n",
        "        try:\n",
        "            # df = pd.read_csv(\"/content/21KHDL-TikTok-Analytics/data/interim/small_video_transcription.csv\")\n",
        "            df = pd.read_csv(\"C:/Users/nguye/OneDrive/Tài liệu/GitHub/21KHDL-TikTok-Analytics/data/interim/small_video_transcription.csv\")\n",
        "            if df.empty:\n",
        "                logging.error(\"Tập dữ liệu được tải về trống\")\n",
        "                return\n",
        "            logging.info(f\"Tập dữ liệu đã tải có {len(df)} dòng\")\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Lỗi khi tải tập dữ liệu: {e}\")\n",
        "            logging.error(traceback.format_exc())\n",
        "            return\n",
        "\n",
        "        # Xử lý một mẫu nhỏ để kiểm thử (sử dụng .head(10) để thử nghiệm, xóa bỏ để xử lý toàn bộ)\n",
        "        sample_df = df.head(10)\n",
        "\n",
        "        # Xử lý dữ liệu văn bản\n",
        "        logging.info(\"Bắt đầu xử lý văn bản và trích xuất thực thể...\")\n",
        "        processed_df = processor.process_dataframe(sample_df, text_column='video_transcription')\n",
        "\n",
        "        # Mở rộng danh sách thực thể bằng phương pháp bootstrapping\n",
        "        logging.info(\"Thực hiện bootstrapping để mở rộng danh sách thực thể...\")\n",
        "        processor.bootstrap_entity_lists(processed_df)\n",
        "\n",
        "        # Lưu kết quả cuối cùng\n",
        "        processed_df.to_csv(\"extracted_data/fully_processed_data.csv\", index=False)\n",
        "        processor.save_entity_list(processor.foods, \"foods\")\n",
        "        processor.save_entity_list(processor.locations, \"locations\")\n",
        "\n",
        "        # Lưu kết quả có cấu trúc dưới dạng JSON gồm video_id, author_id và các thực thể trích xuất\n",
        "        structured_data = []\n",
        "        for _, row in processed_df.iterrows():\n",
        "            structured_data.append({\n",
        "                'video_id': row.get('video_id', ''),\n",
        "                'author_id': row.get('author_id', ''),\n",
        "                'extracted_entities': {\n",
        "                    'foods': row.get('extracted_foods', []),\n",
        "                    'locations': row.get('extracted_locations', []),\n",
        "                    'tastes': row.get('extracted_tastes', [])\n",
        "                }\n",
        "            })\n",
        "\n",
        "        with open(\"extracted_data/structured_entities.json\", 'w', encoding='utf-8') as f:\n",
        "            json.dump(structured_data, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "        logging.info(\"Quá trình xử lý hoàn tất. Kết quả đã được lưu trong thư mục 'extracted_data'.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Lỗi nghiêm trọng trong hàm main: {e}\")\n",
        "        logging.error(traceback.format_exc())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 184,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-03-09 23:18:40,074 - INFO - Không tìm thấy danh sách foods hiện có, bắt đầu với tập rỗng\n",
            "2025-03-09 23:18:40,074 - INFO - Không tìm thấy danh sách locations hiện có, bắt đầu với tập rỗng\n",
            "2025-03-09 23:18:40,078 - INFO - Đang tải tập dữ liệu...\n",
            "2025-03-09 23:18:40,332 - INFO - Tập dữ liệu đã tải có 10673 dòng\n",
            "2025-03-09 23:18:40,333 - INFO - Bắt đầu xử lý văn bản và trích xuất thực thể...\n",
            "C:\\Users\\nguye\\AppData\\Local\\Temp\\ipykernel_13796\\74775411.py:22: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df['preprocessed_text'] = \"\"\n",
            "C:\\Users\\nguye\\AppData\\Local\\Temp\\ipykernel_13796\\74775411.py:23: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df['extracted_foods'] = None\n",
            "C:\\Users\\nguye\\AppData\\Local\\Temp\\ipykernel_13796\\74775411.py:24: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df['extracted_locations'] = None\n",
            "C:\\Users\\nguye\\AppData\\Local\\Temp\\ipykernel_13796\\74775411.py:25: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df['extracted_tastes'] = None\n",
            "Đang xử lý batch:   0%|          | 0/1 [00:00<?, ?it/s]2025-03-09 23:18:40,337 - ERROR - Lỗi khi tiền xử lý văn bản: The provided dictionary language (vi) does not exist!\n",
            "2025-03-09 23:18:40,339 - ERROR - Lỗi khi tiền xử lý văn bản: The provided dictionary language (vi) does not exist!\n",
            "2025-03-09 23:18:40,341 - ERROR - Lỗi khi tiền xử lý văn bản: The provided dictionary language (vi) does not exist!\n",
            "2025-03-09 23:18:40,344 - ERROR - Lỗi khi tiền xử lý văn bản: The provided dictionary language (vi) does not exist!\n",
            "2025-03-09 23:18:40,344 - ERROR - Lỗi khi tiền xử lý văn bản: The provided dictionary language (vi) does not exist!\n",
            "2025-03-09 23:18:40,347 - ERROR - Lỗi khi tiền xử lý văn bản: The provided dictionary language (vi) does not exist!\n",
            "2025-03-09 23:18:40,349 - ERROR - Lỗi khi tiền xử lý văn bản: The provided dictionary language (vi) does not exist!\n",
            "2025-03-09 23:18:40,351 - ERROR - Lỗi khi tiền xử lý văn bản: The provided dictionary language (vi) does not exist!\n",
            "2025-03-09 23:18:40,352 - ERROR - Lỗi khi tiền xử lý văn bản: The provided dictionary language (vi) does not exist!\n",
            "2025-03-09 23:18:40,354 - ERROR - Lỗi khi tiền xử lý văn bản: The provided dictionary language (vi) does not exist!\n",
            "2025-03-09 23:18:53,066 - INFO - Đã lưu 83 foods vào extracted_data/foods_list.json\n",
            "2025-03-09 23:18:53,067 - INFO - Đã lưu 7 locations vào extracted_data/locations_list.json\n",
            "2025-03-09 23:18:53,069 - INFO - Đã lưu kết quả trung gian vào extracted_data/processed_data_batch_1.csv sau batch 1/1\n",
            "Đang xử lý batch: 100%|██████████| 1/1 [00:12<00:00, 12.73s/it]\n",
            "2025-03-09 23:18:53,071 - INFO - Trích xuất hoàn tất. Tìm thấy 83 thực thể món ăn và 7 thực thể địa điểm.\n",
            "2025-03-09 23:18:53,072 - INFO - Thực hiện bootstrapping để mở rộng danh sách thực thể...\n",
            "2025-03-09 23:18:55,627 - INFO - Đã thêm 102 thực thể món ăn tiềm năng từ mở rộng thực thể\n",
            "2025-03-09 23:18:55,627 - INFO - Đã lưu 179 foods vào extracted_data/foods_list.json\n",
            "2025-03-09 23:18:55,627 - INFO - Đã lưu 7 locations vào extracted_data/locations_list.json\n",
            "2025-03-09 23:18:55,643 - INFO - Quá trình xử lý hoàn tất. Kết quả đã được lưu trong thư mục 'extracted_data'.\n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 185,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_food_prompts():\n",
        "    food_prompt_template = \"\"\"\n",
        "    Hãy phân tích thông tin về món ăn sau đây:\n",
        "    \n",
        "    Danh sách món ăn: {foods}\n",
        "    \n",
        "    Yêu cầu:\n",
        "    1. Phân loại các món ăn thành các nhóm (ví dụ: món nước, món nướng, đồ uống, etc.)\n",
        "    2. Xác định các món đặc trưng nhất\n",
        "    3. Đề xuất món ăn phổ biến nhất dựa trên tần suất xuất hiện\n",
        "    4. Liên kết món ăn với văn hóa ẩm thực Việt Nam\n",
        "    5. Đề xuất các kết hợp món ăn phù hợp\n",
        "    \n",
        "    Vui lòng trình bày kết quả một cách chi tiết và có cấu trúc.\n",
        "    \"\"\"\n",
        "    return food_prompt_template\n",
        "\n",
        "def create_location_prompts():\n",
        "    location_prompt_template = \"\"\"\n",
        "    Hãy phân tích thông tin về địa điểm ẩm thực sau đây:\n",
        "    \n",
        "    Danh sách địa điểm: {locations}\n",
        "    \n",
        "    Yêu cầu:\n",
        "    1. Nhóm các địa điểm theo khu vực (quận/huyện)\n",
        "    2. Xác định các khu vực ẩm thực nổi tiếng\n",
        "    3. Đề xuất tuyến đường khám phá ẩm thực\n",
        "    4. Liên kết địa điểm với đặc trưng ẩm thực\n",
        "    5. Xác định các điểm ẩm thực có mật độ cao\n",
        "    \n",
        "    Vui lòng phân tích và đưa ra các gợi ý chi tiết cho người dùng.\n",
        "    \"\"\"\n",
        "    return location_prompt_template\n",
        "\n",
        "def analyze_food_locations(structured_data):\n",
        "    \"\"\"Analyze food and location data using Gemini API\"\"\"\n",
        "    \n",
        "    import google.generativeai as genai\n",
        "    from collections import Counter\n",
        "    \n",
        "    # Configure API\n",
        "    genai.configure(api_key='AIzaSyD1WFlkEtQnFVDJCPbnitmaHQVdw2pXRK4')\n",
        "    model = genai.GenerativeModel('models/gemini-2.0-flash-thinking-exp-1219')\n",
        "    \n",
        "    # Extract unique foods and locations\n",
        "    all_foods = []\n",
        "    all_locations = []\n",
        "    \n",
        "    for item in structured_data:\n",
        "        all_foods.extend(item['extracted_entities']['foods'])\n",
        "        all_locations.extend(item['extracted_entities']['locations'])\n",
        "    \n",
        "    # Count frequencies\n",
        "    food_counts = Counter(all_foods)\n",
        "    location_counts = Counter(all_locations)\n",
        "    \n",
        "    # Create prompts\n",
        "    food_prompt = create_food_prompts().format(\n",
        "        foods=\"\\n\".join(f\"- {food} (xuất hiện {count} lần)\" \n",
        "                       for food, count in food_counts.most_common())\n",
        "    )\n",
        "    \n",
        "    location_prompt = create_location_prompts().format(\n",
        "        locations=\"\\n\".join(f\"- {loc} (xuất hiện {count} lần)\"\n",
        "                           for loc, count in location_counts.most_common())\n",
        "    )\n",
        "    \n",
        "    # Get responses from Gemini\n",
        "    food_analysis = model.generate_content(food_prompt)\n",
        "    location_analysis = model.generate_content(location_prompt)\n",
        "    \n",
        "    return {\n",
        "        'food_analysis': food_analysis.text,\n",
        "        'location_analysis': location_analysis.text,\n",
        "        'statistics': {\n",
        "            'total_unique_foods': len(set(all_foods)),\n",
        "            'total_unique_locations': len(set(all_locations)),\n",
        "            'most_common_foods': dict(food_counts.most_common(10)),\n",
        "            'most_common_locations': dict(location_counts.most_common(10))\n",
        "        }\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Phân tích món ăn và địa điểm:\n",
            "\n",
            "Top 5 món ăn phổ biến nhất:\n",
            "- gà: 9 lần\n",
            "- cá: 8 lần\n",
            "- thịt: 7 lần\n",
            "- ốc: 7 lần\n",
            "- nướng: 6 lần\n",
            "\n",
            "Top 5 địa điểm phổ biến nhất:\n",
            "- bình thạnh: 2 lần\n",
            "- thủ đức: 1 lần\n",
            "- củ chi: 1 lần\n",
            "- tân bình: 1 lần\n",
            "- phú nhuận: 1 lần\n"
          ]
        }
      ],
      "source": [
        "# Example usage\n",
        "import json\n",
        "\n",
        "# Load structured data\n",
        "with open('C:/Users/nguye/OneDrive/Tài liệu/GitHub/21KHDL-TikTok-Analytics/notebooks/extracted_data/structured_entities.json', 'r', encoding='utf-8') as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "# Run analysis\n",
        "results = analyze_food_locations(data)\n",
        "\n",
        "# Save results\n",
        "with open('food_location_analysis.json', 'w', encoding='utf-8') as f:\n",
        "    json.dump(results, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "# Print summary\n",
        "print(\"Phân tích món ăn và địa điểm:\")\n",
        "print(\"\\nTop 5 món ăn phổ biến nhất:\")\n",
        "for food, count in list(results['statistics']['most_common_foods'].items())[:10]:\n",
        "    print(f\"- {food}: {count} lần\")\n",
        "\n",
        "print(\"\\nTop 5 địa điểm phổ biến nhất:\")\n",
        "for loc, count in list(results['statistics']['most_common_locations'].items())[:10]:\n",
        "    print(f\"- {loc}: {count} lần\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "AppliedDataProject",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
