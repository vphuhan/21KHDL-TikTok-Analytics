{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 172,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U91UVADr4Phh",
        "outputId": "f4a3bd1b-2a17-4e04-dbda-2bfbe125eeda"
      },
      "outputs": [],
      "source": [
        "# %%shell\n",
        "# git clone --branch TrggTin --single-branch https://github.com/vphuhan/21KHDL-TikTok-Analytics.git\n",
        "# cd 21KHDL-TikTok-Analytics\n",
        "# git sparse-checkout init --cone\n",
        "# git sparse-checkout set data/interim\n",
        "# git checkout"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 173,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0NZ7_Zrw5PFe",
        "outputId": "d4ed584f-a229-4763-aa43-af3871f0893e"
      },
      "outputs": [],
      "source": [
        "# pip install pandas nltk underthesea scikit-learn tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Imports and Initialization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 174,
      "metadata": {
        "id": "7GYwq-Tf4PTa"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import unicodedata\n",
        "import nltk\n",
        "from underthesea import word_tokenize, pos_tag, ner\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from difflib import get_close_matches\n",
        "import logging\n",
        "import json\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import string\n",
        "import regex as re\n",
        "import traceback\n",
        "import jdc  \n",
        "from spellchecker import SpellChecker\n",
        "from datetime import datetime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 175,
      "metadata": {
        "id": "pfZUR8vY4POT"
      },
      "outputs": [],
      "source": [
        "# Configure logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
        "    handlers=[\n",
        "        logging.FileHandler(\"extraction_log.log\"),\n",
        "        logging.StreamHandler()\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 176,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aDuHFYPU6NKm",
        "outputId": "2a87ddda-2e79-4d97-ed72-1a24c515c520"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt_tab to\n",
            "[nltk_data]     C:\\Users\\nguye\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 176,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# VietnameseTextProcessor Class Definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class VietnameseTextProcessor:\n",
        "    def __init__(self, food_list_path=None, location_list_path=None):\n",
        "        \"\"\"\n",
        "        Kh·ªüi t·∫°o B·ªô x·ª≠ l√Ω vƒÉn b·∫£n ti·∫øng Vi·ªát\n",
        "\n",
        "        Tham s·ªë:\n",
        "            food_list_path (str): ƒê∆∞·ªùng d·∫´n ƒë·∫øn t·ªáp JSON ch·ª©a danh s√°ch m√≥n ƒÉn Vi·ªát Nam\n",
        "            location_list_path (str): ƒê∆∞·ªùng d·∫´n ƒë·∫øn t·ªáp JSON ch·ª©a danh s√°ch ƒë·ªãa ƒëi·ªÉm ·ªü Vi·ªát Nam\n",
        "        \"\"\"\n",
        "        # T·∫£i ho·∫∑c kh·ªüi t·∫°o danh s√°ch m√≥n ƒÉn v√† ƒë·ªãa ƒëi·ªÉm\n",
        "        self.foods = self._load_entity_list(food_list_path, \"foods\")\n",
        "        self.locations = self._load_entity_list(location_list_path, \"locations\")\n",
        "\n",
        "        # C√°c t·ª´ kh√≥a ph·ªï bi·∫øn li√™n quan ƒë·∫øn m√≥n ƒÉn v√† h∆∞∆°ng v·ªã trong ti·∫øng Vi·ªát ƒë·ªÉ h·ªó tr·ª£ nh·∫≠n di·ªán\n",
        "        self.food_indicators = [\n",
        "            \"b√°nh m√¨\", \"ph·ªü\", \"b√∫n\", \"x√®o\", \"c∆°m\", \"g·ªèi\", \"ch·∫£\", \"x√¥i\", \"cao l·∫ßu\", \"ch√°o\",\n",
        "            \"m√¨ g√≥i\", \"h·ªß ti·∫øu\", \"nem\", \"ch·∫£ ram\", \"b√°nh kh·ªçt\",\n",
        "            \"l·∫©u\", \"c√°\", \"th·ªãt\", \"canh\", \"rau\", \"ƒë·∫≠u\", \"·ªëc\", \"s√∫p\", \"b·∫Øp\", \"l∆∞∆°n\", \"mƒÉng\", \"n·∫•m\",\n",
        "            \"chu·ªëi\", \"n·ªôm\", \"tr√†\", \"c√† ph√™\", \"sinh t·ªë\", \"kem\", \"t√†u h·ªß\", \"ch√®\", \"yaourt\", \"n∆∞·ªõc m√≠a\",\n",
        "            \"s·ªØa\", \"k·∫πo\", \"ƒëa\", \"nem chua\", \"g√†\", \"b√≤\", \"heo\", \"v·ªãt\", \"c√°\", \"t√¥m\", \"m·ª±c, ·ªëc\", \"s√≤\", \"h√†u\",\n",
        "            \"b√∫n ri√™u\", \"b√∫n b√≤\", \"b√∫n m·∫Øm\", \"b√∫n m·ªçc\", \"b√∫n ch·∫£\", \"b√∫n ƒë·∫≠u\", \"b√∫n ·ªëc\"\n",
        "        ]\n",
        "\n",
        "        self.taste_indicators = [\n",
        "            \"ngon\", \"ng·ªçt\", \"chua\", \"cay\", \"ƒë·∫Øng\", \"m·∫∑n\", \"b√πi\", \"b√©o\", \"gi√≤n\", \"m·ªÅm\",\n",
        "            \"th∆°m\", \"n·ªìng\", \"ƒë·∫≠m ƒë√†\", \"nh·∫°t\", \"thanh\", \"t∆∞∆°i\", \"ch√°t\", \"cay n·ªìng\", \"cay nh·∫π\", \"cay v·ª´a\",\n",
        "            \"s·∫ßn s·∫≠t\", \"m·ªçng n∆∞·ªõc\", \"ƒë·∫Øng ngh√©t\", \"ch√°t\", \"cay x√®\", \"t√™\", \"m·∫∑n ch√°t\", \"ng·ªçt l·ªãm\", \"b√©o ng·∫≠y\", \"th∆°m l·ª´ng\",\n",
        "            \"n·ªìng n√†n\", \"ƒë·∫≠m v·ªã\", \"nh·∫°t nh·∫Ωo\", \"thanh m√°t\", \"t∆∞∆°i\", \"ƒë·∫≠m ƒë√† h∆∞∆°ng v·ªã\", \"v·ª´a ƒÉn\", \"h·ª£p kh·∫©u v·ªã\"\n",
        "        ]\n",
        "\n",
        "        self.locations_indicators = [ \n",
        "            \"qu·∫≠n 1\", \"qu·∫≠n 2\", \"qu·∫≠n 3\", \"qu·∫≠n 4\", \"qu·∫≠n 5\", \"qu·∫≠n 6\", \"qu·∫≠n 7\", \"qu·∫≠n 8\", \"qu·∫≠n 9\", \"qu·∫≠n 10\",\n",
        "            \"qu·∫≠n 11\", \"qu·∫≠n 12\", \"b√¨nh th·∫°nh\", \"t√¢n b√¨nh\", \"t√¢n ph√∫\", \"ph√∫ nhu·∫≠n\", \"g√≤ v·∫•p\", \"b√¨nh t√¢n\", \"th·ªß ƒë·ª©c\", \"h√≥c m√¥n\",\n",
        "            \"c·ªß chi\", \"nh√† b√®\", \"c·∫ßn gi·ªù\", \"b√¨nh ch√°nh\", \"tp th·ªß ƒë·ª©c\",\n",
        "            \"h√† n·ªôi\", \"h·ªì ch√≠ minh\", \"ƒë√† n·∫µng\", \"h·∫£i ph√≤ng\", \"c·∫ßn th∆°\", \"hu·∫ø\", \"nha trang\", \"v≈©ng t√†u\", \"ƒë√† l·∫°t\",\n",
        "            \"h·∫° long\", \"m·ªπ tho\", \"long xuy√™n\", \"r·∫°ch gi√°\", \"c√† mau\", \"bi√™n h√≤a\", \"bu√¥n ma thu·ªôt\", \"th√°i nguy√™n\", \"nam ƒë·ªãnh\"\n",
        "        ]\n",
        "\n",
        "\n",
        "        # T·∫£i c√°c t√†i nguy√™n c·ªßa NLTK n·∫øu c·∫ßn\n",
        "        try:\n",
        "            nltk.data.find('tokenizers/punkt')\n",
        "        except LookupError:\n",
        "            nltk.download('punkt')\n",
        "\n",
        "        # T·∫°o th∆∞ m·ª•c ƒë·ªÉ l∆∞u tr·ªØ c√°c t·ªáp d·ªØ li·ªáu ƒë∆∞·ª£c tr√≠ch xu·∫•t\n",
        "        os.makedirs(\"extracted_data\", exist_ok=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Helper Methods"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 178,
      "metadata": {},
      "outputs": [],
      "source": [
        "def _load_entity_list(self, file_path, entity_type):\n",
        "    \"\"\"T·∫£i danh s√°ch th·ª±c th·ªÉ t·ª´ t·ªáp ho·∫∑c tr·∫£ v·ªÅ t·∫≠p r·ªóng m·∫∑c ƒë·ªãnh\"\"\"\n",
        "    if file_path and os.path.exists(file_path):\n",
        "        try:\n",
        "            with open(file_path, 'r', encoding='utf-8') as f:\n",
        "                return set(json.load(f))\n",
        "        except Exception as e:\n",
        "            logging.warning(f\"L·ªói khi t·∫£i danh s√°ch {entity_type}: {e}\")\n",
        "\n",
        "    logging.info(f\"Kh√¥ng t√¨m th·∫•y danh s√°ch {entity_type} hi·ªán c√≥, b·∫Øt ƒë·∫ßu v·ªõi t·∫≠p r·ªóng\")\n",
        "    return set()\n",
        "\n",
        "def save_entity_list(self, entity_list, entity_type):\n",
        "    \"\"\"L∆∞u danh s√°ch th·ª±c th·ªÉ ƒë√£ c·∫≠p nh·∫≠t v√†o t·ªáp\"\"\"\n",
        "    file_path = f\"extracted_data/{entity_type}_list.json\"\n",
        "    with open(file_path, 'w', encoding='utf-8') as f:\n",
        "        json.dump(list(entity_list), f, ensure_ascii=False, indent=2)\n",
        "    logging.info(f\"ƒê√£ l∆∞u {len(entity_list)} {entity_type} v√†o {file_path}\")\n",
        "\n",
        "def normalize_vietnamese_text(self, text):\n",
        "    \"\"\"Chu·∫©n h√≥a vƒÉn b·∫£n ti·∫øng Vi·ªát b·∫±ng c√°ch x·ª≠ l√Ω d·∫•u v√† ch·ªØ hoa/th∆∞·ªùng\"\"\"\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "\n",
        "    # Chu·∫©n h√≥a k√Ω t·ª± Unicode\n",
        "    text = unicodedata.normalize('NFC', text)\n",
        "\n",
        "    # Lo·∫°i b·ªè kho·∫£ng tr·∫Øng th·ª´a\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    return text\n",
        "\n",
        "def clean_text(self, text):\n",
        "    \"\"\"L√†m s·∫°ch vƒÉn b·∫£n b·∫±ng c√°ch lo·∫°i b·ªè k√Ω t·ª± ƒë·∫∑c bi·ªát v√† chu·∫©n h√≥a\"\"\"\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "\n",
        "    # Chu·∫©n h√≥a vƒÉn b·∫£n\n",
        "    text = self.normalize_vietnamese_text(text)\n",
        "    text = text.lower()\n",
        "\n",
        "    # Lo·∫°i b·ªè ƒë∆∞·ªùng d·∫´n URL\n",
        "    text = re.sub(r'https?://\\S+|www\\.\\S+|\\S+@\\S+\\.\\S+', '', text)\n",
        "    text = re.sub(r'@\\w+|#\\w+', '', text)\n",
        "\n",
        "    # Lo·∫°i b·ªè bi·ªÉu t∆∞·ª£ng c·∫£m x√∫c v√† k√Ω t·ª± ƒë·∫∑c bi·ªát trong khi gi·ªØ l·∫°i ch·ªØ ti·∫øng Vi·ªát\n",
        "    emoji_pattern = re.compile(\"[\"\n",
        "            u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "            u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "            u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "            u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "            u\"\\U00002702-\\U000027B0\"  # dingbats\n",
        "            u\"\\U000024C2-\\U0001F251\" \n",
        "            \"]+\", flags=re.UNICODE)\n",
        "    text = emoji_pattern.sub('', text)\n",
        "\n",
        "    symbols_to_remove = [\n",
        "            '!', '\"', '#', '$', '%', '&', \"'\", '*', '+', ',', \n",
        "            '-', '.', '/', ':', ';', '<', '=', '>', '?', '@', \n",
        "            '[', '\\\\', ']', '^', '_', '`', '{', '|', '}', '~',\n",
        "            '\"', '\"', ''', ''', '‚Ä¶', '‚Äì', '‚Äî', '‚Ä¢', '‚Ä≤', '‚Ä≥',\n",
        "            '‚Äû', '¬´', '¬ª', '‚Äπ', '‚Ä∫', '‚ü®', '‚ü©', '„Äà', '„Äâ'\n",
        "    ]\n",
        "    \n",
        "    # Create a pattern that excludes Vietnamese diacritics\n",
        "    pattern = f'[{\"\".join(map(re.escape, symbols_to_remove))}]'\n",
        "    text = re.sub(pattern, ' ', text)\n",
        "\n",
        "    # Handle ellipsis and multiple dots\n",
        "    text = re.sub(r'\\.{2,}', ' ', text)\n",
        "\n",
        "    # Handle multiple spaces and normalize whitespace\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "\n",
        "    # Handle parentheses and brackets\n",
        "    text = re.sub(r'[\\(\\)\\[\\]\\{\\}‚ü®‚ü©„Äà„Äâ]', ' ', text)\n",
        "\n",
        "    # Clean up extra spaces around Vietnamese words\n",
        "    text = re.sub(r'\\s+([^\\w\\s])|([^\\w\\s])\\s+', r'\\1\\2', text)\n",
        "\n",
        "    # Final whitespace cleanup\n",
        "    text = text.strip()\n",
        "\n",
        "    return text\n",
        "\n",
        "def auto_correct_text(self, text):\n",
        "    \"\"\"T·ª± ƒë·ªông s·ª≠a l·ªói ch√≠nh t·∫£ b·∫±ng b·ªô ki·ªÉm tra ch√≠nh t·∫£\"\"\"\n",
        "    spell = SpellChecker(language='vi')\n",
        "    words = word_tokenize(text)\n",
        "    corrected_words = [spell.correction(word) for word in words]\n",
        "    return \" \".join(corrected_words)\n",
        "\n",
        "def load_stopwords(self, file_path):\n",
        "    \"\"\"T·∫£i danh s√°ch t·ª´ d·ª´ng t·ª´ t·ªáp\"\"\"\n",
        "    if file_path and os.path.exists(file_path):\n",
        "        try:\n",
        "            with open(file_path, 'r', encoding='utf-8') as f:\n",
        "                return set(f.read().splitlines())\n",
        "        except Exception as e:\n",
        "            logging.warning(f\"L·ªói khi t·∫£i danh s√°ch t·ª´ d·ª´ng: {e}\")\n",
        "    logging.info(\"Kh√¥ng t√¨m th·∫•y t·ªáp t·ª´ d·ª´ng, b·∫Øt ƒë·∫ßu v·ªõi t·∫≠p r·ªóng\")\n",
        "    return set()\n",
        "\n",
        "def remove_stopwords(self, text, stopwords):\n",
        "    \"\"\"Lo·∫°i b·ªè t·ª´ d·ª´ng kh·ªèi vƒÉn b·∫£n\"\"\"\n",
        "    words = word_tokenize(text)\n",
        "    filtered_words = [word for word in words if word.lower() not in stopwords]\n",
        "    return \" \".join(filtered_words)\n",
        "\n",
        "def preprocess_text(self, text):\n",
        "    \"\"\"√Åp d·ª•ng t·∫•t c·∫£ c√°c b∆∞·ªõc ti·ªÅn x·ª≠ l√Ω l√™n vƒÉn b·∫£n\"\"\"\n",
        "    try:\n",
        "        text = self.clean_text(text)\n",
        "        text = self.auto_correct_text(text)  # ƒê√£ s·ª≠a l·ªói t·∫°i ƒë√¢y\n",
        "        stopwords = self.load_stopwords('vietnamese-stopwords.txt')\n",
        "        text = self.remove_stopwords(text, stopwords)\n",
        "        return text\n",
        "    except Exception as e:\n",
        "        logging.error(f\"L·ªói khi ti·ªÅn x·ª≠ l√Ω vƒÉn b·∫£n: {e}\")\n",
        "        return text if isinstance(text, str) else \"\"\n",
        "\n",
        "# G√°n c√°c ph∆∞∆°ng th·ª©c v√†o l·ªõp VietnameseTextProcessor\n",
        "VietnameseTextProcessor._load_entity_list = _load_entity_list\n",
        "VietnameseTextProcessor.save_entity_list = save_entity_list \n",
        "VietnameseTextProcessor.normalize_vietnamese_text = normalize_vietnamese_text\n",
        "VietnameseTextProcessor.clean_text = clean_text\n",
        "VietnameseTextProcessor.auto_correct_text = auto_correct_text\n",
        "VietnameseTextProcessor.load_stopwords = load_stopwords\n",
        "VietnameseTextProcessor.preprocess_text = preprocess_text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 179,
      "metadata": {},
      "outputs": [],
      "source": [
        "def test_text_processing():\n",
        "    \"\"\"Test function to demonstrate all text preprocessing and cleaning steps\"\"\"\n",
        "    \n",
        "    # Initialize the processor\n",
        "    processor = VietnameseTextProcessor()\n",
        "    \n",
        "    # Test text with various cases to check\n",
        "    test_text = \"\"\"\n",
        "    üî• Qu√°n Ph·ªü ngon ·ªü Q.1 TPHCM! https://example.com\n",
        "    M√≥n ph·ªü b√≤ t√°i n·∫°m g·∫ßu c·ª±c k·ª≥ ngon, n∆∞·ªõc d√πng ƒë·∫≠m ƒë√†...\n",
        "    ƒê·ªãa ch·ªâ: 123 L√™ L·ª£i, P. B·∫øn Ngh√©, Qu·∫≠n 1, TP.HCM\n",
        "    #pho #amthuc #reviewdoan @foodblogger\n",
        "    \"\"\"\n",
        "    \n",
        "    print(\"Original Text:\")\n",
        "    print(\"-\" * 50)\n",
        "    print(test_text)\n",
        "    print(\"\\n\")\n",
        "\n",
        "    # Test normalize_vietnamese_text\n",
        "    print(\"1. After Vietnamese Text Normalization:\")\n",
        "    print(\"-\" * 50)\n",
        "    normalized = processor.normalize_vietnamese_text(test_text)\n",
        "    print(normalized)\n",
        "    print(\"\\n\")\n",
        "\n",
        "    # Test clean_text\n",
        "    print(\"2. After Text Cleaning:\")\n",
        "    print(\"-\" * 50)\n",
        "    cleaned = processor.clean_text(test_text)\n",
        "    print(cleaned)\n",
        "    print(\"\\n\")\n",
        "\n",
        "    # Test stopwords removal\n",
        "    print(\"3. After Stopwords Removal:\")\n",
        "    print(\"-\" * 50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 180,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-03-09 23:18:39,988 - INFO - Kh√¥ng t√¨m th·∫•y danh s√°ch foods hi·ªán c√≥, b·∫Øt ƒë·∫ßu v·ªõi t·∫≠p r·ªóng\n",
            "2025-03-09 23:18:39,989 - INFO - Kh√¥ng t√¨m th·∫•y danh s√°ch locations hi·ªán c√≥, b·∫Øt ƒë·∫ßu v·ªõi t·∫≠p r·ªóng\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original Text:\n",
            "--------------------------------------------------\n",
            "\n",
            "    üî• Qu√°n Ph·ªü ngon ·ªü Q.1 TPHCM! https://example.com\n",
            "    M√≥n ph·ªü b√≤ t√°i n·∫°m g·∫ßu c·ª±c k·ª≥ ngon, n∆∞·ªõc d√πng ƒë·∫≠m ƒë√†...\n",
            "    ƒê·ªãa ch·ªâ: 123 L√™ L·ª£i, P. B·∫øn Ngh√©, Qu·∫≠n 1, TP.HCM\n",
            "    #pho #amthuc #reviewdoan @foodblogger\n",
            "    \n",
            "\n",
            "\n",
            "1. After Vietnamese Text Normalization:\n",
            "--------------------------------------------------\n",
            "üî• Qu√°n Ph·ªü ngon ·ªü Q.1 TPHCM! https://example.com M√≥n ph·ªü b√≤ t√°i n·∫°m g·∫ßu c·ª±c k·ª≥ ngon, n∆∞·ªõc d√πng ƒë·∫≠m ƒë√†... ƒê·ªãa ch·ªâ: 123 L√™ L·ª£i, P. B·∫øn Ngh√©, Qu·∫≠n 1, TP.HCM #pho #amthuc #reviewdoan @foodblogger\n",
            "\n",
            "\n",
            "2. After Text Cleaning:\n",
            "--------------------------------------------------\n",
            "qu√°n ph·ªü ngon ·ªü q 1 tphcm m√≥n ph·ªü b√≤ t√°i n·∫°m g·∫ßu c·ª±c k·ª≥ ngon n∆∞·ªõc d√πng ƒë·∫≠m ƒë√† ƒë·ªãa ch·ªâ 123 l√™ l·ª£i p b·∫øn ngh√© qu·∫≠n 1 tp hcm\n",
            "\n",
            "\n",
            "3. After Stopwords Removal:\n",
            "--------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "test_text_processing()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Entity Extraction Methods"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_entities_from_ner(self, text):\n",
        "    \"\"\"Tr√≠ch xu·∫•t th·ª±c th·ªÉ t·ª´ vƒÉn b·∫£n b·∫±ng Named Entity Recognition (NER) c·ªßa underthesea.\"\"\"\n",
        "    locations = []\n",
        "\n",
        "    try:\n",
        "        ner_tags = ner(text)  # Th·ª±c hi·ªán nh·∫≠n d·∫°ng th·ª±c th·ªÉ c√≥ t√™n (NER)\n",
        "\n",
        "        # Ki·ªÉm tra n·∫øu k·∫øt qu·∫£ t·ª´ NER c√≥ ƒë·ªãnh d·∫°ng mong ƒë·ª£i\n",
        "        if not isinstance(ner_tags, list):\n",
        "            return locations\n",
        "\n",
        "        # Tr√≠ch xu·∫•t c√°c ƒë·ªãa ƒëi·ªÉm t·ª´ NER\n",
        "        current_loc = []\n",
        "\n",
        "        for item in ner_tags:\n",
        "            # X·ª≠ l√Ω c√°c ƒë·ªãnh d·∫°ng ƒë·∫ßu ra kh√°c nhau t·ª´ NER\n",
        "            if isinstance(item, (list, tuple)) and len(item) == 2:\n",
        "                word, tag = item\n",
        "            else:\n",
        "                continue\n",
        "\n",
        "            if tag.startswith('B-LOC'):\n",
        "                if current_loc:\n",
        "                    locations.append(' '.join(current_loc))\n",
        "                    current_loc = []\n",
        "                current_loc.append(word)\n",
        "            elif tag.startswith('I-LOC') and current_loc:\n",
        "                current_loc.append(word)\n",
        "            elif current_loc:\n",
        "                locations.append(' '.join(current_loc))\n",
        "                current_loc = []\n",
        "\n",
        "        # Th√™m th·ª±c th·ªÉ ƒë·ªãa ƒëi·ªÉm cu·ªëi c√πng n·∫øu c√≥\n",
        "        if current_loc:\n",
        "            locations.append(' '.join(current_loc))\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"L·ªói khi tr√≠ch xu·∫•t th·ª±c th·ªÉ b·∫±ng NER: {e}\")\n",
        "        logging.error(traceback.format_exc())\n",
        "\n",
        "    return locations\n",
        "\n",
        "def extract_entities_from_patterns(self, text, sentences, pos_tags):\n",
        "    \"\"\"Tr√≠ch xu·∫•t th·ª±c th·ªÉ b·∫±ng c√°ch s·ª≠ d·ª•ng ph∆∞∆°ng ph√°p d·ª±a tr√™n m·∫´u (Pattern Matching).\"\"\"\n",
        "    foods = []\n",
        "    locations = []\n",
        "    tastes = []\n",
        "\n",
        "    # X·ª≠ l√Ω t·ª´ng c√¢u ƒë·ªÉ tr√≠ch xu·∫•t th·ª±c th·ªÉ\n",
        "    for idx, sentence in enumerate(sentences):\n",
        "        words = word_tokenize(sentence)\n",
        "        sentence_pos_tags = pos_tags[idx] if idx < len(pos_tags) else []\n",
        "\n",
        "        # T√¨m th·ª±c th·ªÉ v·ªÅ th·ª±c ph·∫©m\n",
        "        self._extract_food_entities(sentence, sentence_pos_tags, foods)\n",
        "\n",
        "        # T√¨m th·ª±c th·ªÉ v·ªÅ ƒë·ªãa ƒëi·ªÉm\n",
        "        self._extract_location_entities(sentence, sentence_pos_tags, locations)\n",
        "\n",
        "        # T√¨m m√¥ t·∫£ v·ªÅ h∆∞∆°ng v·ªã\n",
        "        self._extract_taste_descriptions(sentence, words, tastes)\n",
        "\n",
        "    return foods, locations, tastes\n",
        "\n",
        "def _validate_entity(self, phrase, indicators):\n",
        "    \"\"\"Validate if a phrase contains at least one indicator\"\"\"\n",
        "    phrase_lower = phrase.lower()\n",
        "    return any(indicator.lower() in phrase_lower for indicator in indicators)\n",
        "\n",
        "def _extract_food_entities(self, sentence, pos_tags, foods):\n",
        "    \"\"\"Extract food entities with improved indicator matching\"\"\"\n",
        "    # Check existing food list\n",
        "    for food in self.foods:\n",
        "        if food.lower() in sentence.lower() and self._validate_entity(food, self.food_indicators):\n",
        "            foods.append(food)\n",
        "\n",
        "    # Find food indicators\n",
        "    for idx, (word, tag) in enumerate(pos_tags):\n",
        "        if word.lower() in self.food_indicators:\n",
        "            noun_phrase = [word]\n",
        "            max_look_ahead = 4\n",
        "            \n",
        "            # Look ahead for related words\n",
        "            for i in range(1, max_look_ahead):\n",
        "                if idx + i < len(pos_tags):\n",
        "                    next_word, next_tag = pos_tags[idx + i]\n",
        "                    # Accept nouns, adjectives, and numbers for quantities\n",
        "                    if next_tag.startswith(('N', 'A', 'M')):\n",
        "                        noun_phrase.append(next_word)\n",
        "                    else:\n",
        "                        # Check if we should continue based on common food patterns\n",
        "                        if len(noun_phrase) < 2 or not self._validate_entity(\" \".join(noun_phrase), self.food_indicators):\n",
        "                            continue\n",
        "                        break\n",
        "            \n",
        "            if noun_phrase and self._validate_entity(\" \".join(noun_phrase), self.food_indicators):\n",
        "                food_name = \" \".join(noun_phrase)\n",
        "                foods.append(food_name)\n",
        "                self.foods.add(food_name)\n",
        "\n",
        "def _extract_location_entities(self, sentence, pos_tags, locations):\n",
        "    \"\"\"Extract location entities with improved indicator matching\"\"\"\n",
        "    # Check existing location list\n",
        "    for location in self.locations:\n",
        "        if location.lower() in sentence.lower() and self._validate_entity(location, self.locations_indicators):\n",
        "            locations.append(location)\n",
        "\n",
        "    # Find location indicators\n",
        "    for idx, (word, tag) in enumerate(pos_tags):\n",
        "        if any(indicator.lower() in word.lower() for indicator in self.locations_indicators):\n",
        "            noun_phrase = [word]\n",
        "            max_look_ahead = 4\n",
        "            \n",
        "            # Look ahead for related words\n",
        "            for i in range(1, max_look_ahead):\n",
        "                if idx + i < len(pos_tags):\n",
        "                    next_word, next_tag = pos_tags[idx + i]\n",
        "                    # Accept proper nouns, numbers, and regular nouns\n",
        "                    if next_tag.startswith(('N', 'M', 'Np', 'Nu')):\n",
        "                        noun_phrase.append(next_word)\n",
        "                    else:\n",
        "                        # Check if we should continue based on location patterns\n",
        "                        if len(noun_phrase) < 2 or not self._validate_entity(\" \".join(noun_phrase), self.locations_indicators):\n",
        "                            continue\n",
        "                        break\n",
        "            \n",
        "            if noun_phrase and self._validate_entity(\" \".join(noun_phrase), self.locations_indicators):\n",
        "                location_name = \" \".join(noun_phrase)\n",
        "                locations.append(location_name)\n",
        "                self.locations.add(location_name)\n",
        "\n",
        "def _extract_taste_descriptions(self, sentence, words, tastes):\n",
        "    \"\"\"Extract taste descriptions with improved matching\"\"\"\n",
        "    for taste_word in self.taste_indicators:\n",
        "        if taste_word in sentence.lower():\n",
        "            taste_idx = -1\n",
        "            for idx, word in enumerate(words):\n",
        "                if taste_word in word.lower():\n",
        "                    taste_idx = idx\n",
        "                    break\n",
        "            \n",
        "            if taste_idx >= 0:\n",
        "                # Look for a wider context\n",
        "                start = max(0, taste_idx - 2)\n",
        "                end = min(len(words), taste_idx + 3)\n",
        "                taste_phrase = \" \".join(words[start:end])\n",
        "                \n",
        "                # Validate the taste phrase\n",
        "                if 2 <= len(taste_phrase.split()) <= 4 and self._validate_entity(taste_phrase, self.taste_indicators):\n",
        "                    tastes.append(taste_phrase)\n",
        "\n",
        "def extract_entities(self, text):\n",
        "    \"\"\"Extract entities with improved validation and matching\"\"\"\n",
        "    if not text or not isinstance(text, str):\n",
        "        return {\"foods\": [], \"locations\": [], \"tastes\": []}\n",
        "\n",
        "    try:\n",
        "        results = {\"foods\": [], \"locations\": [], \"tastes\": []}\n",
        "\n",
        "        # Extract locations using NER\n",
        "        ner_locations = self.extract_entities_from_ner(text)\n",
        "        validated_locations = [loc for loc in ner_locations if self._validate_entity(loc, self.locations_indicators)]\n",
        "        results[\"locations\"].extend(validated_locations)\n",
        "        self.locations.update(validated_locations)\n",
        "\n",
        "        # Extract entities using pattern matching\n",
        "        sentences = nltk.sent_tokenize(text)\n",
        "        pos_tags = [pos_tag(sent) for sent in sentences]\n",
        "\n",
        "        foods, locations, tastes = self.extract_entities_from_patterns(text, sentences, pos_tags)\n",
        "\n",
        "        # Validate and extend results\n",
        "        results[\"foods\"].extend([f for f in foods if self._validate_entity(f, self.food_indicators)])\n",
        "        results[\"locations\"].extend([l for l in locations if self._validate_entity(l, self.locations_indicators)])\n",
        "        results[\"tastes\"].extend([t for t in tastes if self._validate_entity(t, self.taste_indicators)])\n",
        "\n",
        "        # Update entity lists\n",
        "        self.foods.update(results[\"foods\"])\n",
        "        self.locations.update(results[\"locations\"])\n",
        "\n",
        "        # Remove duplicates and empty strings\n",
        "        for key in results:\n",
        "            results[key] = list(set(filter(None, results[key])))\n",
        "\n",
        "        return results\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error extracting entities: {e}\")\n",
        "        logging.error(traceback.format_exc())\n",
        "        return {\"foods\": [], \"locations\": [], \"tastes\": []}\n",
        "\n",
        "# G√°n c√°c ph∆∞∆°ng th·ª©c v√†o l·ªõp VietnameseTextProcessor\n",
        "VietnameseTextProcessor.validate_entity = _validate_entity\n",
        "VietnameseTextProcessor.extract_entities_from_ner = extract_entities_from_ner\n",
        "VietnameseTextProcessor.extract_entities_from_patterns = extract_entities_from_patterns\n",
        "VietnameseTextProcessor._extract_food_entities = _extract_food_entities\n",
        "VietnameseTextProcessor._extract_location_entities = _extract_location_entities\n",
        "VietnameseTextProcessor._extract_taste_descriptions = _extract_taste_descriptions\n",
        "VietnameseTextProcessor.extract_entities = extract_entities\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# DataFrame Processing and Bootstrapping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 182,
      "metadata": {},
      "outputs": [],
      "source": [
        "def process_dataframe(self, df, text_column=\"video_transcription\", batch_size=100):\n",
        "    \"\"\"\n",
        "    X·ª≠ l√Ω to√†n b·ªô DataFrame v√† tr√≠ch xu·∫•t c√°c th·ª±c th·ªÉ.\n",
        "\n",
        "    Tham s·ªë:\n",
        "        df (pd.DataFrame): DataFrame ch·ª©a d·ªØ li·ªáu vƒÉn b·∫£n.\n",
        "        text_column (str): T√™n c·ªôt ch·ª©a vƒÉn b·∫£n.\n",
        "        batch_size (int): K√≠ch th∆∞·ªõc batch ƒë·ªÉ x·ª≠ l√Ω nh·∫±m ti·∫øt ki·ªám b·ªô nh·ªõ.\n",
        "\n",
        "    Tr·∫£ v·ªÅ:\n",
        "        pd.DataFrame: DataFrame g·ªëc v·ªõi c√°c c·ªôt ch·ª©a th·ª±c th·ªÉ ƒë∆∞·ª£c tr√≠ch xu·∫•t.\n",
        "    \"\"\"\n",
        "    # Ki·ªÉm tra n·∫øu DataFrame tr·ªëng ho·∫∑c kh√¥ng c√≥ c·ªôt vƒÉn b·∫£n\n",
        "    if df.empty or text_column not in df.columns:\n",
        "        logging.error(f\"DataFrame kh√¥ng h·ª£p l·ªá ho·∫∑c thi·∫øu c·ªôt '{text_column}'\")\n",
        "        return df\n",
        "\n",
        "    # T·∫°o th∆∞ m·ª•c l∆∞u tr·ªØ n·∫øu ch∆∞a t·ªìn t·∫°i\n",
        "    os.makedirs(\"extracted_data\", exist_ok=True)\n",
        "\n",
        "    # Kh·ªüi t·∫°o c√°c c·ªôt ƒë·ªÉ l∆∞u th·ª±c th·ªÉ tr√≠ch xu·∫•t\n",
        "    df['preprocessed_text'] = \"\"\n",
        "    df['extracted_foods'] = None\n",
        "    df['extracted_locations'] = None\n",
        "    df['extracted_tastes'] = None\n",
        "\n",
        "    total_batches = (len(df) + batch_size - 1) // batch_size  # T√≠nh s·ªë batch c·∫ßn x·ª≠ l√Ω\n",
        "\n",
        "    for i in tqdm(range(total_batches), desc=\"ƒêang x·ª≠ l√Ω batch\"):\n",
        "        start_idx = i * batch_size\n",
        "        end_idx = min((i + 1) * batch_size, len(df))\n",
        "\n",
        "        batch = df.iloc[start_idx:end_idx].copy()\n",
        "\n",
        "        # Ti·ªÅn x·ª≠ l√Ω vƒÉn b·∫£n\n",
        "        batch['preprocessed_text'] = batch[text_column].apply(self.preprocess_text)\n",
        "\n",
        "        # Tr√≠ch xu·∫•t th·ª±c th·ªÉ\n",
        "        entities_list = []\n",
        "        for text in batch['preprocessed_text']:\n",
        "            entities_list.append(self.extract_entities(text))\n",
        "\n",
        "        # C·∫≠p nh·∫≠t DataFrame v·ªõi th·ª±c th·ªÉ tr√≠ch xu·∫•t\n",
        "        batch['extracted_foods'] = [data['foods'] for data in entities_list]\n",
        "        batch['extracted_locations'] = [data['locations'] for data in entities_list]\n",
        "        batch['extracted_tastes'] = [data['tastes'] for data in entities_list]\n",
        "\n",
        "        # C·∫≠p nh·∫≠t v√†o DataFrame g·ªëc\n",
        "        df.iloc[start_idx:end_idx] = batch\n",
        "\n",
        "        # L∆∞u k·∫øt qu·∫£ t·∫°m th·ªùi theo t·ª´ng batch\n",
        "        if (i + 1) % 5 == 0 or (i + 1) == total_batches:\n",
        "            self.save_entity_list(self.foods, \"foods\")\n",
        "            self.save_entity_list(self.locations, \"locations\")\n",
        "\n",
        "            # L∆∞u k·∫øt qu·∫£ trung gian\n",
        "            checkpoint_file = f\"extracted_data/processed_data_batch_{i+1}.csv\"\n",
        "            df.iloc[:end_idx].to_csv(checkpoint_file, index=False)\n",
        "            logging.info(f\"ƒê√£ l∆∞u k·∫øt qu·∫£ trung gian v√†o {checkpoint_file} sau batch {i+1}/{total_batches}\")\n",
        "\n",
        "    # Th·ªëng k√™ s·ªë l∆∞·ª£ng th·ª±c th·ªÉ ƒë√£ t√¨m th·∫•y\n",
        "    food_count = len(self.foods)\n",
        "    location_count = len(self.locations)\n",
        "\n",
        "    logging.info(f\"Tr√≠ch xu·∫•t ho√†n t·∫•t. T√¨m th·∫•y {food_count} th·ª±c th·ªÉ m√≥n ƒÉn v√† {location_count} th·ª±c th·ªÉ ƒë·ªãa ƒëi·ªÉm.\")\n",
        "\n",
        "    return df\n",
        "\n",
        "def bootstrap_entity_lists(self, df, text_column=\"preprocessed_text\", min_freq=3):\n",
        "    \"\"\"\n",
        "    M·ªü r·ªông danh s√°ch th·ª±c th·ªÉ b·∫±ng TF-IDF ƒë·ªÉ t√¨m c√°c th·ª±c th·ªÉ ti·ªÅm nƒÉng.\n",
        "    \n",
        "    Tham s·ªë:\n",
        "        df (pd.DataFrame): DataFrame ch·ª©a d·ªØ li·ªáu vƒÉn b·∫£n.\n",
        "        text_column (str): T√™n c·ªôt ch·ª©a vƒÉn b·∫£n ƒë√£ ti·ªÅn x·ª≠ l√Ω.\n",
        "        min_freq (int): S·ªë l·∫ßn xu·∫•t hi·ªán t·ªëi thi·ªÉu ƒë·ªÉ xem x√©t m·ªôt th·ª±c th·ªÉ.\n",
        "\n",
        "    Tr·∫£ v·ªÅ:\n",
        "        set: T·∫≠p h·ª£p c√°c th·ª±c th·ªÉ m√≥n ƒÉn m·ªõi ƒë∆∞·ª£c nh·∫≠n di·ªán.\n",
        "    \"\"\"\n",
        "    if df.empty or text_column not in df.columns:\n",
        "        logging.error(f\"Kh√¥ng th·ªÉ m·ªü r·ªông th·ª±c th·ªÉ: DataFrame kh√¥ng h·ª£p l·ªá ho·∫∑c thi·∫øu c·ªôt '{text_column}'\")\n",
        "        return set()\n",
        "\n",
        "    # L·ªçc ra c√°c vƒÉn b·∫£n h·ª£p l·ªá\n",
        "    valid_texts = df[text_column].dropna().replace('', pd.NA).dropna().tolist()\n",
        "\n",
        "    if not valid_texts:\n",
        "        logging.warning(\"Kh√¥ng t√¨m th·∫•y vƒÉn b·∫£n h·ª£p l·ªá ƒë·ªÉ m·ªü r·ªông th·ª±c th·ªÉ\")\n",
        "        return set()\n",
        "\n",
        "    try:\n",
        "        min_df_val = max(1, min(min_freq, len(valid_texts) // 2))\n",
        "        \n",
        "        tfidf = TfidfVectorizer(\n",
        "            ngram_range=(1, 3),  # X√©t c√°c n-gram t·ª´ 1 ƒë·∫øn 3 t·ª´\n",
        "            min_df=min_df_val,  # ƒêi·ªÅu ch·ªânh min_df\n",
        "            max_df=0.9  # Lo·∫°i b·ªè c√°c c·ª•m t·ª´ qu√° ph·ªï bi·∫øn\n",
        "        )\n",
        "\n",
        "        tfidf_matrix = tfidf.fit_transform(valid_texts)\n",
        "        feature_names = tfidf.get_feature_names_out()\n",
        "\n",
        "        # L·∫•y danh s√°ch n-gram c√≥ gi√° tr·ªã TF-IDF cao\n",
        "        important_ngrams = []\n",
        "        for i in range(min(tfidf_matrix.shape[0], 100)):\n",
        "            feature_index = tfidf_matrix[i,:].nonzero()[1]\n",
        "            tfidf_scores = zip(feature_index, [tfidf_matrix[i, x] for x in feature_index])\n",
        "            # S·∫Øp x·∫øp theo ƒëi·ªÉm TF-IDF gi·∫£m d·∫ßn\n",
        "            for idx, score in sorted(tfidf_scores, key=lambda x: x[1], reverse=True)[:20]:\n",
        "                important_ngrams.append(feature_names[idx])\n",
        "\n",
        "        # L·ªçc c√°c c·ª•m t·ª´ c√≥ th·ªÉ l√† t√™n m√≥n ƒÉn (d·ª±a v√†o t·ª´ ch·ªâ m√≥n ƒÉn)\n",
        "        potential_foods = set()\n",
        "        for text in valid_texts:\n",
        "            for indicator in self.food_indicators:\n",
        "                if indicator in text:\n",
        "                    for ngram in important_ngrams:\n",
        "                        # Ki·ªÉm tra n·∫øu ngram xu·∫•t hi·ªán g·∫ßn t·ª´ ch·ªâ m√≥n ƒÉn\n",
        "                        if ngram in text and re.search(r'\\b' + re.escape(indicator) + r'.{0,30}' + re.escape(ngram), text, re.IGNORECASE):\n",
        "                            potential_foods.add(ngram)\n",
        "                        if ngram in text and re.search(r'\\b' + re.escape(ngram) + r'.{0,30}' + re.escape(indicator), text, re.IGNORECASE):\n",
        "                            potential_foods.add(ngram)\n",
        "\n",
        "        # L·ªçc b·ªè c√°c th·ª±c th·ªÉ kh√¥ng h·ª£p l·ªá (qu√° ng·∫Øn, ch·ªâ ch·ª©a s·ªë, v.v.)\n",
        "        filtered_foods = {food for food in potential_foods if len(food) > 2 and not food.isdigit()}\n",
        "\n",
        "        # C·∫≠p nh·∫≠t danh s√°ch m√≥n ƒÉn\n",
        "        self.foods.update(filtered_foods)\n",
        "        logging.info(f\"ƒê√£ th√™m {len(filtered_foods)} th·ª±c th·ªÉ m√≥n ƒÉn ti·ªÅm nƒÉng t·ª´ m·ªü r·ªông th·ª±c th·ªÉ\")\n",
        "\n",
        "        return filtered_foods\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"L·ªói khi m·ªü r·ªông th·ª±c th·ªÉ: {e}\")\n",
        "        logging.error(traceback.format_exc())\n",
        "        return set()\n",
        "\n",
        "VietnameseTextProcessor.process_dataframe = process_dataframe\n",
        "VietnameseTextProcessor.bootstrap_entity_lists = bootstrap_entity_lists"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 183,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YI6pKb0WGWVC",
        "outputId": "174f413c-1aac-4c97-9d7b-a886256d1cf2"
      },
      "outputs": [],
      "source": [
        "def main():\n",
        "    try:\n",
        "        # T·∫°o m·ªôt th·ªÉ hi·ªán c·ªßa b·ªô x·ª≠ l√Ω vƒÉn b·∫£n\n",
        "        processor = VietnameseTextProcessor()\n",
        "\n",
        "        # T·∫£i t·∫≠p d·ªØ li·ªáu\n",
        "        logging.info(\"ƒêang t·∫£i t·∫≠p d·ªØ li·ªáu...\")\n",
        "        try:\n",
        "            # df = pd.read_csv(\"/content/21KHDL-TikTok-Analytics/data/interim/small_video_transcription.csv\")\n",
        "            df = pd.read_csv(\"C:/Users/nguye/OneDrive/TaÃÄi li√™Ã£u/GitHub/21KHDL-TikTok-Analytics/data/interim/small_video_transcription.csv\")\n",
        "            if df.empty:\n",
        "                logging.error(\"T·∫≠p d·ªØ li·ªáu ƒë∆∞·ª£c t·∫£i v·ªÅ tr·ªëng\")\n",
        "                return\n",
        "            logging.info(f\"T·∫≠p d·ªØ li·ªáu ƒë√£ t·∫£i c√≥ {len(df)} d√≤ng\")\n",
        "        except Exception as e:\n",
        "            logging.error(f\"L·ªói khi t·∫£i t·∫≠p d·ªØ li·ªáu: {e}\")\n",
        "            logging.error(traceback.format_exc())\n",
        "            return\n",
        "\n",
        "        # X·ª≠ l√Ω m·ªôt m·∫´u nh·ªè ƒë·ªÉ ki·ªÉm th·ª≠ (s·ª≠ d·ª•ng .head(10) ƒë·ªÉ th·ª≠ nghi·ªám, x√≥a b·ªè ƒë·ªÉ x·ª≠ l√Ω to√†n b·ªô)\n",
        "        sample_df = df.head(10)\n",
        "\n",
        "        # X·ª≠ l√Ω d·ªØ li·ªáu vƒÉn b·∫£n\n",
        "        logging.info(\"B·∫Øt ƒë·∫ßu x·ª≠ l√Ω vƒÉn b·∫£n v√† tr√≠ch xu·∫•t th·ª±c th·ªÉ...\")\n",
        "        processed_df = processor.process_dataframe(sample_df, text_column='video_transcription')\n",
        "\n",
        "        # M·ªü r·ªông danh s√°ch th·ª±c th·ªÉ b·∫±ng ph∆∞∆°ng ph√°p bootstrapping\n",
        "        logging.info(\"Th·ª±c hi·ªán bootstrapping ƒë·ªÉ m·ªü r·ªông danh s√°ch th·ª±c th·ªÉ...\")\n",
        "        processor.bootstrap_entity_lists(processed_df)\n",
        "\n",
        "        # L∆∞u k·∫øt qu·∫£ cu·ªëi c√πng\n",
        "        processed_df.to_csv(\"extracted_data/fully_processed_data.csv\", index=False)\n",
        "        processor.save_entity_list(processor.foods, \"foods\")\n",
        "        processor.save_entity_list(processor.locations, \"locations\")\n",
        "\n",
        "        # L∆∞u k·∫øt qu·∫£ c√≥ c·∫•u tr√∫c d∆∞·ªõi d·∫°ng JSON g·ªìm video_id, author_id v√† c√°c th·ª±c th·ªÉ tr√≠ch xu·∫•t\n",
        "        structured_data = []\n",
        "        for _, row in processed_df.iterrows():\n",
        "            structured_data.append({\n",
        "                'video_id': row.get('video_id', ''),\n",
        "                'author_id': row.get('author_id', ''),\n",
        "                'extracted_entities': {\n",
        "                    'foods': row.get('extracted_foods', []),\n",
        "                    'locations': row.get('extracted_locations', []),\n",
        "                    'tastes': row.get('extracted_tastes', [])\n",
        "                }\n",
        "            })\n",
        "\n",
        "        with open(\"extracted_data/structured_entities.json\", 'w', encoding='utf-8') as f:\n",
        "            json.dump(structured_data, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "        logging.info(\"Qu√° tr√¨nh x·ª≠ l√Ω ho√†n t·∫•t. K·∫øt qu·∫£ ƒë√£ ƒë∆∞·ª£c l∆∞u trong th∆∞ m·ª•c 'extracted_data'.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"L·ªói nghi√™m tr·ªçng trong h√†m main: {e}\")\n",
        "        logging.error(traceback.format_exc())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 184,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-03-09 23:18:40,074 - INFO - Kh√¥ng t√¨m th·∫•y danh s√°ch foods hi·ªán c√≥, b·∫Øt ƒë·∫ßu v·ªõi t·∫≠p r·ªóng\n",
            "2025-03-09 23:18:40,074 - INFO - Kh√¥ng t√¨m th·∫•y danh s√°ch locations hi·ªán c√≥, b·∫Øt ƒë·∫ßu v·ªõi t·∫≠p r·ªóng\n",
            "2025-03-09 23:18:40,078 - INFO - ƒêang t·∫£i t·∫≠p d·ªØ li·ªáu...\n",
            "2025-03-09 23:18:40,332 - INFO - T·∫≠p d·ªØ li·ªáu ƒë√£ t·∫£i c√≥ 10673 d√≤ng\n",
            "2025-03-09 23:18:40,333 - INFO - B·∫Øt ƒë·∫ßu x·ª≠ l√Ω vƒÉn b·∫£n v√† tr√≠ch xu·∫•t th·ª±c th·ªÉ...\n",
            "C:\\Users\\nguye\\AppData\\Local\\Temp\\ipykernel_13796\\74775411.py:22: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df['preprocessed_text'] = \"\"\n",
            "C:\\Users\\nguye\\AppData\\Local\\Temp\\ipykernel_13796\\74775411.py:23: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df['extracted_foods'] = None\n",
            "C:\\Users\\nguye\\AppData\\Local\\Temp\\ipykernel_13796\\74775411.py:24: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df['extracted_locations'] = None\n",
            "C:\\Users\\nguye\\AppData\\Local\\Temp\\ipykernel_13796\\74775411.py:25: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df['extracted_tastes'] = None\n",
            "ƒêang x·ª≠ l√Ω batch:   0%|          | 0/1 [00:00<?, ?it/s]2025-03-09 23:18:40,337 - ERROR - L·ªói khi ti·ªÅn x·ª≠ l√Ω vƒÉn b·∫£n: The provided dictionary language (vi) does not exist!\n",
            "2025-03-09 23:18:40,339 - ERROR - L·ªói khi ti·ªÅn x·ª≠ l√Ω vƒÉn b·∫£n: The provided dictionary language (vi) does not exist!\n",
            "2025-03-09 23:18:40,341 - ERROR - L·ªói khi ti·ªÅn x·ª≠ l√Ω vƒÉn b·∫£n: The provided dictionary language (vi) does not exist!\n",
            "2025-03-09 23:18:40,344 - ERROR - L·ªói khi ti·ªÅn x·ª≠ l√Ω vƒÉn b·∫£n: The provided dictionary language (vi) does not exist!\n",
            "2025-03-09 23:18:40,344 - ERROR - L·ªói khi ti·ªÅn x·ª≠ l√Ω vƒÉn b·∫£n: The provided dictionary language (vi) does not exist!\n",
            "2025-03-09 23:18:40,347 - ERROR - L·ªói khi ti·ªÅn x·ª≠ l√Ω vƒÉn b·∫£n: The provided dictionary language (vi) does not exist!\n",
            "2025-03-09 23:18:40,349 - ERROR - L·ªói khi ti·ªÅn x·ª≠ l√Ω vƒÉn b·∫£n: The provided dictionary language (vi) does not exist!\n",
            "2025-03-09 23:18:40,351 - ERROR - L·ªói khi ti·ªÅn x·ª≠ l√Ω vƒÉn b·∫£n: The provided dictionary language (vi) does not exist!\n",
            "2025-03-09 23:18:40,352 - ERROR - L·ªói khi ti·ªÅn x·ª≠ l√Ω vƒÉn b·∫£n: The provided dictionary language (vi) does not exist!\n",
            "2025-03-09 23:18:40,354 - ERROR - L·ªói khi ti·ªÅn x·ª≠ l√Ω vƒÉn b·∫£n: The provided dictionary language (vi) does not exist!\n",
            "2025-03-09 23:18:53,066 - INFO - ƒê√£ l∆∞u 83 foods v√†o extracted_data/foods_list.json\n",
            "2025-03-09 23:18:53,067 - INFO - ƒê√£ l∆∞u 7 locations v√†o extracted_data/locations_list.json\n",
            "2025-03-09 23:18:53,069 - INFO - ƒê√£ l∆∞u k·∫øt qu·∫£ trung gian v√†o extracted_data/processed_data_batch_1.csv sau batch 1/1\n",
            "ƒêang x·ª≠ l√Ω batch: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:12<00:00, 12.73s/it]\n",
            "2025-03-09 23:18:53,071 - INFO - Tr√≠ch xu·∫•t ho√†n t·∫•t. T√¨m th·∫•y 83 th·ª±c th·ªÉ m√≥n ƒÉn v√† 7 th·ª±c th·ªÉ ƒë·ªãa ƒëi·ªÉm.\n",
            "2025-03-09 23:18:53,072 - INFO - Th·ª±c hi·ªán bootstrapping ƒë·ªÉ m·ªü r·ªông danh s√°ch th·ª±c th·ªÉ...\n",
            "2025-03-09 23:18:55,627 - INFO - ƒê√£ th√™m 102 th·ª±c th·ªÉ m√≥n ƒÉn ti·ªÅm nƒÉng t·ª´ m·ªü r·ªông th·ª±c th·ªÉ\n",
            "2025-03-09 23:18:55,627 - INFO - ƒê√£ l∆∞u 179 foods v√†o extracted_data/foods_list.json\n",
            "2025-03-09 23:18:55,627 - INFO - ƒê√£ l∆∞u 7 locations v√†o extracted_data/locations_list.json\n",
            "2025-03-09 23:18:55,643 - INFO - Qu√° tr√¨nh x·ª≠ l√Ω ho√†n t·∫•t. K·∫øt qu·∫£ ƒë√£ ƒë∆∞·ª£c l∆∞u trong th∆∞ m·ª•c 'extracted_data'.\n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 185,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_food_prompts():\n",
        "    food_prompt_template = \"\"\"\n",
        "    H√£y ph√¢n t√≠ch th√¥ng tin v·ªÅ m√≥n ƒÉn sau ƒë√¢y:\n",
        "    \n",
        "    Danh s√°ch m√≥n ƒÉn: {foods}\n",
        "    \n",
        "    Y√™u c·∫ßu:\n",
        "    1. Ph√¢n lo·∫°i c√°c m√≥n ƒÉn th√†nh c√°c nh√≥m (v√≠ d·ª•: m√≥n n∆∞·ªõc, m√≥n n∆∞·ªõng, ƒë·ªì u·ªëng, etc.)\n",
        "    2. X√°c ƒë·ªãnh c√°c m√≥n ƒë·∫∑c tr∆∞ng nh·∫•t\n",
        "    3. ƒê·ªÅ xu·∫•t m√≥n ƒÉn ph·ªï bi·∫øn nh·∫•t d·ª±a tr√™n t·∫ßn su·∫•t xu·∫•t hi·ªán\n",
        "    4. Li√™n k·∫øt m√≥n ƒÉn v·ªõi vƒÉn h√≥a ·∫©m th·ª±c Vi·ªát Nam\n",
        "    5. ƒê·ªÅ xu·∫•t c√°c k·∫øt h·ª£p m√≥n ƒÉn ph√π h·ª£p\n",
        "    \n",
        "    Vui l√≤ng tr√¨nh b√†y k·∫øt qu·∫£ m·ªôt c√°ch chi ti·∫øt v√† c√≥ c·∫•u tr√∫c.\n",
        "    \"\"\"\n",
        "    return food_prompt_template\n",
        "\n",
        "def create_location_prompts():\n",
        "    location_prompt_template = \"\"\"\n",
        "    H√£y ph√¢n t√≠ch th√¥ng tin v·ªÅ ƒë·ªãa ƒëi·ªÉm ·∫©m th·ª±c sau ƒë√¢y:\n",
        "    \n",
        "    Danh s√°ch ƒë·ªãa ƒëi·ªÉm: {locations}\n",
        "    \n",
        "    Y√™u c·∫ßu:\n",
        "    1. Nh√≥m c√°c ƒë·ªãa ƒëi·ªÉm theo khu v·ª±c (qu·∫≠n/huy·ªán)\n",
        "    2. X√°c ƒë·ªãnh c√°c khu v·ª±c ·∫©m th·ª±c n·ªïi ti·∫øng\n",
        "    3. ƒê·ªÅ xu·∫•t tuy·∫øn ƒë∆∞·ªùng kh√°m ph√° ·∫©m th·ª±c\n",
        "    4. Li√™n k·∫øt ƒë·ªãa ƒëi·ªÉm v·ªõi ƒë·∫∑c tr∆∞ng ·∫©m th·ª±c\n",
        "    5. X√°c ƒë·ªãnh c√°c ƒëi·ªÉm ·∫©m th·ª±c c√≥ m·∫≠t ƒë·ªô cao\n",
        "    \n",
        "    Vui l√≤ng ph√¢n t√≠ch v√† ƒë∆∞a ra c√°c g·ª£i √Ω chi ti·∫øt cho ng∆∞·ªùi d√πng.\n",
        "    \"\"\"\n",
        "    return location_prompt_template\n",
        "\n",
        "def analyze_food_locations(structured_data):\n",
        "    \"\"\"Analyze food and location data using Gemini API\"\"\"\n",
        "    \n",
        "    import google.generativeai as genai\n",
        "    from collections import Counter\n",
        "    \n",
        "    # Configure API\n",
        "    genai.configure(api_key='AIzaSyD1WFlkEtQnFVDJCPbnitmaHQVdw2pXRK4')\n",
        "    model = genai.GenerativeModel('models/gemini-2.0-flash-thinking-exp-1219')\n",
        "    \n",
        "    # Extract unique foods and locations\n",
        "    all_foods = []\n",
        "    all_locations = []\n",
        "    \n",
        "    for item in structured_data:\n",
        "        all_foods.extend(item['extracted_entities']['foods'])\n",
        "        all_locations.extend(item['extracted_entities']['locations'])\n",
        "    \n",
        "    # Count frequencies\n",
        "    food_counts = Counter(all_foods)\n",
        "    location_counts = Counter(all_locations)\n",
        "    \n",
        "    # Create prompts\n",
        "    food_prompt = create_food_prompts().format(\n",
        "        foods=\"\\n\".join(f\"- {food} (xu·∫•t hi·ªán {count} l·∫ßn)\" \n",
        "                       for food, count in food_counts.most_common())\n",
        "    )\n",
        "    \n",
        "    location_prompt = create_location_prompts().format(\n",
        "        locations=\"\\n\".join(f\"- {loc} (xu·∫•t hi·ªán {count} l·∫ßn)\"\n",
        "                           for loc, count in location_counts.most_common())\n",
        "    )\n",
        "    \n",
        "    # Get responses from Gemini\n",
        "    food_analysis = model.generate_content(food_prompt)\n",
        "    location_analysis = model.generate_content(location_prompt)\n",
        "    \n",
        "    return {\n",
        "        'food_analysis': food_analysis.text,\n",
        "        'location_analysis': location_analysis.text,\n",
        "        'statistics': {\n",
        "            'total_unique_foods': len(set(all_foods)),\n",
        "            'total_unique_locations': len(set(all_locations)),\n",
        "            'most_common_foods': dict(food_counts.most_common(10)),\n",
        "            'most_common_locations': dict(location_counts.most_common(10))\n",
        "        }\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ph√¢n t√≠ch m√≥n ƒÉn v√† ƒë·ªãa ƒëi·ªÉm:\n",
            "\n",
            "Top 5 m√≥n ƒÉn ph·ªï bi·∫øn nh·∫•t:\n",
            "- g√†: 9 l·∫ßn\n",
            "- c√°: 8 l·∫ßn\n",
            "- th·ªãt: 7 l·∫ßn\n",
            "- ·ªëc: 7 l·∫ßn\n",
            "- n∆∞·ªõng: 6 l·∫ßn\n",
            "\n",
            "Top 5 ƒë·ªãa ƒëi·ªÉm ph·ªï bi·∫øn nh·∫•t:\n",
            "- b√¨nh th·∫°nh: 2 l·∫ßn\n",
            "- th·ªß ƒë·ª©c: 1 l·∫ßn\n",
            "- c·ªß chi: 1 l·∫ßn\n",
            "- t√¢n b√¨nh: 1 l·∫ßn\n",
            "- ph√∫ nhu·∫≠n: 1 l·∫ßn\n"
          ]
        }
      ],
      "source": [
        "# Example usage\n",
        "import json\n",
        "\n",
        "# Load structured data\n",
        "with open('C:/Users/nguye/OneDrive/TaÃÄi li√™Ã£u/GitHub/21KHDL-TikTok-Analytics/notebooks/extracted_data/structured_entities.json', 'r', encoding='utf-8') as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "# Run analysis\n",
        "results = analyze_food_locations(data)\n",
        "\n",
        "# Save results\n",
        "with open('food_location_analysis.json', 'w', encoding='utf-8') as f:\n",
        "    json.dump(results, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "# Print summary\n",
        "print(\"Ph√¢n t√≠ch m√≥n ƒÉn v√† ƒë·ªãa ƒëi·ªÉm:\")\n",
        "print(\"\\nTop 5 m√≥n ƒÉn ph·ªï bi·∫øn nh·∫•t:\")\n",
        "for food, count in list(results['statistics']['most_common_foods'].items())[:10]:\n",
        "    print(f\"- {food}: {count} l·∫ßn\")\n",
        "\n",
        "print(\"\\nTop 5 ƒë·ªãa ƒëi·ªÉm ph·ªï bi·∫øn nh·∫•t:\")\n",
        "for loc, count in list(results['statistics']['most_common_locations'].items())[:10]:\n",
        "    print(f\"- {loc}: {count} l·∫ßn\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "AppliedDataProject",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
